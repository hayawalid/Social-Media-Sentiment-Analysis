{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "fcA3aUy5gXSz"
      ],
      "authorship_tag": "ABX9TyMmhrkRa6/fWcZg0Nt8yXEw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janaghoniem/Social-Media-Sentiment-Analysis/blob/main/ML_Arabic%26English.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "gQ80u7VsaXde"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "from scipy.sparse import vstack\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "import warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Arabic Testing"
      ],
      "metadata": {
        "id": "fcA3aUy5gXSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/cleaned_arabic_data.csv')\n",
        "df.dropna(subset=[\"cleanedtext\"], inplace=True)\n",
        "X = df[\"cleanedtext\"]\n",
        "y = df[\"label\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAIgQCb-fqfE",
        "outputId": "6cafb2b7-adae-46d3-823d-3d3ef88dabcf"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Random forest"
      ],
      "metadata": {
        "id": "fyLD107ug7KS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "# Train-Test Split (80% train/valid, 20% test)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Further split the training set into training and validation (80% train, 20% valid from the 80%)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2\n",
        "\n",
        "# Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=10, max_depth=10, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "train_preds = rf_model.predict(X_train)\n",
        "val_preds = rf_model.predict(X_val)\n",
        "test_preds = rf_model.predict(X_test)\n",
        "\n",
        "# Accuracy scores\n",
        "train_accuracy = accuracy_score(y_train, train_preds)\n",
        "validation_accuracy = accuracy_score(y_val, val_preds)\n",
        "test_accuracy = accuracy_score(y_test, test_preds)\n",
        "\n",
        "# Output\n",
        "print(\"Random Forest Training Accuracy:\", train_accuracy)\n",
        "print(\"Random Forest Validation Accuracy:\", validation_accuracy)\n",
        "print(\"Random Forest Testing Accuracy:\", test_accuracy)\n",
        "\n",
        "# Common setup for all models\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)\n",
        "\n",
        "# Combine train and validation sets for GridSearchCV\n",
        "X_train_val = vstack([X_train, X_val])\n",
        "y_train_val = pd.concat([y_train, y_val])\n",
        "\n",
        "# Random Forest Hyperparameter Tuning\n",
        "rf_params = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [10, 20, 30, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "rf_grid = GridSearchCV(RandomForestClassifier(random_state=42),\n",
        "                      rf_params,\n",
        "                      cv=3,\n",
        "                      n_jobs=-1,\n",
        "                      verbose=1)\n",
        "rf_grid.fit(X_train_val, y_train_val)\n",
        "\n",
        "# Best model\n",
        "best_rf = rf_grid.best_estimator_\n",
        "\n",
        "# Predictions\n",
        "rf_train_preds = best_rf.predict(X_train)\n",
        "rf_val_preds = best_rf.predict(X_val)\n",
        "rf_test_preds = best_rf.predict(X_test)\n",
        "\n",
        "# Accuracy scores\n",
        "rf_train_acc = accuracy_score(y_train, rf_train_preds)\n",
        "rf_val_acc = accuracy_score(y_val, rf_val_preds)\n",
        "rf_test_acc = accuracy_score(y_test, rf_test_preds)\n",
        "\n",
        "print(\"Random Forest - Best Parameters:\", rf_grid.best_params_)\n",
        "print(\"Random Forest Training Accuracy:\", rf_train_acc)\n",
        "print(\"Random Forest Validation Accuracy:\", rf_val_acc)\n",
        "print(\"Random Forest Testing Accuracy:\", rf_test_acc)\n",
        "print(\"----------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4LDdqpLfrh9",
        "outputId": "ca695362-ea79-46c9-9704-772338410b25"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Training Accuracy: 0.7630631531576579\n",
            "Random Forest Validation Accuracy: 0.7395184879621991\n",
            "Random Forest Testing Accuracy: 0.7427435685892148\n",
            "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n",
            "Random Forest - Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}\n",
            "Random Forest Training Accuracy: 0.9946247312365618\n",
            "Random Forest Validation Accuracy: 0.9952748818720468\n",
            "Random Forest Testing Accuracy: 0.8288457211430286\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) SVM"
      ],
      "metadata": {
        "id": "AOXvTmD6hSN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "# First split: 80% train/valid, 20% test\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Second split: 80% train, 20% validation from the 80% temp data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)  # 0.25 of 80% = 20%\n",
        "\n",
        "# SVM Classifier\n",
        "svm_model = LinearSVC()\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "train_preds = svm_model.predict(X_train)\n",
        "val_preds = svm_model.predict(X_val)\n",
        "test_preds = svm_model.predict(X_test)\n",
        "\n",
        "# Accuracy scores\n",
        "train_accuracy = accuracy_score(y_train, train_preds)\n",
        "validation_accuracy = accuracy_score(y_val, val_preds)\n",
        "test_accuracy = accuracy_score(y_test, test_preds)\n",
        "\n",
        "# Output\n",
        "print(\"SVM Training Accuracy:\", train_accuracy)\n",
        "print(\"SVM Validation Accuracy:\", validation_accuracy)\n",
        "print(\"SVM Testing Accuracy:\", test_accuracy)\n",
        "\n",
        "# Common setup for all models\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)\n",
        "\n",
        "# Combine train and validation sets for GridSearchCV\n",
        "X_train_val = vstack([X_train, X_val])\n",
        "y_train_val = pd.concat([y_train, y_val])\n",
        "\n",
        "# SVM Hyperparameter Tuning\n",
        "# Improved SVM Hyperparameter Tuning\n",
        "print(\"\\nStarting SVM hyperparameter tuning...\")\n",
        "svm_params = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'penalty': ['l2'],  # Only l2 penalty to avoid incompatible combinations\n",
        "    'loss': ['squared_hinge'],  # Only squared_hinge to avoid issues\n",
        "    'dual': [False]  # Prefer primal form for large n_samples > n_features\n",
        "}\n",
        "\n",
        "svm_grid = GridSearchCV(\n",
        "    LinearSVC(random_state=42, max_iter=10000),  # Increased max_iter for convergence\n",
        "    svm_params,\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "svm_grid.fit(X_train_val, y_train_val)\n",
        "best_svm = svm_grid.best_estimator_\n",
        "\n",
        "# Predictions\n",
        "svm_train_preds = best_svm.predict(X_train)\n",
        "svm_val_preds = best_svm.predict(X_val)\n",
        "svm_test_preds = best_svm.predict(X_test)\n",
        "\n",
        "# Accuracy scores\n",
        "svm_train_acc = accuracy_score(y_train, svm_train_preds)\n",
        "svm_val_acc = accuracy_score(y_val, svm_val_preds)\n",
        "svm_test_acc = accuracy_score(y_test, svm_test_preds)\n",
        "\n",
        "print(\"SVM - Best Parameters:\", svm_grid.best_params_)\n",
        "print(\"SVM Training Accuracy:\", svm_train_acc)\n",
        "print(\"SVM Validation Accuracy:\", svm_val_acc)\n",
        "print(\"SVM Testing Accuracy:\", svm_test_acc)\n",
        "print(\"----------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtyTu2sihTOk",
        "outputId": "a1e2feb1-bdaf-475d-9a1e-9718d0cbe229"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Training Accuracy: 0.8954697734886744\n",
            "SVM Validation Accuracy: 0.8290707267681692\n",
            "SVM Testing Accuracy: 0.8349208730218255\n",
            "\n",
            "Starting SVM hyperparameter tuning...\n",
            "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
            "SVM - Best Parameters: {'C': 0.1, 'dual': False, 'loss': 'squared_hinge', 'penalty': 'l2'}\n",
            "SVM Training Accuracy: 0.8779688984449222\n",
            "SVM Validation Accuracy: 0.8720468011700292\n",
            "SVM Testing Accuracy: 0.8469961749043726\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Logistic regression"
      ],
      "metadata": {
        "id": "s32o3qjihjgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "# First split: 80% train/valid, 20% test\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Second split: 80% train, 20% validation from the 80% temp data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)  # 0.25 of 80%\n",
        "\n",
        "# Logistic Regression Model\n",
        "lr_model = LogisticRegression(max_iter=1000)\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "train_preds = lr_model.predict(X_train)\n",
        "val_preds = lr_model.predict(X_val)\n",
        "test_preds = lr_model.predict(X_test)\n",
        "\n",
        "# Accuracy scores\n",
        "train_accuracy = accuracy_score(y_train, train_preds)\n",
        "validation_accuracy = accuracy_score(y_val, val_preds)\n",
        "test_accuracy = accuracy_score(y_test, test_preds)\n",
        "\n",
        "# Output\n",
        "print(\"Logistic Regression Training Accuracy:\", train_accuracy)\n",
        "print(\"Logistic Regression Validation Accuracy:\", validation_accuracy)\n",
        "print(\"Logistic Regression Testing Accuracy:\", test_accuracy)\n",
        "\n",
        "# Common setup for all models\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)\n",
        "\n",
        "# Combine train and validation sets for GridSearchCV\n",
        "X_train_val = vstack([X_train, X_val])\n",
        "y_train_val = pd.concat([y_train, y_val])\n",
        "\n",
        "# Logistic Regression Hyperparameter Tuning\n",
        "lr_params = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "lr_grid = GridSearchCV(LogisticRegression(max_iter=1000, random_state=42),\n",
        "                      lr_params,\n",
        "                      cv=3,\n",
        "                      n_jobs=-1,\n",
        "                      verbose=1)\n",
        "lr_grid.fit(X_train_val, y_train_val)\n",
        "\n",
        "# Best model\n",
        "best_lr = lr_grid.best_estimator_\n",
        "\n",
        "# Predictions\n",
        "lr_train_preds = best_lr.predict(X_train)\n",
        "lr_val_preds = best_lr.predict(X_val)\n",
        "lr_test_preds = best_lr.predict(X_test)\n",
        "\n",
        "# Accuracy scores\n",
        "lr_train_acc = accuracy_score(y_train, lr_train_preds)\n",
        "lr_val_acc = accuracy_score(y_val, lr_val_preds)\n",
        "lr_test_acc = accuracy_score(y_test, lr_test_preds)\n",
        "\n",
        "print(\"Logistic Regression - Best Parameters:\", lr_grid.best_params_)\n",
        "print(\"Logistic Regression Training Accuracy:\", lr_train_acc)\n",
        "print(\"Logistic Regression Validation Accuracy:\", lr_val_acc)\n",
        "print(\"Logistic Regression Testing Accuracy:\", lr_test_acc)\n",
        "print(\"----------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzOt86EJhiu2",
        "outputId": "0bd27f5d-27f4-476e-8cd2-85261cace4b0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Training Accuracy: 0.879993999699985\n",
            "Logistic Regression Validation Accuracy: 0.8408460211505288\n",
            "Logistic Regression Testing Accuracy: 0.8480462011550288\n",
            "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
            "Logistic Regression - Best Parameters: {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
            "Logistic Regression Training Accuracy: 0.8770438521926096\n",
            "Logistic Regression Validation Accuracy: 0.8715217880447012\n",
            "Logistic Regression Testing Accuracy: 0.8470711767794195\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Naive Bayes"
      ],
      "metadata": {
        "id": "30rzbGl7hv4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "# First split: 80% train/valid, 20% test\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Second split: 80% train, 20% validation from the 80% temp data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)  # 0.25 of 80%\n",
        "\n",
        "# Naive Bayes Model\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "train_preds = nb_model.predict(X_train)\n",
        "val_preds = nb_model.predict(X_val)\n",
        "test_preds = nb_model.predict(X_test)\n",
        "\n",
        "# Accuracy scores\n",
        "train_accuracy = accuracy_score(y_train, train_preds)\n",
        "validation_accuracy = accuracy_score(y_val, val_preds)\n",
        "test_accuracy = accuracy_score(y_test, test_preds)\n",
        "\n",
        "# Output\n",
        "print(\"Naive Bayes Training Accuracy:\", train_accuracy)\n",
        "print(\"Naive Bayes Validation Accuracy:\", validation_accuracy)\n",
        "print(\"Naive Bayes Testing Accuracy:\", test_accuracy)\n",
        "\n",
        "# Common setup for all models\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)\n",
        "\n",
        "# Combine train and validation sets for GridSearchCV\n",
        "X_train_val = vstack([X_train, X_val])\n",
        "y_train_val = pd.concat([y_train, y_val])\n",
        "\n",
        "# Naive Bayes Hyperparameter Tuning\n",
        "nb_params = {\n",
        "    'alpha': [0.1, 0.5, 1.0, 1.5, 2.0],\n",
        "    'fit_prior': [True, False]\n",
        "}\n",
        "\n",
        "nb_grid = GridSearchCV(MultinomialNB(),\n",
        "                      nb_params,\n",
        "                      cv=3,\n",
        "                      n_jobs=-1,\n",
        "                      verbose=1)\n",
        "nb_grid.fit(X_train_val, y_train_val)\n",
        "\n",
        "# Best model\n",
        "best_nb = nb_grid.best_estimator_\n",
        "\n",
        "# Predictions\n",
        "nb_train_preds = best_nb.predict(X_train)\n",
        "nb_val_preds = best_nb.predict(X_val)\n",
        "nb_test_preds = best_nb.predict(X_test)\n",
        "\n",
        "# Accuracy scores\n",
        "nb_train_acc = accuracy_score(y_train, nb_train_preds)\n",
        "nb_val_acc = accuracy_score(y_val, nb_val_preds)\n",
        "nb_test_acc = accuracy_score(y_test, nb_test_preds)\n",
        "\n",
        "print(\"Naive Bayes - Best Parameters:\", nb_grid.best_params_)\n",
        "print(\"Naive Bayes Training Accuracy:\", nb_train_acc)\n",
        "print(\"Naive Bayes Validation Accuracy:\", nb_val_acc)\n",
        "print(\"Naive Bayes Testing Accuracy:\", nb_test_acc)\n",
        "print(\"----------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UKsCXImhxHf",
        "outputId": "a5d7af3a-79d7-42bd-c886-e34a9c93f1bd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Training Accuracy: 0.8457922896144807\n",
            "Naive Bayes Validation Accuracy: 0.8221705542638565\n",
            "Naive Bayes Testing Accuracy: 0.828545713642841\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "Naive Bayes - Best Parameters: {'alpha': 2.0, 'fit_prior': False}\n",
            "Naive Bayes Training Accuracy: 0.8435421771088555\n",
            "Naive Bayes Validation Accuracy: 0.8373209330233256\n",
            "Naive Bayes Testing Accuracy: 0.8301207530188255\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### English testing"
      ],
      "metadata": {
        "id": "SBGkVmHageFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/English_cleaned.csv')\n",
        "X = df['normalized_tweet']\n",
        "y = df['target']"
      ],
      "metadata": {
        "id": "j5D_s8ktgja6"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Random Forest"
      ],
      "metadata": {
        "id": "zs7P71LSppTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train+validation and test sets first (80% train_val, 20% test)\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Then split train+validation into train and validation sets (80/20 split of train_val)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val, test_size=0.25, random_state=42, stratify=y_train_val\n",
        ")  # 0.25 x 0.8 = 0.2, so final: 60% train, 20% val, 20% test\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_val_tfidf = vectorizer.transform(X_val)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,  # Number of trees\n",
        "    random_state=42,\n",
        "    n_jobs=-1  # Use all cores for training\n",
        ")\n",
        "rf_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_train_pred = rf_model.predict(X_train_tfidf)\n",
        "y_val_pred = rf_model.predict(X_val_tfidf)\n",
        "y_test_pred = rf_model.predict(X_test_tfidf)\n",
        "\n",
        "# Accuracy calculations\n",
        "train_acc = accuracy_score(y_train, y_train_pred)\n",
        "val_acc = accuracy_score(y_val, y_val_pred)\n",
        "test_acc = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "# Output results\n",
        "print(f\"Random Forest Train Accuracy: {train_acc:.4f}\")\n",
        "print(f\"Random Forest Validation Accuracy: {val_acc:.4f}\")\n",
        "print(f\"Random Forest Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    RandomForestClassifier(random_state=42, n_jobs=-1),\n",
        "    param_grid,\n",
        "    cv=3,\n",
        "    scoring='accuracy',\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit grid search to the training data\n",
        "grid_search.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Best parameters and model\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "best_rf_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate tuned model\n",
        "y_train_pred_tuned = best_rf_model.predict(X_train_tfidf)\n",
        "y_val_pred_tuned = best_rf_model.predict(X_val_tfidf)\n",
        "y_test_pred_tuned = best_rf_model.predict(X_test_tfidf)\n",
        "\n",
        "# Accuracy calculations for tuned model\n",
        "train_acc_tuned = accuracy_score(y_train, y_train_pred_tuned)\n",
        "val_acc_tuned = accuracy_score(y_val, y_val_pred_tuned)\n",
        "test_acc_tuned = accuracy_score(y_test, y_test_pred_tuned)\n",
        "\n",
        "# Output tuned model results\n",
        "print(f\"TUNED Random Forest Train Accuracy: {train_acc_tuned:.4f}\")\n",
        "print(f\"TUNED Random Forest Validation Accuracy: {val_acc_tuned:.4f}\")\n",
        "print(f\"TUNED Random Forest Test Accuracy: {test_acc_tuned:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "wEN7lB1vppki",
        "outputId": "367d595d-c42c-438c-8cf0-296bcb9196f6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-21326156ad83>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m  \u001b[0;31m# Use all cores for training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m )\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mrf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m             trees = Parallel(\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2005\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2007\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 (self._jobs[0].get_status(\n\u001b[1;32m   1761\u001b[0m                     timeout=self.timeout) == TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) SVM"
      ],
      "metadata": {
        "id": "EpiSicYDpqMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train+validation and test sets first (80% train_val, 20% test)\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Then split train+validation into train and validation sets (80/20 split of train_val)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val, test_size=0.25, random_state=42, stratify=y_train_val\n",
        ")  # Final: 60% train, 20% val, 20% test\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_val_tfidf = vectorizer.transform(X_val)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Linear SVM Model\n",
        "svm_model = LinearSVC(max_iter=10000, random_state=42)  # Increased max_iter for stability\n",
        "svm_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_train_pred = svm_model.predict(X_train_tfidf)\n",
        "y_val_pred = svm_model.predict(X_val_tfidf)\n",
        "y_test_pred = svm_model.predict(X_test_tfidf)\n",
        "\n",
        "# Accuracy calculations\n",
        "train_acc = accuracy_score(y_train, y_train_pred)\n",
        "val_acc = accuracy_score(y_val, y_val_pred)\n",
        "test_acc = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "# Output results\n",
        "print(f\"SVM Train Accuracy: {train_acc:.4f}\")\n",
        "print(f\"SVM Validation Accuracy: {val_acc:.4f}\")\n",
        "print(f\"SVM Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Define parameter grid\n",
        "svm_param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'loss': ['hinge', 'squared_hinge'],\n",
        "    'max_iter': [20000, 30000]\n",
        "}\n",
        "\n",
        "# Set up GridSearchCV\n",
        "svm_grid_search = GridSearchCV(\n",
        "    LinearSVC(random_state=42),\n",
        "    svm_param_grid,\n",
        "    cv=3,\n",
        "    scoring='accuracy',\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit grid search to training data\n",
        "svm_grid_search.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Best parameters and model\n",
        "print(\"Best SVM Hyperparameters:\", svm_grid_search.best_params_)\n",
        "best_svm_model = svm_grid_search.best_estimator_\n",
        "\n",
        "# Predictions using the tuned model\n",
        "y_train_pred_tuned = best_svm_model.predict(X_train_tfidf)\n",
        "y_val_pred_tuned = best_svm_model.predict(X_val_tfidf)\n",
        "y_test_pred_tuned = best_svm_model.predict(X_test_tfidf)\n",
        "\n",
        "# Accuracy calculations for tuned model\n",
        "train_acc_tuned = accuracy_score(y_train, y_train_pred_tuned)\n",
        "val_acc_tuned = accuracy_score(y_val, y_val_pred_tuned)\n",
        "test_acc_tuned = accuracy_score(y_test, y_test_pred_tuned)\n",
        "\n",
        "# Output tuned model results\n",
        "print(f\"TUNED SVM Train Accuracy: {train_acc_tuned:.4f}\")\n",
        "print(f\"TUNED SVM Validation Accuracy: {val_acc_tuned:.4f}\")\n",
        "print(f\"TUNED SVM Test Accuracy: {test_acc_tuned:.4f}\")"
      ],
      "metadata": {
        "id": "yxRb4lNYpqZp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "outputId": "ab85575e-4904-4c9b-d28a-153517163d0f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Train Accuracy: 0.7900\n",
            "SVM Validation Accuracy: 0.7856\n",
            "SVM Test Accuracy: 0.7862\n",
            "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-50afd67ed993>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# Fit grid search to training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0msvm_grid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m# Best parameters and model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1022\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1569\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1571\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    968\u001b[0m                     )\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    971\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    972\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2005\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2007\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 (self._jobs[0].get_status(\n\u001b[1;32m   1761\u001b[0m                     timeout=self.timeout) == TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Logistic Regression"
      ],
      "metadata": {
        "id": "incM8HpcpBQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train+validation and test sets first (80% train_val, 20% test)\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Then split train+validation into train and validation sets (80/20 split of train_val)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val, test_size=0.25, random_state=42, stratify=y_train_val\n",
        ")  # 0.25 x 0.8 = 0.2, so final: 60% train, 20% val, 20% test\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_val_tfidf = vectorizer.transform(X_val)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Logistic Regression Model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_train_pred = model.predict(X_train_tfidf)\n",
        "y_val_pred = model.predict(X_val_tfidf)\n",
        "y_test_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "# Accuracy calculations\n",
        "train_acc = accuracy_score(y_train, y_train_pred)\n",
        "val_acc = accuracy_score(y_val, y_val_pred)\n",
        "test_acc = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "# Output results\n",
        "print(f\"Train Accuracy: {train_acc:.4f}\")\n",
        "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l2'],\n",
        "    'solver': ['lbfgs'],\n",
        "    'max_iter': [1000]\n",
        "}\n",
        "\n",
        "# Grid search setup\n",
        "grid_search = GridSearchCV(\n",
        "    LogisticRegression(),\n",
        "    param_grid,\n",
        "    cv=3,\n",
        "    scoring='accuracy',\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Train grid search on training data\n",
        "grid_search.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Best parameters and model\n",
        "print(\"Best Logistic Regression Parameters:\", grid_search.best_params_)\n",
        "best_lr_model = grid_search.best_estimator_\n",
        "\n",
        "# Predictions using tuned model\n",
        "y_train_pred_tuned = best_lr_model.predict(X_train_tfidf)\n",
        "y_val_pred_tuned = best_lr_model.predict(X_val_tfidf)\n",
        "y_test_pred_tuned = best_lr_model.predict(X_test_tfidf)\n",
        "\n",
        "# Accuracy calculations for tuned model\n",
        "train_acc_tuned = accuracy_score(y_train, y_train_pred_tuned)\n",
        "val_acc_tuned = accuracy_score(y_val, y_val_pred_tuned)\n",
        "test_acc_tuned = accuracy_score(y_test, y_test_pred_tuned)\n",
        "\n",
        "# Output tuned model results\n",
        "print(f\"TUNED Logistic Regression Train Accuracy: {train_acc_tuned:.4f}\")\n",
        "print(f\"TUNED Logistic Regression Validation Accuracy: {val_acc_tuned:.4f}\")\n",
        "print(f\"TUNED Logistic Regression Test Accuracy: {test_acc_tuned:.4f}\")"
      ],
      "metadata": {
        "id": "LFjGxRnvo48l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Naive Bayes"
      ],
      "metadata": {
        "id": "nko6spbOqD0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train+validation and test sets first (80% train_val, 20% test)\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Then split train+validation into train and validation sets (80/20 split of train_val)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val, test_size=0.25, random_state=42, stratify=y_train_val\n",
        ")  # Final: 60% train, 20% val, 20% test\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_val_tfidf = vectorizer.transform(X_val)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Naive Bayes Model\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_train_pred = nb_model.predict(X_train_tfidf)\n",
        "y_val_pred = nb_model.predict(X_val_tfidf)\n",
        "y_test_pred = nb_model.predict(X_test_tfidf)\n",
        "\n",
        "# Accuracy calculations\n",
        "train_acc = accuracy_score(y_train, y_train_pred)\n",
        "val_acc = accuracy_score(y_val, y_val_pred)\n",
        "test_acc = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "# Output results\n",
        "print(f\"Naive Bayes Train Accuracy: {train_acc:.4f}\")\n",
        "print(f\"Naive Bayes Validation Accuracy: {val_acc:.4f}\")\n",
        "print(f\"Naive Bayes Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Parameter grid for Naive Bayes\n",
        "nb_param_grid = {\n",
        "    'alpha': [0.01, 0.1, 0.5, 1.0, 2.0]  # Laplace smoothing\n",
        "}\n",
        "\n",
        "# Set up GridSearchCV\n",
        "nb_grid_search = GridSearchCV(\n",
        "    MultinomialNB(),\n",
        "    nb_param_grid,\n",
        "    cv=3,\n",
        "    scoring='accuracy',\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit grid search on training data\n",
        "nb_grid_search.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Best parameters and model\n",
        "print(\"Best Naive Bayes Hyperparameters:\", nb_grid_search.best_params_)\n",
        "best_nb_model = nb_grid_search.best_estimator_\n",
        "\n",
        "# Predictions using tuned model\n",
        "y_train_pred_tuned = best_nb_model.predict(X_train_tfidf)\n",
        "y_val_pred_tuned = best_nb_model.predict(X_val_tfidf)\n",
        "y_test_pred_tuned = best_nb_model.predict(X_test_tfidf)\n",
        "\n",
        "# Accuracy calculations for tuned model\n",
        "train_acc_tuned = accuracy_score(y_train, y_train_pred_tuned)\n",
        "val_acc_tuned = accuracy_score(y_val, y_val_pred_tuned)\n",
        "test_acc_tuned = accuracy_score(y_test, y_test_pred_tuned)\n",
        "\n",
        "# Output tuned model results\n",
        "print(f\"TUNED Naive Bayes Train Accuracy: {train_acc_tuned:.4f}\")\n",
        "print(f\"TUNED Naive Bayes Validation Accuracy: {val_acc_tuned:.4f}\")\n",
        "print(f\"TUNED Naive Bayes Test Accuracy: {test_acc_tuned:.4f}\")"
      ],
      "metadata": {
        "id": "1eV8QfV4qD_l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}