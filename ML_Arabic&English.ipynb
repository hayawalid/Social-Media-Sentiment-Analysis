{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "fcA3aUy5gXSz"
      ],
      "authorship_tag": "ABX9TyPqXj9IYYNlbDx+DZYPu5eR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janaghoniem/Social-Media-Sentiment-Analysis/blob/main/ML_Arabic%26English.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gQ80u7VsaXde"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "from scipy.sparse import vstack\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "import warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Arabic Testing"
      ],
      "metadata": {
        "id": "fcA3aUy5gXSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/embedded_arabic.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAIgQCb-fqfE",
        "outputId": "fdf1768c-4136-4d0b-ad1e-6f39f05ccd6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "p89-h3JFnDRH",
        "outputId": "77d9c046-938d-4cc0-fb4f-e8c063cce2f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  \\\n",
              "0  ممتاز نوعا ما . النظافة والموقع والتجهيز والشا...   \n",
              "1  أحد أسباب نجاح الإمارات أن كل شخص في هذه الدول...   \n",
              "2  هادفة .. وقوية. تنقلك من صخب شوارع القاهرة الى...   \n",
              "3  خلصنا .. مبدئيا اللي مستني ابهار زي الفيل الاز...   \n",
              "4  ياسات جلوريا جزء لا يتجزأ من دبي . فندق متكامل...   \n",
              "\n",
              "                                         stemmedtext  label  \\\n",
              "0               متز نوع ما . نظف وقع جهز شاطيء . طعم      2   \n",
              "1  احد سبب نجح امر ان شخص دول عشق ترب . نحب امر ....      2   \n",
              "2  هدف . وقي . نقل صخب شرع قهر الي هدء جبل شيش . ...      2   \n",
              "3  خلص . بدي الل مست بهر زي فيل زرق ميقراش احس . ...      2   \n",
              "4  ياس جلر جزء لا تجز دبي . ندق كامل خدم ريح نفس ...      2   \n",
              "\n",
              "                                     text_embeddings  \n",
              "0  [-8.53703637e-03 -1.90370362e-02  1.65396303e-...  \n",
              "1  [-3.13104391e-02 -4.76208851e-02  1.56378314e-...  \n",
              "2  [-3.92383114e-02 -4.87467274e-02  1.76576644e-...  \n",
              "3  [-0.01965529 -0.05666145  0.15446861 -0.006716...  \n",
              "4  [-2.29525026e-02 -5.09175062e-02  1.79437503e-...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-86108024-dc29-4d85-a5b0-af5386d48af3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>stemmedtext</th>\n",
              "      <th>label</th>\n",
              "      <th>text_embeddings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ممتاز نوعا ما . النظافة والموقع والتجهيز والشا...</td>\n",
              "      <td>متز نوع ما . نظف وقع جهز شاطيء . طعم</td>\n",
              "      <td>2</td>\n",
              "      <td>[-8.53703637e-03 -1.90370362e-02  1.65396303e-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>أحد أسباب نجاح الإمارات أن كل شخص في هذه الدول...</td>\n",
              "      <td>احد سبب نجح امر ان شخص دول عشق ترب . نحب امر ....</td>\n",
              "      <td>2</td>\n",
              "      <td>[-3.13104391e-02 -4.76208851e-02  1.56378314e-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>هادفة .. وقوية. تنقلك من صخب شوارع القاهرة الى...</td>\n",
              "      <td>هدف . وقي . نقل صخب شرع قهر الي هدء جبل شيش . ...</td>\n",
              "      <td>2</td>\n",
              "      <td>[-3.92383114e-02 -4.87467274e-02  1.76576644e-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>خلصنا .. مبدئيا اللي مستني ابهار زي الفيل الاز...</td>\n",
              "      <td>خلص . بدي الل مست بهر زي فيل زرق ميقراش احس . ...</td>\n",
              "      <td>2</td>\n",
              "      <td>[-0.01965529 -0.05666145  0.15446861 -0.006716...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ياسات جلوريا جزء لا يتجزأ من دبي . فندق متكامل...</td>\n",
              "      <td>ياس جلر جزء لا تجز دبي . ندق كامل خدم ريح نفس ...</td>\n",
              "      <td>2</td>\n",
              "      <td>[-2.29525026e-02 -5.09175062e-02  1.79437503e-...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-86108024-dc29-4d85-a5b0-af5386d48af3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-86108024-dc29-4d85-a5b0-af5386d48af3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-86108024-dc29-4d85-a5b0-af5386d48af3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-7c100a1b-3585-4a43-b2f3-e0f2187c0467\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7c100a1b-3585-4a43-b2f3-e0f2187c0467')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-7c100a1b-3585-4a43-b2f3-e0f2187c0467 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade numpy\n",
        "!pip uninstall -y gensim\n",
        "!pip install gensim\n"
      ],
      "metadata": {
        "id": "jdTIBpjhvKwX",
        "outputId": "42daa2f6-14e9-4845-c2f4-e708f682a7a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Collecting numpy\n",
            "  Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.5 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.5\n",
            "Found existing installation: gensim 4.3.3\n",
            "Uninstalling gensim-4.3.3:\n",
            "  Successfully uninstalled gensim-4.3.3\n",
            "Collecting gensim\n",
            "  Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Installing collected packages: numpy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.5\n",
            "    Uninstalling numpy-2.2.5:\n",
            "      Successfully uninstalled numpy-2.2.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Random forest"
      ],
      "metadata": {
        "id": "fyLD107ug7KS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "import re\n",
        "import pickle\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "# import gensim.downloader as api\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/Data Science/Arabic Sentiment/Features/training_data.pkl\", 'rb') as f:\n",
        "    training_data = pickle.load(f)\n",
        "\n",
        "X = training_data['X_fasttext']\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(df['label'])\n",
        "y_cat = tf.keras.utils.to_categorical(y)\n",
        "\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y_cat, test_size=0.2, stratify=y_cat, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.1, stratify=y_temp, random_state=42)\n",
        "\n",
        "# Train initial Random Forest\n",
        "rf_model = RandomForestClassifier(n_estimators=10, max_depth=10, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Initial Random Forest Training Accuracy:\", accuracy_score(y_train, rf_model.predict(X_train)))\n",
        "print(\"Initial Random Forest Validation Accuracy:\", accuracy_score(y_val, rf_model.predict(X_val)))\n",
        "print(\"Initial Random Forest Testing Accuracy:\", accuracy_score(y_test, rf_model.predict(X_test)))\n",
        "\n",
        "print(\"--------------------------------------------------\")\n",
        "\n",
        "# Combine train and validation sets for hyperparameter tuning\n",
        "X_train_val = np.vstack([X_train, X_val])\n",
        "y_train_val = pd.concat([y_train, y_val])\n",
        "\n",
        "# Define hyperparameter grid\n",
        "rf_params = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [10, 20, 30, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Grid Search with Cross Validation\n",
        "rf_grid = GridSearchCV(RandomForestClassifier(random_state=42),\n",
        "                       rf_params,\n",
        "                       cv=3,\n",
        "                       n_jobs=-1,\n",
        "                       verbose=1)\n",
        "rf_grid.fit(X_train_val, y_train_val)\n",
        "\n",
        "# Best model\n",
        "best_rf = rf_grid.best_estimator_\n",
        "\n",
        "# Evaluate best model\n",
        "print(\"Best Parameters:\", rf_grid.best_params_)\n",
        "print(\"Best Random Forest Training Accuracy:\", accuracy_score(y_train, best_rf.predict(X_train)))\n",
        "print(\"Best Random Forest Validation Accuracy:\", accuracy_score(y_val, best_rf.predict(X_val)))\n",
        "print(\"Best Random Forest Testing Accuracy:\", accuracy_score(y_test, best_rf.predict(X_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "R4LDdqpLfrh9",
        "outputId": "9f0f47b0-b1e4-41f4-948f-7fa96661cf44"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/Data Science/Arabic Sentiment/Features/training_data.pkl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-543d0555d007>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/Colab Notebooks/Data Science/Arabic Sentiment/Features/training_data.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/Data Science/Arabic Sentiment/Features/training_data.pkl'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) SVM"
      ],
      "metadata": {
        "id": "AOXvTmD6hSN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "# First split: 80% train/valid, 20% test\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Second split: 80% train, 20% validation from the 80% temp data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)  # 0.25 of 80% = 20%\n",
        "\n",
        "# SVM Classifier\n",
        "svm_model = LinearSVC()\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "train_preds = svm_model.predict(X_train)\n",
        "val_preds = svm_model.predict(X_val)\n",
        "test_preds = svm_model.predict(X_test)\n",
        "\n",
        "# Accuracy scores\n",
        "train_accuracy = accuracy_score(y_train, train_preds)\n",
        "validation_accuracy = accuracy_score(y_val, val_preds)\n",
        "test_accuracy = accuracy_score(y_test, test_preds)\n",
        "\n",
        "# Output\n",
        "print(\"SVM Training Accuracy:\", train_accuracy)\n",
        "print(\"SVM Validation Accuracy:\", validation_accuracy)\n",
        "print(\"SVM Testing Accuracy:\", test_accuracy)\n",
        "\n",
        "# Common setup for all models\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)\n",
        "\n",
        "# Combine train and validation sets for GridSearchCV\n",
        "X_train_val = vstack([X_train, X_val])\n",
        "y_train_val = pd.concat([y_train, y_val])\n",
        "\n",
        "# SVM Hyperparameter Tuning\n",
        "# Improved SVM Hyperparameter Tuning\n",
        "print(\"\\nStarting SVM hyperparameter tuning...\")\n",
        "svm_params = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'penalty': ['l2'],  # Only l2 penalty to avoid incompatible combinations\n",
        "    'loss': ['squared_hinge'],  # Only squared_hinge to avoid issues\n",
        "    'dual': [False]  # Prefer primal form for large n_samples > n_features\n",
        "}\n",
        "\n",
        "svm_grid = GridSearchCV(\n",
        "    LinearSVC(random_state=42, max_iter=10000),  # Increased max_iter for convergence\n",
        "    svm_params,\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "svm_grid.fit(X_train_val, y_train_val)\n",
        "best_svm = svm_grid.best_estimator_\n",
        "\n",
        "# Predictions\n",
        "svm_train_preds = best_svm.predict(X_train)\n",
        "svm_val_preds = best_svm.predict(X_val)\n",
        "svm_test_preds = best_svm.predict(X_test)\n",
        "\n",
        "# Accuracy scores\n",
        "svm_train_acc = accuracy_score(y_train, svm_train_preds)\n",
        "svm_val_acc = accuracy_score(y_val, svm_val_preds)\n",
        "svm_test_acc = accuracy_score(y_test, svm_test_preds)\n",
        "\n",
        "print(\"SVM - Best Parameters:\", svm_grid.best_params_)\n",
        "print(\"SVM Training Accuracy:\", svm_train_acc)\n",
        "print(\"SVM Validation Accuracy:\", svm_val_acc)\n",
        "print(\"SVM Testing Accuracy:\", svm_test_acc)\n",
        "print(\"----------------------------------------\")"
      ],
      "metadata": {
        "id": "OtyTu2sihTOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Logistic regression"
      ],
      "metadata": {
        "id": "s32o3qjihjgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "# First split: 80% train/valid, 20% test\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Second split: 80% train, 20% validation from the 80% temp data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)  # 0.25 of 80%\n",
        "\n",
        "# Logistic Regression Model\n",
        "lr_model = LogisticRegression(max_iter=1000)\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "train_preds = lr_model.predict(X_train)\n",
        "val_preds = lr_model.predict(X_val)\n",
        "test_preds = lr_model.predict(X_test)\n",
        "\n",
        "# Accuracy scores\n",
        "train_accuracy = accuracy_score(y_train, train_preds)\n",
        "validation_accuracy = accuracy_score(y_val, val_preds)\n",
        "test_accuracy = accuracy_score(y_test, test_preds)\n",
        "\n",
        "# Output\n",
        "print(\"Logistic Regression Training Accuracy:\", train_accuracy)\n",
        "print(\"Logistic Regression Validation Accuracy:\", validation_accuracy)\n",
        "print(\"Logistic Regression Testing Accuracy:\", test_accuracy)\n",
        "\n",
        "# Common setup for all models\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)\n",
        "\n",
        "# Combine train and validation sets for GridSearchCV\n",
        "X_train_val = vstack([X_train, X_val])\n",
        "y_train_val = pd.concat([y_train, y_val])\n",
        "\n",
        "# Logistic Regression Hyperparameter Tuning\n",
        "lr_params = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "lr_grid = GridSearchCV(LogisticRegression(max_iter=1000, random_state=42),\n",
        "                      lr_params,\n",
        "                      cv=3,\n",
        "                      n_jobs=-1,\n",
        "                      verbose=1)\n",
        "lr_grid.fit(X_train_val, y_train_val)\n",
        "\n",
        "# Best model\n",
        "best_lr = lr_grid.best_estimator_\n",
        "\n",
        "# Predictions\n",
        "lr_train_preds = best_lr.predict(X_train)\n",
        "lr_val_preds = best_lr.predict(X_val)\n",
        "lr_test_preds = best_lr.predict(X_test)\n",
        "\n",
        "# Accuracy scores\n",
        "lr_train_acc = accuracy_score(y_train, lr_train_preds)\n",
        "lr_val_acc = accuracy_score(y_val, lr_val_preds)\n",
        "lr_test_acc = accuracy_score(y_test, lr_test_preds)\n",
        "\n",
        "print(\"Logistic Regression - Best Parameters:\", lr_grid.best_params_)\n",
        "print(\"Logistic Regression Training Accuracy:\", lr_train_acc)\n",
        "print(\"Logistic Regression Validation Accuracy:\", lr_val_acc)\n",
        "print(\"Logistic Regression Testing Accuracy:\", lr_test_acc)\n",
        "print(\"----------------------------------------\")"
      ],
      "metadata": {
        "id": "uzOt86EJhiu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Naive Bayes"
      ],
      "metadata": {
        "id": "30rzbGl7hv4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "# First split: 80% train/valid, 20% test\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Second split: 80% train, 20% validation from the 80% temp data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)  # 0.25 of 80%\n",
        "\n",
        "# Naive Bayes Model\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "train_preds = nb_model.predict(X_train)\n",
        "val_preds = nb_model.predict(X_val)\n",
        "test_preds = nb_model.predict(X_test)\n",
        "\n",
        "# Accuracy scores\n",
        "train_accuracy = accuracy_score(y_train, train_preds)\n",
        "validation_accuracy = accuracy_score(y_val, val_preds)\n",
        "test_accuracy = accuracy_score(y_test, test_preds)\n",
        "\n",
        "# Output\n",
        "print(\"Naive Bayes Training Accuracy:\", train_accuracy)\n",
        "print(\"Naive Bayes Validation Accuracy:\", validation_accuracy)\n",
        "print(\"Naive Bayes Testing Accuracy:\", test_accuracy)\n",
        "\n",
        "# Common setup for all models\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)\n",
        "\n",
        "# Combine train and validation sets for GridSearchCV\n",
        "X_train_val = vstack([X_train, X_val])\n",
        "y_train_val = pd.concat([y_train, y_val])\n",
        "\n",
        "# Naive Bayes Hyperparameter Tuning\n",
        "nb_params = {\n",
        "    'alpha': [0.1, 0.5, 1.0, 1.5, 2.0],\n",
        "    'fit_prior': [True, False]\n",
        "}\n",
        "\n",
        "nb_grid = GridSearchCV(MultinomialNB(),\n",
        "                      nb_params,\n",
        "                      cv=3,\n",
        "                      n_jobs=-1,\n",
        "                      verbose=1)\n",
        "nb_grid.fit(X_train_val, y_train_val)\n",
        "\n",
        "# Best model\n",
        "best_nb = nb_grid.best_estimator_\n",
        "\n",
        "# Predictions\n",
        "nb_train_preds = best_nb.predict(X_train)\n",
        "nb_val_preds = best_nb.predict(X_val)\n",
        "nb_test_preds = best_nb.predict(X_test)\n",
        "\n",
        "# Accuracy scores\n",
        "nb_train_acc = accuracy_score(y_train, nb_train_preds)\n",
        "nb_val_acc = accuracy_score(y_val, nb_val_preds)\n",
        "nb_test_acc = accuracy_score(y_test, nb_test_preds)\n",
        "\n",
        "print(\"Naive Bayes - Best Parameters:\", nb_grid.best_params_)\n",
        "print(\"Naive Bayes Training Accuracy:\", nb_train_acc)\n",
        "print(\"Naive Bayes Validation Accuracy:\", nb_val_acc)\n",
        "print(\"Naive Bayes Testing Accuracy:\", nb_test_acc)\n",
        "print(\"----------------------------------------\")"
      ],
      "metadata": {
        "id": "5UKsCXImhxHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### English testing"
      ],
      "metadata": {
        "id": "SBGkVmHageFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/English_cleaned.csv')\n",
        "X = df['normalized_tweet']\n",
        "y = df['target']"
      ],
      "metadata": {
        "id": "j5D_s8ktgja6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Random Forest"
      ],
      "metadata": {
        "id": "zs7P71LSppTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train+validation and test sets first (80% train_val, 20% test)\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Then split train+validation into train and validation sets (80/20 split of train_val)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val, test_size=0.25, random_state=42, stratify=y_train_val\n",
        ")  # 0.25 x 0.8 = 0.2, so final: 60% train, 20% val, 20% test\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_val_tfidf = vectorizer.transform(X_val)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,  # Number of trees\n",
        "    random_state=42,\n",
        "    n_jobs=-1  # Use all cores for training\n",
        ")\n",
        "rf_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_train_pred = rf_model.predict(X_train_tfidf)\n",
        "y_val_pred = rf_model.predict(X_val_tfidf)\n",
        "y_test_pred = rf_model.predict(X_test_tfidf)\n",
        "\n",
        "# Accuracy calculations\n",
        "train_acc = accuracy_score(y_train, y_train_pred)\n",
        "val_acc = accuracy_score(y_val, y_val_pred)\n",
        "test_acc = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "# Output results\n",
        "print(f\"Random Forest Train Accuracy: {train_acc:.4f}\")\n",
        "print(f\"Random Forest Validation Accuracy: {val_acc:.4f}\")\n",
        "print(f\"Random Forest Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    RandomForestClassifier(random_state=42, n_jobs=-1),\n",
        "    param_grid,\n",
        "    cv=3,\n",
        "    scoring='accuracy',\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit grid search to the training data\n",
        "grid_search.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Best parameters and model\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "best_rf_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate tuned model\n",
        "y_train_pred_tuned = best_rf_model.predict(X_train_tfidf)\n",
        "y_val_pred_tuned = best_rf_model.predict(X_val_tfidf)\n",
        "y_test_pred_tuned = best_rf_model.predict(X_test_tfidf)\n",
        "\n",
        "# Accuracy calculations for tuned model\n",
        "train_acc_tuned = accuracy_score(y_train, y_train_pred_tuned)\n",
        "val_acc_tuned = accuracy_score(y_val, y_val_pred_tuned)\n",
        "test_acc_tuned = accuracy_score(y_test, y_test_pred_tuned)\n",
        "\n",
        "# Output tuned model results\n",
        "print(f\"TUNED Random Forest Train Accuracy: {train_acc_tuned:.4f}\")\n",
        "print(f\"TUNED Random Forest Validation Accuracy: {val_acc_tuned:.4f}\")\n",
        "print(f\"TUNED Random Forest Test Accuracy: {test_acc_tuned:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "wEN7lB1vppki",
        "outputId": "a4269e7e-0991-47b1-b847-f655be067651"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Train Accuracy: 0.7924\n",
            "SVM Validation Accuracy: 0.7255\n",
            "SVM Test Accuracy: 0.7256\n",
            "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-7eba54e73d18>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# Fit grid search to training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0msvm_grid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m# Best parameters and model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1022\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1569\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1571\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    968\u001b[0m                     )\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    971\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    972\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2005\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2007\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 (self._jobs[0].get_status(\n\u001b[1;32m   1761\u001b[0m                     timeout=self.timeout) == TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) SVM"
      ],
      "metadata": {
        "id": "EpiSicYDpqMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train+validation and test sets first (80% train_val, 20% test)\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Then split train+validation into train and validation sets (80/20 split of train_val)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val, test_size=0.25, random_state=42, stratify=y_train_val\n",
        ")  # Final: 60% train, 20% val, 20% test\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_val_tfidf = vectorizer.transform(X_val)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Linear SVM Model\n",
        "svm_model = LinearSVC(max_iter=10000, random_state=42)  # Increased max_iter for stability\n",
        "svm_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_train_pred = svm_model.predict(X_train_tfidf)\n",
        "y_val_pred = svm_model.predict(X_val_tfidf)\n",
        "y_test_pred = svm_model.predict(X_test_tfidf)\n",
        "\n",
        "# Accuracy calculations\n",
        "train_acc = accuracy_score(y_train, y_train_pred)\n",
        "val_acc = accuracy_score(y_val, y_val_pred)\n",
        "test_acc = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "# Output results\n",
        "print(f\"SVM Train Accuracy: {train_acc:.4f}\")\n",
        "print(f\"SVM Validation Accuracy: {val_acc:.4f}\")\n",
        "print(f\"SVM Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Define parameter grid\n",
        "svm_param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'loss': ['hinge', 'squared_hinge'],\n",
        "    'max_iter': [20000, 30000]\n",
        "}\n",
        "\n",
        "# Set up GridSearchCV\n",
        "svm_grid_search = GridSearchCV(\n",
        "    LinearSVC(random_state=42),\n",
        "    svm_param_grid,\n",
        "    cv=3,\n",
        "    scoring='accuracy',\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit grid search to training data\n",
        "svm_grid_search.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Best parameters and model\n",
        "print(\"Best SVM Hyperparameters:\", svm_grid_search.best_params_)\n",
        "best_svm_model = svm_grid_search.best_estimator_\n",
        "\n",
        "# Predictions using the tuned model\n",
        "y_train_pred_tuned = best_svm_model.predict(X_train_tfidf)\n",
        "y_val_pred_tuned = best_svm_model.predict(X_val_tfidf)\n",
        "y_test_pred_tuned = best_svm_model.predict(X_test_tfidf)\n",
        "\n",
        "# Accuracy calculations for tuned model\n",
        "train_acc_tuned = accuracy_score(y_train, y_train_pred_tuned)\n",
        "val_acc_tuned = accuracy_score(y_val, y_val_pred_tuned)\n",
        "test_acc_tuned = accuracy_score(y_test, y_test_pred_tuned)\n",
        "\n",
        "# Output tuned model results\n",
        "print(f\"TUNED SVM Train Accuracy: {train_acc_tuned:.4f}\")\n",
        "print(f\"TUNED SVM Validation Accuracy: {val_acc_tuned:.4f}\")\n",
        "print(f\"TUNED SVM Test Accuracy: {test_acc_tuned:.4f}\")"
      ],
      "metadata": {
        "id": "yxRb4lNYpqZp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "outputId": "2f60208e-2310-4ff7-f9e2-dc7ddd427a6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-7eba54e73d18>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mX_train_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mX_val_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mX_test_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Linear SVM Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   2126\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"The TF-IDF vectorizer is not fitted\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2128\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2129\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m         \u001b[0;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1421\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1422\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1306\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         )\n\u001b[0;32m-> 1308\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1309\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/_compressed.py\u001b[0m in \u001b[0;36msort_indices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_sorted_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1218\u001b[0;31m             _sparsetools.csr_sort_indices(len(self.indptr) - 1, self.indptr,\n\u001b[0m\u001b[1;32m   1219\u001b[0m                                           self.indices, self.data)\n\u001b[1;32m   1220\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_sorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Logistic Regression"
      ],
      "metadata": {
        "id": "incM8HpcpBQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train+validation and test sets first (80% train_val, 20% test)\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Then split train+validation into train and validation sets (80/20 split of train_val)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val, test_size=0.25, random_state=42, stratify=y_train_val\n",
        ")  # 0.25 x 0.8 = 0.2, so final: 60% train, 20% val, 20% test\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_val_tfidf = vectorizer.transform(X_val)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Logistic Regression Model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_train_pred = model.predict(X_train_tfidf)\n",
        "y_val_pred = model.predict(X_val_tfidf)\n",
        "y_test_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "# Accuracy calculations\n",
        "train_acc = accuracy_score(y_train, y_train_pred)\n",
        "val_acc = accuracy_score(y_val, y_val_pred)\n",
        "test_acc = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "# Output results\n",
        "print(f\"Train Accuracy: {train_acc:.4f}\")\n",
        "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l2'],\n",
        "    'solver': ['lbfgs'],\n",
        "    'max_iter': [1000]\n",
        "}\n",
        "\n",
        "# Grid search setup\n",
        "grid_search = GridSearchCV(\n",
        "    LogisticRegression(),\n",
        "    param_grid,\n",
        "    cv=3,\n",
        "    scoring='accuracy',\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Train grid search on training data\n",
        "grid_search.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Best parameters and model\n",
        "print(\"Best Logistic Regression Parameters:\", grid_search.best_params_)\n",
        "best_lr_model = grid_search.best_estimator_\n",
        "\n",
        "# Predictions using tuned model\n",
        "y_train_pred_tuned = best_lr_model.predict(X_train_tfidf)\n",
        "y_val_pred_tuned = best_lr_model.predict(X_val_tfidf)\n",
        "y_test_pred_tuned = best_lr_model.predict(X_test_tfidf)\n",
        "\n",
        "# Accuracy calculations for tuned model\n",
        "train_acc_tuned = accuracy_score(y_train, y_train_pred_tuned)\n",
        "val_acc_tuned = accuracy_score(y_val, y_val_pred_tuned)\n",
        "test_acc_tuned = accuracy_score(y_test, y_test_pred_tuned)\n",
        "\n",
        "# Output tuned model results\n",
        "print(f\"TUNED Logistic Regression Train Accuracy: {train_acc_tuned:.4f}\")\n",
        "print(f\"TUNED Logistic Regression Validation Accuracy: {val_acc_tuned:.4f}\")\n",
        "print(f\"TUNED Logistic Regression Test Accuracy: {test_acc_tuned:.4f}\")"
      ],
      "metadata": {
        "id": "LFjGxRnvo48l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Naive Bayes"
      ],
      "metadata": {
        "id": "nko6spbOqD0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train+validation and test sets first (80% train_val, 20% test)\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Then split train+validation into train and validation sets (80/20 split of train_val)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val, test_size=0.25, random_state=42, stratify=y_train_val\n",
        ")  # Final: 60% train, 20% val, 20% test\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_val_tfidf = vectorizer.transform(X_val)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Naive Bayes Model\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_train_pred = nb_model.predict(X_train_tfidf)\n",
        "y_val_pred = nb_model.predict(X_val_tfidf)\n",
        "y_test_pred = nb_model.predict(X_test_tfidf)\n",
        "\n",
        "# Accuracy calculations\n",
        "train_acc = accuracy_score(y_train, y_train_pred)\n",
        "val_acc = accuracy_score(y_val, y_val_pred)\n",
        "test_acc = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "# Output results\n",
        "print(f\"Naive Bayes Train Accuracy: {train_acc:.4f}\")\n",
        "print(f\"Naive Bayes Validation Accuracy: {val_acc:.4f}\")\n",
        "print(f\"Naive Bayes Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Parameter grid for Naive Bayes\n",
        "nb_param_grid = {\n",
        "    'alpha': [0.01, 0.1, 0.5, 1.0, 2.0]  # Laplace smoothing\n",
        "}\n",
        "\n",
        "# Set up GridSearchCV\n",
        "nb_grid_search = GridSearchCV(\n",
        "    MultinomialNB(),\n",
        "    nb_param_grid,\n",
        "    cv=3,\n",
        "    scoring='accuracy',\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit grid search on training data\n",
        "nb_grid_search.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Best parameters and model\n",
        "print(\"Best Naive Bayes Hyperparameters:\", nb_grid_search.best_params_)\n",
        "best_nb_model = nb_grid_search.best_estimator_\n",
        "\n",
        "# Predictions using tuned model\n",
        "y_train_pred_tuned = best_nb_model.predict(X_train_tfidf)\n",
        "y_val_pred_tuned = best_nb_model.predict(X_val_tfidf)\n",
        "y_test_pred_tuned = best_nb_model.predict(X_test_tfidf)\n",
        "\n",
        "# Accuracy calculations for tuned model\n",
        "train_acc_tuned = accuracy_score(y_train, y_train_pred_tuned)\n",
        "val_acc_tuned = accuracy_score(y_val, y_val_pred_tuned)\n",
        "test_acc_tuned = accuracy_score(y_test, y_test_pred_tuned)\n",
        "\n",
        "# Output tuned model results\n",
        "print(f\"TUNED Naive Bayes Train Accuracy: {train_acc_tuned:.4f}\")\n",
        "print(f\"TUNED Naive Bayes Validation Accuracy: {val_acc_tuned:.4f}\")\n",
        "print(f\"TUNED Naive Bayes Test Accuracy: {test_acc_tuned:.4f}\")"
      ],
      "metadata": {
        "id": "1eV8QfV4qD_l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}