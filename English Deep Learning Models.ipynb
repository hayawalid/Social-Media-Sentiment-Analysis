{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f45b859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Dense Feedforward...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Social Media Sentiment Analysis\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7813/7813 - 124s - 16ms/step - accuracy: 0.7660 - loss: 0.4900 - val_accuracy: 0.7769 - val_loss: 0.4677\n",
      "Epoch 2/5\n",
      "7813/7813 - 151s - 19ms/step - accuracy: 0.7837 - loss: 0.4621 - val_accuracy: 0.7791 - val_loss: 0.4639\n",
      "Epoch 3/5\n",
      "7813/7813 - 152s - 19ms/step - accuracy: 0.7912 - loss: 0.4485 - val_accuracy: 0.7794 - val_loss: 0.4635\n",
      "Epoch 4/5\n",
      "7813/7813 - 149s - 19ms/step - accuracy: 0.7983 - loss: 0.4367 - val_accuracy: 0.7791 - val_loss: 0.4657\n",
      "Epoch 5/5\n",
      "7813/7813 - 189s - 24ms/step - accuracy: 0.8043 - loss: 0.4264 - val_accuracy: 0.7791 - val_loss: 0.4670\n",
      "\u001b[1m8681/8681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step\n",
      "Dense Feedforward Accuracy: 0.7795\n",
      "Dense Feedforward F1 Score: 0.7794\n",
      "\n",
      "Training 1D CNN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Social Media Sentiment Analysis\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7813/7813 - 661s - 85ms/step - accuracy: 0.7675 - loss: 0.4832 - val_accuracy: 0.7791 - val_loss: 0.4652\n",
      "Epoch 2/5\n",
      "7813/7813 - 674s - 86ms/step - accuracy: 0.7851 - loss: 0.4560 - val_accuracy: 0.7842 - val_loss: 0.4625\n",
      "Epoch 3/5\n",
      "7813/7813 - 691s - 88ms/step - accuracy: 0.7926 - loss: 0.4425 - val_accuracy: 0.7839 - val_loss: 0.4609\n",
      "Epoch 4/5\n",
      "7813/7813 - 656s - 84ms/step - accuracy: 0.7998 - loss: 0.4306 - val_accuracy: 0.7838 - val_loss: 0.4595\n",
      "Epoch 5/5\n",
      "7813/7813 - 647s - 83ms/step - accuracy: 0.8060 - loss: 0.4197 - val_accuracy: 0.7818 - val_loss: 0.4640\n",
      "\u001b[1m8681/8681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 8ms/step\n",
      "1D CNN Accuracy: 0.7835\n",
      "1D CNN F1 Score: 0.7835\n",
      "\n",
      "Training BiLSTM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Social Media Sentiment Analysis\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7813/7813 - 2281s - 292ms/step - accuracy: 0.7720 - loss: 0.4764 - val_accuracy: 0.7834 - val_loss: 0.4576\n",
      "Epoch 2/5\n",
      "7813/7813 - 2218s - 284ms/step - accuracy: 0.7897 - loss: 0.4489 - val_accuracy: 0.7877 - val_loss: 0.4502\n",
      "Epoch 3/5\n",
      "7813/7813 - 2051s - 263ms/step - accuracy: 0.7974 - loss: 0.4355 - val_accuracy: 0.7882 - val_loss: 0.4501\n",
      "Epoch 4/5\n",
      "7813/7813 - 2057s - 263ms/step - accuracy: 0.8038 - loss: 0.4241 - val_accuracy: 0.7885 - val_loss: 0.4505\n",
      "Epoch 5/5\n",
      "7813/7813 - 2050s - 262ms/step - accuracy: 0.8099 - loss: 0.4134 - val_accuracy: 0.7874 - val_loss: 0.4547\n",
      "\u001b[1m8681/8681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m280s\u001b[0m 32ms/step\n",
      "BiLSTM Accuracy: 0.7887\n",
      "BiLSTM F1 Score: 0.7886\n",
      "\n",
      "\n",
      "=== Results Summary ===\n",
      "BiLSTM: Accuracy = 0.7887, F1 = 0.7886\n",
      "1D CNN: Accuracy = 0.7835, F1 = 0.7835\n",
      "Dense Feedforward: Accuracy = 0.7795, F1 = 0.7794\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, Dropout, GlobalMaxPool1D, Conv1D, LSTM, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load and clean data\n",
    "df = pd.read_csv(\"English_cleaned.csv\")\n",
    "\n",
    "def fix_nested_char_tokens(row):\n",
    "    if isinstance(row, str):\n",
    "        row = ast.literal_eval(row)\n",
    "    flat = []\n",
    "    for token in row:\n",
    "        if isinstance(token, list):\n",
    "            joined = ''.join([c for c in token if c.isalpha()])\n",
    "            if joined:\n",
    "                flat.append(joined)\n",
    "        elif isinstance(token, str):\n",
    "            cleaned = ''.join(token.split())\n",
    "            if cleaned:\n",
    "                flat.append(cleaned)\n",
    "    return flat\n",
    "\n",
    "df['tokens'] = df['tokens'].apply(fix_nested_char_tokens)\n",
    "df['cleanedtext'] = df['tokens'].apply(lambda x: ' '.join(x))\n",
    "df = df[df['cleanedtext'].str.strip() != '']\n",
    "df = df.dropna(subset=['cleanedtext'])\n",
    "\n",
    "# Tokenization and Padding\n",
    "max_words = 10000\n",
    "max_len = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df['cleanedtext'])\n",
    "X = tokenizer.texts_to_sequences(df['cleanedtext'])\n",
    "X = pad_sequences(X, maxlen=max_len)\n",
    "\n",
    "# Labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['target'])  # -1 → 0, 1 → 1\n",
    "y_cat = to_categorical(y)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_cat, test_size=0.2, random_state=42, stratify=y_cat)\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "# ========== Model 1: Dense Feedforward ==========\n",
    "def create_dense_model():\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=max_words, output_dim=64, input_length=max_len),\n",
    "        GlobalMaxPool1D(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(2, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ========== Model 2: 1D CNN ==========\n",
    "def create_cnn_model():\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=max_words, output_dim=64, input_length=max_len),\n",
    "        Conv1D(128, 5, activation='relu'),\n",
    "        GlobalMaxPool1D(),\n",
    "        Dropout(0.5),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(2, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ========== Model 3: Bidirectional LSTM ==========\n",
    "def create_bilstm_model():\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=max_words, output_dim=64, input_length=max_len),\n",
    "        Bidirectional(LSTM(64, return_sequences=False)),\n",
    "        Dropout(0.5),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(2, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ========== Training and Evaluation Function ==========\n",
    "def train_and_evaluate(model_fn, name):\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    model = model_fn()\n",
    "    model.fit(X_train, y_train, epochs=5, batch_size=128, validation_split=0.1, callbacks=[early_stop], verbose=2)\n",
    "\n",
    "    y_pred_probs = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    print(f\"{name} Accuracy: {acc:.4f}\")\n",
    "    print(f\"{name} F1 Score: {f1:.4f}\")\n",
    "    return model, acc, f1\n",
    "\n",
    "# ========== Run All Models ==========\n",
    "results = []\n",
    "models = [\n",
    "    (\"Dense Feedforward\", create_dense_model),\n",
    "    (\"1D CNN\", create_cnn_model),\n",
    "    (\"BiLSTM\", create_bilstm_model)\n",
    "]\n",
    "\n",
    "for name, fn in models:\n",
    "    model, acc, f1 = train_and_evaluate(fn, name)\n",
    "    results.append((name, acc, f1))\n",
    "\n",
    "# ========== Print Summary ==========\n",
    "print(\"\\n\\n=== Results Summary ===\")\n",
    "for name, acc, f1 in sorted(results, key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{name}: Accuracy = {acc:.4f}, F1 = {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a3552fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm_model = model  # 'model' refers to the last trained model\n",
    "bilstm_model.save(\"bilstm_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e6bf098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sample_text(sample_text, model, tokenizer, label_encoder, threshold=0.2):\n",
    "    # Preprocess input\n",
    "    sequence = tokenizer.texts_to_sequences([sample_text])\n",
    "    padded = pad_sequences(sequence, maxlen=max_len)\n",
    "\n",
    "    # Predict\n",
    "    pred_prob = model.predict(padded)\n",
    "    pred_class = np.argmax(pred_prob, axis=1)[0]\n",
    "    original_label = label_encoder.inverse_transform([pred_class])[0]\n",
    "\n",
    "    # Calculate confidence gap between the top two probabilities\n",
    "    sorted_probs = sorted(pred_prob[0], reverse=True)\n",
    "    confidence_gap = sorted_probs[0] - sorted_probs[1]\n",
    "\n",
    "    # Map to string labels with neutral for close probabilities\n",
    "    if confidence_gap < threshold:\n",
    "        sentiment = \"neutral\"\n",
    "    else:\n",
    "        sentiment = \"positive\" if original_label == 1 else \"negative\"\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Input Text: {sample_text}\")\n",
    "    print(f\"Predicted Sentiment: {sentiment}\")\n",
    "    print(f\"Predicted Probabilities: {pred_prob[0]}\")  # Show the probabilities for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d645c54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Input Text: I love this product, it's amazing!\n",
      "Predicted Sentiment: positive\n",
      "Predicted Probabilities: [0.13570344 0.86429656]\n"
     ]
    }
   ],
   "source": [
    "predict_sample_text(\"I love this product, it's amazing!\", bilstm_model, tokenizer, label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "876b9016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "Input Text: I love this product!\n",
      "Predicted Sentiment: positive\n",
      "Predicted Probabilities: [0.07155262 0.9284473 ]\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Input Text: This is the worst experience I've ever had.\n",
      "Predicted Sentiment: negative\n",
      "Predicted Probabilities: [0.7588108  0.24118914]\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Input Text: Absolutely fantastic performance.\n",
      "Predicted Sentiment: positive\n",
      "Predicted Probabilities: [0.3228619 0.6771381]\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "Input Text: Not what I expected, pretty bad.\n",
      "Predicted Sentiment: negative\n",
      "Predicted Probabilities: [0.7433778  0.25662217]\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "Input Text: It was okay, not great but not terrible.\n",
      "Predicted Sentiment: positive\n",
      "Predicted Probabilities: [0.18008696 0.819913  ]\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Input Text: I'm extremely satisfied with the service.\n",
      "Predicted Sentiment: positive\n",
      "Predicted Probabilities: [0.3135766 0.6864234]\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Input Text: Terrible! I want my money back.\n",
      "Predicted Sentiment: negative\n",
      "Predicted Probabilities: [0.78760546 0.21239449]\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step\n",
      "Input Text: Such a pleasant surprise.\n",
      "Predicted Sentiment: positive\n",
      "Predicted Probabilities: [0.20671198 0.793288  ]\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "Input Text: Disappointing result after all the hype.\n",
      "Predicted Sentiment: positive\n",
      "Predicted Probabilities: [0.15269165 0.8473084 ]\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "Input Text: Highly recommend this to everyone!\n",
      "Predicted Sentiment: positive\n",
      "Predicted Probabilities: [0.08310127 0.91689867]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_samples = [\n",
    "    \"I love this product!\",\n",
    "    \"This is the worst experience I've ever had.\",\n",
    "    \"Absolutely fantastic performance.\",\n",
    "    \"Not what I expected, pretty bad.\",\n",
    "    \"It was okay, not great but not terrible.\",\n",
    "    \"I'm extremely satisfied with the service.\",\n",
    "    \"Terrible! I want my money back.\",\n",
    "    \"Such a pleasant surprise.\",\n",
    "    \"Disappointing result after all the hype.\",\n",
    "    \"Highly recommend this to everyone!\"\n",
    "]\n",
    "\n",
    "for text in test_samples:\n",
    "    predict_sample_text(text, bilstm_model, tokenizer, label_encoder)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "31987dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "Input Text: This movie was a masterpiece, I loved every second of it!\n",
      "Predicted Sentiment: positive\n",
      "Predicted Probabilities: [0.31940058 0.6805994 ]\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "Input Text: I can't believe how bad the food was. It was cold and tasteless.\n",
      "Predicted Sentiment: negative\n",
      "Predicted Probabilities: [0.63131195 0.36868805]\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Input Text: The event was well organized and enjoyable. I had a great time.\n",
      "Predicted Sentiment: positive\n",
      "Predicted Probabilities: [0.09833427 0.9016658 ]\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Input Text: I’m really disappointed with the quality of this product. It broke after one use.\n",
      "Predicted Sentiment: negative\n",
      "Predicted Probabilities: [0.72360665 0.2763934 ]\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Input Text: The hotel was nice, but the staff could have been more helpful.\n",
      "Predicted Sentiment: positive\n",
      "Predicted Probabilities: [0.31547868 0.6845214 ]\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Input Text: It’s a decent book, but the ending didn’t live up to the expectations.\n",
      "Predicted Sentiment: neutral\n",
      "Predicted Probabilities: [0.44461355 0.5553864 ]\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "Input Text: I had such an amazing time at the concert. Highly recommended!\n",
      "Predicted Sentiment: positive\n",
      "Predicted Probabilities: [0.34229836 0.6577016 ]\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Input Text: I’m never buying from this store again. Terrible customer service.\n",
      "Predicted Sentiment: neutral\n",
      "Predicted Probabilities: [0.4194034  0.58059657]\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Input Text: The app is okay but keeps crashing. Needs improvement.\n",
      "Predicted Sentiment: neutral\n",
      "Predicted Probabilities: [0.4858915 0.5141085]\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Input Text: Such a lovely day at the park, the weather was perfect!\n",
      "Predicted Sentiment: positive\n",
      "Predicted Probabilities: [0.1534348  0.84656525]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_samples = [\n",
    "    \"This movie was a masterpiece, I loved every second of it!\",\n",
    "    \"I can't believe how bad the food was. It was cold and tasteless.\",\n",
    "    \"The event was well organized and enjoyable. I had a great time.\",\n",
    "    \"I’m really disappointed with the quality of this product. It broke after one use.\",\n",
    "    \"The hotel was nice, but the staff could have been more helpful.\",\n",
    "    \"It’s a decent book, but the ending didn’t live up to the expectations.\",\n",
    "    \"I had such an amazing time at the concert. Highly recommended!\",\n",
    "    \"I’m never buying from this store again. Terrible customer service.\",\n",
    "    \"The app is okay but keeps crashing. Needs improvement.\",\n",
    "    \"Such a lovely day at the park, the weather was perfect!\"\n",
    "]\n",
    "\n",
    "for text in test_samples:\n",
    "    predict_sample_text(text, bilstm_model, tokenizer, label_encoder)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f43466c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "Input Text: The meeting started at 10 AM and ended on time.\n",
      "Predicted Sentiment: positive\n",
      "Predicted Probabilities: [0.29524964 0.70475036]\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "Input Text: I received the package yesterday.\n",
      "Predicted Sentiment: neutral\n",
      "Predicted Probabilities: [0.43696126 0.56303877]\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "Input Text: It's just another regular day.\n",
      "Predicted Sentiment: neutral\n",
      "Predicted Probabilities: [0.40240726 0.59759265]\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "Input Text: The results were as expected.\n",
      "Predicted Sentiment: positive\n",
      "Predicted Probabilities: [0.3144089  0.68559116]\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "Input Text: I tried the product; it works as described.\n",
      "Predicted Sentiment: positive\n",
      "Predicted Probabilities: [0.24113752 0.7588625 ]\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "Input Text: This version is different, but I’m still getting used to it.\n",
      "Predicted Sentiment: neutral\n",
      "Predicted Probabilities: [0.5912704  0.40872958]\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "Input Text: There was traffic on the way home, like usual.\n",
      "Predicted Sentiment: negative\n",
      "Predicted Probabilities: [0.63113165 0.36886838]\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Input Text: The movie was neither good nor bad, just average.\n",
      "Predicted Sentiment: negative\n",
      "Predicted Probabilities: [0.6386236 0.3613764]\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Input Text: I used the app. It functions like most others.\n",
      "Predicted Sentiment: positive\n",
      "Predicted Probabilities: [0.33455297 0.66544706]\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
      "Input Text: Not much to say about this experience.\n",
      "Predicted Sentiment: positive\n",
      "Predicted Probabilities: [0.3943817 0.6056183]\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Input Text: The service was acceptable, nothing special.\n",
      "Predicted Sentiment: positive\n",
      "Predicted Probabilities: [0.13598569 0.8640143 ]\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Input Text: The product came in standard packaging.\n",
      "Predicted Sentiment: positive\n",
      "Predicted Probabilities: [0.22639371 0.77360636]\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Input Text: The presentation covered all the required topics.\n",
      "Predicted Sentiment: positive\n",
      "Predicted Probabilities: [0.19640686 0.80359316]\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "Input Text: The quality is fine for the price.\n",
      "Predicted Sentiment: neutral\n",
      "Predicted Probabilities: [0.4162958 0.5837042]\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Input Text: It looks okay. Nothing stands out.\n",
      "Predicted Sentiment: positive\n",
      "Predicted Probabilities: [0.3569011 0.6430989]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "neutral_test_samples = [\n",
    "    \"The meeting started at 10 AM and ended on time.\",\n",
    "    \"I received the package yesterday.\",\n",
    "    \"It's just another regular day.\",\n",
    "    \"The results were as expected.\",\n",
    "    \"I tried the product; it works as described.\",\n",
    "    \"This version is different, but I’m still getting used to it.\",\n",
    "    \"There was traffic on the way home, like usual.\",\n",
    "    \"The movie was neither good nor bad, just average.\",\n",
    "    \"I used the app. It functions like most others.\",\n",
    "    \"Not much to say about this experience.\",\n",
    "    \"The service was acceptable, nothing special.\",\n",
    "    \"The product came in standard packaging.\",\n",
    "    \"The presentation covered all the required topics.\",\n",
    "    \"The quality is fine for the price.\",\n",
    "    \"It looks okay. Nothing stands out.\",\n",
    "]\n",
    "\n",
    "for text in neutral_test_samples:\n",
    "    predict_sample_text(text, bilstm_model, tokenizer, label_encoder)\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
