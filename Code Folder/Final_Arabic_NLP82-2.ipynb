{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_7H3wtBbgA5",
        "outputId": "ca49e23f-8c4f-4844-813e-302e9d027b9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FN3zVnh60cQO",
        "outputId": "d1fcdb16-4335-4ed2-e3ea-084075db0bd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting pyarabic\n",
            "  Downloading PyArabic-0.6.15-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install gensim pyarabic emoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xs7KZEEi0sgT",
        "outputId": "a1b094c4-1f97-4589-cebc-5913e5f316df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install numpy==1.26.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GD-DJXD-X3B9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "f7c56a5e-9661-4c7b-c94f-2502d4f3be45"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-1dfca57bdb8a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# import gensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyarabic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maraby\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Check the dependencies satisfy the minimal versions required.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdependency_versions_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m from .utils import (\n\u001b[1;32m     28\u001b[0m     \u001b[0mOptionalDependencyNotAvailable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/dependency_versions_check.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdependency_versions_table\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequire_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_version_core\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackbone_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBackboneConfigMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBackboneMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mchat_template_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocstringParsingException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeHintParsingException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_json_schema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconstants\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIMAGENET_DEFAULT_MEAN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGENET_DEFAULT_STD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGENET_STANDARD_MEAN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGENET_STANDARD_STD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m from .doc import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/chat_template_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_lock_unlock_module\u001b[0;34m(name)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import gensim\n",
        "from transformers import AutoTokenizer\n",
        "import nltk\n",
        "from pyarabic import araby as ar\n",
        "# from Stemmer import Stemmer\n",
        "# import Stemmer\n",
        "\n",
        "# NLTK\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LM-EHbvcysQ"
      },
      "outputs": [],
      "source": [
        "# Standard Library\n",
        "# !pip install numpy==1.26.4 pandas==2.2.2 scipy==1.13.1 gensim==4.3.2 tensorflow==2.18.0\n",
        "\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import re\n",
        "import string\n",
        "import emoji\n",
        "import nltk\n",
        "import scipy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gensim\n",
        "import tensorflow as tf\n",
        "from pyarabic import araby as ar\n",
        "# from Stemmer import Stemmer\n",
        "# import Stemmer\n",
        "\n",
        "# NLTK\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Scikit-Learn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# TensorFlow/Keras\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Concatenate, Embedding, Input, LSTM\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.optimizers import Adam, Nadam, RMSprop\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Transformers\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "5yZi9AXLYB0z",
        "outputId": "2df0db78-b5c0-45e5-cc89-f4d3208796cb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        label                                            content\n",
              "0           1  النعال المريحة: أرتدي هذه النعال كثيرًا!فهي دا...\n",
              "1           1  منتج جميل ، خدمة سيئة: لقد اشتريت زوجًا من الن...\n",
              "2           1  جيد للأشياء الصغيرة: هذا يعمل بشكل جيد لالتقاط...\n",
              "3           0  واهية للغاية: flimsyif للغاية ، فأنت تشتريه ، ...\n",
              "4           1  Pop for Girls and Girly Boys ، والأشخاص الذين ...\n",
              "...       ...                                                ...\n",
              "329995      0  DOA: فتح العلامة التجارية الجديدة من Box.تم تث...\n",
              "329996      0  شركة صعبة التعامل معها: المنتج كان على ما يرام...\n",
              "329997      0  SDK Sansa Leather Case: فقير للغاية.لم يتم الإ...\n",
              "329998      0  حسنًا ، لكن ليس رائعًا: حسنًا ، لقد اشتريت هذا...\n",
              "329999      1  مريحة جدا!: هذه النعال رائعة!أنها ناعمة جدا وم...\n",
              "\n",
              "[330000 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-637e4759-32fa-403c-9d37-ede143791a7e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>النعال المريحة: أرتدي هذه النعال كثيرًا!فهي دا...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>منتج جميل ، خدمة سيئة: لقد اشتريت زوجًا من الن...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>جيد للأشياء الصغيرة: هذا يعمل بشكل جيد لالتقاط...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>واهية للغاية: flimsyif للغاية ، فأنت تشتريه ، ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>Pop for Girls and Girly Boys ، والأشخاص الذين ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>329995</th>\n",
              "      <td>0</td>\n",
              "      <td>DOA: فتح العلامة التجارية الجديدة من Box.تم تث...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>329996</th>\n",
              "      <td>0</td>\n",
              "      <td>شركة صعبة التعامل معها: المنتج كان على ما يرام...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>329997</th>\n",
              "      <td>0</td>\n",
              "      <td>SDK Sansa Leather Case: فقير للغاية.لم يتم الإ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>329998</th>\n",
              "      <td>0</td>\n",
              "      <td>حسنًا ، لكن ليس رائعًا: حسنًا ، لقد اشتريت هذا...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>329999</th>\n",
              "      <td>1</td>\n",
              "      <td>مريحة جدا!: هذه النعال رائعة!أنها ناعمة جدا وم...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>330000 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-637e4759-32fa-403c-9d37-ede143791a7e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-637e4759-32fa-403c-9d37-ede143791a7e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-637e4759-32fa-403c-9d37-ede143791a7e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-86cebeab-6e6c-4190-ac40-ac97cb91e5f4\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-86cebeab-6e6c-4190-ac40-ac97cb91e5f4')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-86cebeab-6e6c-4190-ac40-ac97cb91e5f4 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_bf6638b6-526c-4135-abac-c0308a144fa9\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_bf6638b6-526c-4135-abac-c0308a144fa9 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Data Science/Arabic Sentiment/Datasets/arabic_sentiment_reviews.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5MHwJaQYAqK"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define Arabic and English punctuations\n",
        "arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n",
        "english_punctuations = string.punctuation\n",
        "punctuations_list = arabic_punctuations + english_punctuations\n",
        "\n",
        "\n",
        "# Load and filter Arabic stopwords\n",
        "stop_words = set(stopwords.words('arabic'))\n",
        "negation_words = {'لا', 'لم', 'لن', 'ما', 'ليس', 'بدون', 'غير'}\n",
        "stop_words = stop_words.difference(negation_words)\n",
        "\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "def remove_gibberish(text):\n",
        "    arabic_pattern = re.compile(r'[^\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FF\\uFB50-\\uFDFF\\uFE70-\\uFEFF\\s.,!?؟]')\n",
        "    return arabic_pattern.sub('', text)\n",
        "\n",
        "def remove_diacritics(text):\n",
        "    return ar.strip_tashkeel(text)\n",
        "\n",
        "def remove_tatweel(text):\n",
        "    return ar.strip_tatweel(text)\n",
        "\n",
        "def remove_emoji(text):\n",
        "    return emoji.replace_emoji(text, replace='')\n",
        "\n",
        "def normalize_elongation(text):\n",
        "    return re.sub(r'(.)\\1+', r'\\1\\1', text)\n",
        "\n",
        "def normalize_letters(text):\n",
        "    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n",
        "    text = re.sub(\"ى\", \"ي\", text)\n",
        "    text = re.sub(\"ؤ\", \"و\", text)\n",
        "    text = re.sub(\"ئ\", \"ي\", text)\n",
        "    return text\n",
        "\n",
        "def clean_punctuation(text):\n",
        "    keep = '،.؟!'  # punctuation to keep\n",
        "    # Replace all punctuation (excluding \"keep\") with space\n",
        "    pattern = rf'[{\"\".join(re.escape(c) for c in punctuations_list if c not in keep)}]'\n",
        "    text = re.sub(pattern, ' ', text)\n",
        "    text = re.sub(r'\\.{2,}', '.', text)  # Normalize multiple dots\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Normalize spaces\n",
        "    return text\n",
        "\n",
        "def remove_noise(text):\n",
        "    text = re.sub(r'http\\S+|www.\\S+', '', text)  # URLs\n",
        "    text = re.sub(r'@\\w+', '', text)            # Mentions\n",
        "    text = re.sub(r'#\\w+', '', text)            # Hashtags\n",
        "    text = re.sub(r'\\d+', '', text)             # Numbers\n",
        "    text = re.sub(r'[a-zA-Z]', '', text)        # English letters\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()    # Extra spaces\n",
        "    return text\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    text = text.strip()\n",
        "    text = remove_emoji(text)\n",
        "    text = remove_noise(text)\n",
        "    text = clean_punctuation(text)\n",
        "    text = remove_gibberish(text)\n",
        "    text = remove_diacritics(text)\n",
        "    text = remove_tatweel(text)\n",
        "    text = normalize_letters(text)\n",
        "    text = normalize_elongation(text)\n",
        "    text = remove_stopwords(text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "a75Kh0Amfg6J"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y openjdk-8-jdk\n",
        "!pip install farasapy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieU29-nndSqM"
      },
      "outputs": [],
      "source": [
        "from farasa.segmenter import FarasaSegmenter\n",
        "from farasa.stemmer import FarasaStemmer\n",
        "from farasa.pos import FarasaPOSTagger\n",
        "from farasa.ner import FarasaNamedEntityRecognizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCHaDe58dk6Z"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import gensim\n",
        "from transformers import AutoTokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-h03BGfuYNgr"
      },
      "outputs": [],
      "source": [
        "farasa_segmenter = FarasaSegmenter(interactive=True)\n",
        "farasa_stemmer = FarasaStemmer(interactive=True)\n",
        "\n",
        "# Initialize paths\n",
        "DATA_DIR = \"/content/drive/MyDrive/Data Science/Arabic Sentiment/\"\n",
        "MODELS_DIR = os.path.join(DATA_DIR, \"Models2/\")\n",
        "FEATURES_DIR = os.path.join(DATA_DIR, \"Features/\")\n",
        "DATASETS_DIR = os.path.join(DATA_DIR, \"Datasets2/\")\n",
        "\n",
        "# Ensure directories exist\n",
        "for dir_path in [MODELS_DIR, FEATURES_DIR, DATASETS_DIR]:\n",
        "    os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "\n",
        "# 1. Function to clean and tokenize text using Farasa\n",
        "def clean_and_tokenize(texts):\n",
        "    print(\"Cleaning and tokenizing texts...\")\n",
        "    cleaned_texts = []\n",
        "    tokenized_texts = []\n",
        "\n",
        "    for text in tqdm(texts):\n",
        "        # Clean first\n",
        "        cleaned = clean_text(text)\n",
        "        cleaned_texts.append(cleaned if cleaned else \"\")\n",
        "\n",
        "        # Then tokenize using Farasa\n",
        "        if cleaned:\n",
        "            # Get segments using Farasa\n",
        "            segments = farasa_segmenter.segment(cleaned).split()\n",
        "            tokenized_texts.append(segments)\n",
        "        else:\n",
        "            tokenized_texts.append([])\n",
        "\n",
        "    return cleaned_texts, tokenized_texts\n",
        "# 3. Function to stem tokenized text using Farasa\n",
        "def stem_with_farasa(tokenized_texts):\n",
        "    print(\"Stemming tokens with Farasa...\")\n",
        "    stemmed_token_lists = []\n",
        "\n",
        "    for tokens in tqdm(tokenized_texts):\n",
        "        if tokens:\n",
        "            # Stem each token individually\n",
        "            stemmed_tokens = []\n",
        "            for token in tokens:\n",
        "                stemmed = farasa_stemmer.stem(token)\n",
        "                stemmed_tokens.append(stemmed)\n",
        "            stemmed_token_lists.append(stemmed_tokens)\n",
        "        else:\n",
        "            stemmed_token_lists.append([])\n",
        "\n",
        "    return stemmed_token_lists\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrqI6A7WYSaW"
      },
      "outputs": [],
      "source": [
        "def embed_with_fasttext(fasttext_model_path, stemmed_tokens_path, embedding_dim=300, output_path=None):\n",
        "    print(\"Loading stemmed tokens...\")\n",
        "    with open(stemmed_tokens_path, 'rb') as f:\n",
        "        stemmed_token_lists = pickle.load(f)\n",
        "\n",
        "    print(\"Loading FastText model...\")\n",
        "    fasttext_model = gensim.models.KeyedVectors.load_word2vec_format(fasttext_model_path)\n",
        "\n",
        "    print(\"Creating FastText embeddings from stemmed tokens...\")\n",
        "    embeddings = []\n",
        "    for tokens in tqdm(stemmed_token_lists):\n",
        "        # Get vectors for tokens that exist in the model\n",
        "        word_vectors = [fasttext_model[word] for word in tokens if word in fasttext_model]\n",
        "        if word_vectors:\n",
        "            # Average the vectors\n",
        "            text_vector = np.mean(word_vectors, axis=0)\n",
        "        else:\n",
        "            # Use zeros if no tokens are in the model\n",
        "            text_vector = np.zeros(embedding_dim)\n",
        "        embeddings.append(text_vector)\n",
        "\n",
        "    embeddings_array = np.array(embeddings)\n",
        "\n",
        "    # Save embeddings if output path is provided\n",
        "    if output_path:\n",
        "        print(f\"Saving embeddings to {output_path}\")\n",
        "        np.save(output_path, embeddings_array)\n",
        "\n",
        "    return embeddings_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzsPPDchlJeL"
      },
      "outputs": [],
      "source": [
        "cleaned_texts, tokenized_texts = clean_and_tokenize(df['content'])\n",
        "df['cleaned_text'] = cleaned_texts\n",
        "df['tokenized_text'] = tokenized_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxGrh3aglZSG"
      },
      "outputs": [],
      "source": [
        "stemmed_token_lists = stem_with_farasa(tokenized_texts)\n",
        "df['stemmed_tokens'] = stemmed_token_lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiJQEVZkIwaW"
      },
      "outputs": [],
      "source": [
        "# Save tokenized data\n",
        "tokenized_path = os.path.join(FEATURES_DIR, \"tokenized_texts.pkl\")\n",
        "with open(tokenized_path, 'wb') as f:\n",
        "    pickle.dump(tokenized_texts, f)\n",
        "print(f\"Tokenized texts saved to {tokenized_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4G6I4KDiI893"
      },
      "outputs": [],
      "source": [
        "# Save stemmed tokens\n",
        "stemmed_path = os.path.join(FEATURES_DIR, \"stemmed_tokens.pkl\")\n",
        "with open(stemmed_path, 'wb') as f:\n",
        "    pickle.dump(stemmed_token_lists, f)\n",
        "print(f\"Stemmed tokens saved to {stemmed_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_TVQQcHp1Kb"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMGPK3Lg8_2e"
      },
      "outputs": [],
      "source": [
        "df.to_csv(os.path.join(DATASETS_DIR, \"Final_arbic_dataset.csv\"), index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVi_yBzTZvAg"
      },
      "outputs": [],
      "source": [
        "newdata = pd.read_csv('/content/drive/MyDrive/Data Science/Arabic Sentiment/Datasets/stemmed_output.csv')\n",
        "newdata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqAAk7GCa6nr"
      },
      "source": [
        "-----------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xy6VIzjBqN8G"
      },
      "outputs": [],
      "source": [
        "# Initialize paths\n",
        "DATA_DIR = \"/content/drive/MyDrive/Data Science/Arabic Sentiment/\"\n",
        "MODELS_DIR = os.path.join(DATA_DIR, \"Models2/\")\n",
        "FEATURES_DIR = os.path.join(DATA_DIR, \"Features/\")\n",
        "DATASETS_DIR = os.path.join(DATA_DIR, \"Datasets2/\")\n",
        "\n",
        "# Ensure directories exist\n",
        "for dir_path in [MODELS_DIR, FEATURES_DIR, DATASETS_DIR]:\n",
        "    os.makedirs(dir_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oftYTpfWbgZ7"
      },
      "outputs": [],
      "source": [
        "fasttext_model_path = os.path.join(DATA_DIR, \"Models/cc.ar.300.vec.gz\")\n",
        "stemmed_tokens_path = os.path.join(FEATURES_DIR, \"stemmed_tokens.pkl\")\n",
        "embeddings_path = os.path.join(FEATURES_DIR, \"fasttext_embeddings.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slt3S_3ZbZzp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pickle\n",
        "import gensim\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define the paths\n",
        "DATA_DIR = \"/content/drive/MyDrive/Data Science/Arabic Sentiment/\"\n",
        "FEATURES_DIR = os.path.join(DATA_DIR, \"Features/\")\n",
        "fasttext_model_path = os.path.join(DATA_DIR, \"Models/cc.ar.300.vec.gz\")\n",
        "stemmed_tokens_path = os.path.join(FEATURES_DIR, \"stemmed_tokens.pkl\")\n",
        "embeddings_path = os.path.join(FEATURES_DIR, \"fasttext_embeddings.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGPD_oRNla2w"
      },
      "outputs": [],
      "source": [
        "# Run the embedding function\n",
        "X = embed_with_fasttext(\n",
        "    fasttext_model_path=fasttext_model_path,\n",
        "    stemmed_tokens_path=stemmed_tokens_path,\n",
        "    output_path=embeddings_path\n",
        ")\n",
        "\n",
        "print(f\"FastText embeddings saved to {embeddings_path}\")\n",
        "print(f\"Embedding shape: {X.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBkC8y7mloNu"
      },
      "outputs": [],
      "source": [
        "# Cell: Save everything in a single file for easy loading\n",
        "training_data = {\n",
        "    'X_fasttext': X\n",
        "}\n",
        "training_data_path = os.path.join(FEATURES_DIR, \"training_data.pkl\")\n",
        "with open(training_data_path, 'wb') as f:\n",
        "    pickle.dump(training_data, f)\n",
        "print(f\"Complete training data saved to {training_data_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjbaWlxc-w26"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/drive/MyDrive/Data Science/Arabic Sentiment/Features/training_data.pkl\", 'rb') as f:\n",
        "    training_data = pickle.load(f)\n",
        "\n",
        "X = training_data['X_fasttext']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5z5cwOp8AsSj",
        "outputId": "9bb33584-ff71-45cd-b0a0-b666982b2856"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.02168182, -0.01280909,  0.01896364, ...,  0.02547273,\n",
              "         0.01144545, -0.0158    ],\n",
              "       [-0.05181778,  0.00087556, -0.00284   , ...,  0.02858222,\n",
              "         0.02779778,  0.01011111],\n",
              "       [-0.0289    , -0.02499091,  0.00394545, ...,  0.02310909,\n",
              "         0.01787273,  0.00412727],\n",
              "       ...,\n",
              "       [-0.05829474,  0.01496842,  0.03438947, ...,  0.01194211,\n",
              "         0.00835263,  0.02857368],\n",
              "       [-0.01589444,  0.00213889, -0.00445555, ...,  0.03727778,\n",
              "         0.02298889, -0.01811111],\n",
              "       [-0.028195  ,  0.0072925 ,  0.0039625 , ...,  0.0098475 ,\n",
              "         0.01218   , -0.007785  ]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yXEq8XkATTU"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(df['label'])\n",
        "y_cat = tf.keras.utils.to_categorical(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dINhYj3m_KCV"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y_cat, test_size=0.2, stratify=y_cat, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.1, stratify=y_temp, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Concatenate, Embedding, Input, LSTM\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.optimizers import Adam, Nadam, RMSprop\n",
        "from tensorflow.keras.utils import to_categorical\n"
      ],
      "metadata": {
        "id": "Vy7fLTgPnj6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPqxXSXhAKrf",
        "outputId": "2e4f4402-fe88-432f-ee1d-76291a8765f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training best model with parameters:\n",
            "dense_units1: 256\n",
            "dense_units2: 64\n",
            "dropout_rate: 0.3053049220534356\n",
            "learning_rate: 0.00012779340697800448\n",
            "optimizer: adam\n",
            "batch_size: 64\n",
            "Epoch 1/20\n",
            "\u001b[1m3713/3713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 6ms/step - accuracy: 0.7163 - loss: 0.5523 - val_accuracy: 0.7948 - val_loss: 0.4473\n",
            "Epoch 2/20\n",
            "\u001b[1m3713/3713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 7ms/step - accuracy: 0.7880 - loss: 0.4556 - val_accuracy: 0.7993 - val_loss: 0.4350\n",
            "Epoch 3/20\n",
            "\u001b[1m3713/3713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 6ms/step - accuracy: 0.7961 - loss: 0.4430 - val_accuracy: 0.8031 - val_loss: 0.4280\n",
            "Epoch 4/20\n",
            "\u001b[1m3713/3713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 6ms/step - accuracy: 0.7995 - loss: 0.4360 - val_accuracy: 0.8014 - val_loss: 0.4274\n",
            "Epoch 5/20\n",
            "\u001b[1m3713/3713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 6ms/step - accuracy: 0.8044 - loss: 0.4287 - val_accuracy: 0.8081 - val_loss: 0.4169\n",
            "Epoch 6/20\n",
            "\u001b[1m3713/3713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 5ms/step - accuracy: 0.8070 - loss: 0.4221 - val_accuracy: 0.8102 - val_loss: 0.4137\n",
            "Epoch 7/20\n",
            "\u001b[1m3713/3713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 6ms/step - accuracy: 0.8106 - loss: 0.4192 - val_accuracy: 0.8134 - val_loss: 0.4090\n",
            "Epoch 8/20\n",
            "\u001b[1m3713/3713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - accuracy: 0.8121 - loss: 0.4140 - val_accuracy: 0.8136 - val_loss: 0.4069\n",
            "Epoch 9/20\n",
            "\u001b[1m3713/3713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 6ms/step - accuracy: 0.8134 - loss: 0.4104 - val_accuracy: 0.8157 - val_loss: 0.4032\n",
            "Epoch 10/20\n",
            "\u001b[1m3713/3713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 7ms/step - accuracy: 0.8145 - loss: 0.4082 - val_accuracy: 0.8162 - val_loss: 0.4004\n",
            "Epoch 11/20\n",
            "\u001b[1m3713/3713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 5ms/step - accuracy: 0.8165 - loss: 0.4042 - val_accuracy: 0.8177 - val_loss: 0.3980\n",
            "Epoch 12/20\n",
            "\u001b[1m3713/3713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 6ms/step - accuracy: 0.8185 - loss: 0.4015 - val_accuracy: 0.8191 - val_loss: 0.3973\n",
            "Epoch 13/20\n",
            "\u001b[1m3713/3713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 6ms/step - accuracy: 0.8204 - loss: 0.3987 - val_accuracy: 0.8175 - val_loss: 0.3987\n",
            "Epoch 14/20\n",
            "\u001b[1m3713/3713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 5ms/step - accuracy: 0.8212 - loss: 0.3960 - val_accuracy: 0.8175 - val_loss: 0.3975\n",
            "Epoch 15/20\n",
            "\u001b[1m3713/3713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 6ms/step - accuracy: 0.8231 - loss: 0.3927 - val_accuracy: 0.8206 - val_loss: 0.3931\n",
            "Epoch 16/20\n",
            "\u001b[1m3713/3713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 6ms/step - accuracy: 0.8233 - loss: 0.3929 - val_accuracy: 0.8200 - val_loss: 0.3923\n",
            "Epoch 17/20\n",
            "\u001b[1m3713/3713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 6ms/step - accuracy: 0.8248 - loss: 0.3904 - val_accuracy: 0.8193 - val_loss: 0.3916\n",
            "Epoch 18/20\n",
            "\u001b[1m3713/3713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 6ms/step - accuracy: 0.8279 - loss: 0.3859 - val_accuracy: 0.8221 - val_loss: 0.3902\n",
            "Epoch 19/20\n",
            "\u001b[1m3713/3713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 6ms/step - accuracy: 0.8262 - loss: 0.3853 - val_accuracy: 0.8231 - val_loss: 0.3903\n",
            "Epoch 20/20\n",
            "\u001b[1m3713/3713\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - accuracy: 0.8278 - loss: 0.3832 - val_accuracy: 0.8165 - val_loss: 0.3952\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model saved to /content/drive/MyDrive/Data Science/Arabic Sentiment/Models/best_training_modelTEST.h5\n",
            "\n",
            "Test accuracy: 0.8195\n",
            "\u001b[1m2063/2063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.83      0.82     32629\n",
            "           1       0.83      0.81      0.82     33371\n",
            "\n",
            "    accuracy                           0.82     66000\n",
            "   macro avg       0.82      0.82      0.82     66000\n",
            "weighted avg       0.82      0.82      0.82     66000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "best_params = {\n",
        "    'dense_units1': 256,\n",
        "    'dense_units2': 64,\n",
        "    'dropout_rate': 0.3053049220534356,\n",
        "    'learning_rate': 0.00012779340697800448,\n",
        "    'optimizer': 'adam',\n",
        "    'batch_size': 64\n",
        "}\n",
        "\n",
        "input_layer = Input(shape=(300,))\n",
        "dense1 = Dense(best_params['dense_units1'], activation='relu')(input_layer)\n",
        "dropout1 = Dropout(best_params['dropout_rate'])(dense1)\n",
        "dense2 = Dense(best_params['dense_units2'], activation='relu')(dropout1)\n",
        "dropout2 = Dropout(best_params['dropout_rate'])(dense2)\n",
        "output = Dense(y_train.shape[1], activation='softmax')(dropout2)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output)\n",
        "\n",
        "optimizer = Adam(learning_rate=best_params['learning_rate'])\n",
        "model.compile(optimizer=optimizer,\n",
        "             loss='categorical_crossentropy',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "# Early stopping callback\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "print(\"Training best model with parameters:\")\n",
        "for k, v in best_params.items():\n",
        "    print(f\"{k}: {v}\")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=20,\n",
        "    batch_size=best_params['batch_size'],\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Save the model (optional)\n",
        "save_dir = '/content/drive/MyDrive/Data Science/Arabic Sentiment/Models'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "model_save_path = os.path.join(save_dir, 'best_training_modelTEST.h5')\n",
        "model.save(model_save_path)\n",
        "print(f\"\\nModel saved to {model_save_path}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"\\nTest accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Generate predictions\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Generate classification report\n",
        "from sklearn.metrics import classification_report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true_classes, y_pred_classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZtT0lbFFfIi"
      },
      "outputs": [],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFoqwT2uCaXy"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "def objective(trial):\n",
        "    dense_units = trial.suggest_categorical(\"dense_units\", [32, 64, 128])\n",
        "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.2, 0.5)\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
        "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"adam\", \"rmsprop\", \"nadam\"])\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32])\n",
        "\n",
        "    input_layer = Input(shape=(X_train.shape[1],))  # Use actual vector shape\n",
        "    dense = Dense(dense_units, activation='relu')(input_layer)\n",
        "    dropout = Dropout(dropout_rate)(dense)\n",
        "    output = Dense(1, activation='sigmoid')(dropout)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output)\n",
        "\n",
        "    if optimizer_name == \"adam\":\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    elif optimizer_name == \"rmsprop\":\n",
        "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
        "    else:\n",
        "        optimizer = tf.keras.optimizers.Nadam(learning_rate=learning_rate)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=0)\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train, y_train[:, 1],\n",
        "        validation_data=(X_val, y_val[:, 1]),\n",
        "        epochs=5,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=[early_stop],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    return max(history.history[\"val_accuracy\"])\n",
        "\n",
        "\n",
        "# ========== Run Optuna Study ==========\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "# ========== Best Trial Results ==========\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "for key, value in trial.params.items():\n",
        "    print(f\"{key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUidO_3q9-Uu"
      },
      "outputs": [],
      "source": [
        "# Load the saved model\n",
        "from tensorflow.keras.models import load_model\n",
        "model_path = '/content/drive/MyDrive/Data Science/Arabic Sentiment/Models/best_training_model.h5'\n",
        "model = load_model(model_path)\n",
        "\n",
        "# Define unlabeled test cases - mix of MSA and dialects\n",
        "test_cases = [\n",
        "    # Modern Standard Arabic\n",
        "    \"الفيلم كان رائعاً من جميع النواحي، التمثيل والإخراج والموسيقى التصويرية\",\n",
        "    \"الخدمة كانت سيئة للغاية ولا أنصح أحداً بهذا المكان\",\n",
        "\n",
        "    # Egyptian Arabic\n",
        "    \"الاكل كان لذيذ جدا والخدمه سريعه والنظافه ممتازه\",\n",
        "    \"المكان مش نظيف والموظفين مش متعاونين خالص\",\n",
        "\n",
        "    # Levantine Arabic\n",
        "    \"المطعم كتير حلو والاكل طيب ونظيف\",\n",
        "    \"الخدمة سيئة وما في اي التزام بالمواعيد\",\n",
        "\n",
        "    # Gulf Arabic\n",
        "    \"الخدمة ممتازة والاكل طيب والجو حلو\",\n",
        "    \"المكان مو نظيف والجو مزعج\",\n",
        "\n",
        "    # Mixed dialect with MSA\n",
        "    \"التجربة كانت جميلة جداً والستاف متعاون كتير\",\n",
        "    \"المنتج مش شغال كويس وهو ليس كما وصف\",\n",
        "\n",
        "    # Emoji and slang cases\n",
        "    \"المنتج رائع 😍😍 يستحق التجربة\",\n",
        "    \"سيء جدا 😡 ما انصح فيه ابدا\",\n",
        "\n",
        "    # Short cases\n",
        "    \"حلو\",\n",
        "    \"سيء\",\n",
        "\n",
        "    # Neutral/ambiguous cases\n",
        "    \"المنتج مقبول ولكن السعر مرتفع بعض الشيء\",\n",
        "    \"الخدمة متوسطة ليست سيئة ولا ممتازة\",\n",
        "\n",
        "    # Code-switching with English\n",
        "    \"المنتج amazing والجودة top\",\n",
        "    \"الservice كان slow جداً\",\n",
        "\n",
        "    # Arabizi (Arabic written in Latin script)\n",
        "    \"el mawdoo3 kwayes awi\",  # الموضوع كويس قوي\n",
        "    \"mabsoot 3ala el service\",  # مبسوط على السيرفيس\n",
        "\n",
        "    # URLs and mentions (to test preprocessing)\n",
        "    \"الموقع جميل https://example.com @username\",\n",
        "    \"تابعني على تويتر @user123 للعروض الجديدة\",\n",
        "\n",
        "    # Numbers and symbols\n",
        "    \"اشتريت 3 قطع وجميعها معيبة\",\n",
        "    \"السعر $$$ لكن الجودة ##\",\n",
        "\n",
        "    # Long text\n",
        "    \"بعد تجربة طويلة مع هذا المنتج يمكنني القول أنه يستحق الشراء بكل تأكيد، الجودة عالية جداً والتغليف ممتاز، ولكن هناك بعض الملاحظات البسيطة على دليل الاستخدام الذي يحتاج إلى مزيد من التوضيح\",\n",
        "\n",
        "    # Very short text\n",
        "    \"لا\"\n",
        "]\n",
        "\n",
        "# Preprocess the test cases using your existing functions\n",
        "print(\"Cleaning and tokenizing test cases...\")\n",
        "cleaned_test, tokenized_test = clean_and_tokenize(test_cases)\n",
        "stemmed_test = stem_with_farasa(tokenized_test)\n",
        "\n",
        "# Modified embedding function that can accept either a file path or direct tokens\n",
        "def embed_with_fasttext(fasttext_model_path, stemmed_tokens=None, stemmed_tokens_path=None, embedding_dim=300, output_path=None):\n",
        "    if stemmed_tokens is None:\n",
        "        if stemmed_tokens_path is None:\n",
        "            raise ValueError(\"Either stemmed_tokens or stemmed_tokens_path must be provided\")\n",
        "        print(\"Loading stemmed tokens from file...\")\n",
        "        with open(stemmed_tokens_path, 'rb') as f:\n",
        "            stemmed_token_lists = pickle.load(f)\n",
        "    else:\n",
        "        print(\"Using provided stemmed tokens...\")\n",
        "        stemmed_token_lists = stemmed_tokens\n",
        "\n",
        "    print(\"Loading FastText model...\")\n",
        "    fasttext_model = gensim.models.KeyedVectors.load_word2vec_format(fasttext_model_path)\n",
        "\n",
        "    print(\"Creating FastText embeddings from stemmed tokens...\")\n",
        "    embeddings = []\n",
        "    for tokens in tqdm(stemmed_token_lists):\n",
        "        # Get vectors for tokens that exist in the model\n",
        "        word_vectors = [fasttext_model[word] for word in tokens if word in fasttext_model]\n",
        "        if word_vectors:\n",
        "            # Average the vectors\n",
        "            text_vector = np.mean(word_vectors, axis=0)\n",
        "        else:\n",
        "            # Use zeros if no tokens are in the model\n",
        "            text_vector = np.zeros(embedding_dim)\n",
        "        embeddings.append(text_vector)\n",
        "\n",
        "    embeddings_array = np.array(embeddings)\n",
        "\n",
        "    # Save embeddings if output path is provided\n",
        "    if output_path:\n",
        "        print(f\"Saving embeddings to {output_path}\")\n",
        "        np.save(output_path, embeddings_array)\n",
        "\n",
        "    return embeddings_array\n",
        "\n",
        "# Now run the test cases with the corrected function\n",
        "print(\"\\nCreating embeddings...\")\n",
        "test_embeddings = embed_with_fasttext(\n",
        "    fasttext_model_path=fasttext_model_path,\n",
        "    stemmed_tokens=stemmed_test,  # Pass the tokens directly\n",
        "    embedding_dim=300\n",
        ")\n",
        "\n",
        "# The rest of your code remains the same...\n",
        "print(\"\\nMaking predictions...\")\n",
        "predictions = model.predict(test_embeddings)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "confidence_scores = np.max(predictions, axis=1)\n",
        "\n",
        "# Display results with confidence scores\n",
        "print(\"\\nPredictions:\")\n",
        "for i, (text, label, confidence) in enumerate(zip(test_cases, predicted_labels, confidence_scores)):\n",
        "    sentiment = \"Positive\" if label == 1 else \"Negative\"\n",
        "    print(f\"Case {i+1}:\")\n",
        "    print(f\"Original: {text}\")\n",
        "    print(f\"Cleaned: {cleaned_test[i]}\")\n",
        "    print(f\"Prediction: {sentiment} (Confidence: {confidence:.2f})\")\n",
        "    print(\"-----\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}