{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["gW9T1ugLiNBW","B8ow6kLuk7vF","1EHzz0Ix_6VC","PjaphrZVAI9O"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","#RNN_CNN_Models"],"metadata":{"id":"gW9T1ugLiNBW"}},{"cell_type":"code","source":["# prompt: mount drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wWBJmXKVGW36","executionInfo":{"status":"ok","timestamp":1748217079557,"user_tz":-180,"elapsed":1316,"user":{"displayName":"Jana Ahmed Nabil Ghoniem","userId":"02650569337915673094"}},"outputId":"6cbbefda-0874-43a7-89f3-f09e62f352be"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import pickle\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","\n","# Load dataset\n","df = pd.read_csv('/content/drive/MyDrive/Data Science/Arabic Sentiment/Datasets/arabic_sentiment_reviews.csv')\n","\n","# Load FastText features\n","with open(\"/content/drive/MyDrive/Data Science/Arabic Sentiment/Features/training_data.pkl\", 'rb') as f:\n","    training_data = pickle.load(f)\n","\n","X = training_data['X_fasttext']\n"],"metadata":{"id":"1soVMzEYDX7Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Encode labels\n","label_encoder = LabelEncoder()\n","y = label_encoder.fit_transform(df['label'])\n","y_cat = tf.keras.utils.to_categorical(y)\n","\n","# Split into train, validation, and test\n","X_temp, X_test, y_temp, y_test = train_test_split(X, y_cat, test_size=0.2, stratify=y_cat, random_state=42)\n","X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.1, stratify=y_temp, random_state=42)\n"],"metadata":{"id":"7BuyIy7nCvNT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import TensorDataset, DataLoader\n","import numpy as np # Import numpy\n","# Convert to tensors\n","def to_tensor(data, labels):\n","    return TensorDataset(torch.tensor(data, dtype=torch.float32),\n","                         torch.tensor(np.argmax(labels, axis=1), dtype=torch.long))\n","\n","train_dataset = to_tensor(X_train, y_train)\n","val_dataset = to_tensor(X_val, y_val)\n","test_dataset = to_tensor(X_test, y_test)\n","\n","# Dataloaders\n","BATCH_SIZE = 64\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n","test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n"],"metadata":{"id":"xFa6bNY2Cvx0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class RNN(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, dropout):\n","        super().__init__()\n","        self.rnn = nn.LSTM(input_dim, hidden_dim, num_layers=n_layers, batch_first=True, dropout=dropout)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        x = x.unsqueeze(1)  # [batch, seq, feat] -> simulate sequence dim\n","        _, (hidden, _) = self.rnn(x)\n","        return self.fc(hidden[-1])\n","\n","class CNN(nn.Module):\n","    def __init__(self, input_dim, output_dim, dropout):\n","        super().__init__()\n","        self.conv1 = nn.Conv1d(1, 100, kernel_size=3, padding=1)\n","        self.relu = nn.ReLU()\n","        self.pool = nn.AdaptiveMaxPool1d(1)\n","        self.fc = nn.Linear(100, output_dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        x = x.unsqueeze(1)  # [B, 1, F]\n","        x = self.pool(self.relu(self.conv1(x))).squeeze(2)\n","        x = self.dropout(x)\n","        return self.fc(x)\n"],"metadata":{"id":"mYRYjUL5ClfW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(model, dataloader, optimizer, criterion):\n","    model.train()\n","    total_loss = 0\n","    for X_batch, y_batch in dataloader:\n","        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n","        optimizer.zero_grad()\n","        predictions = model(X_batch)\n","        loss = criterion(predictions, y_batch)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    return total_loss / len(dataloader)\n","\n","def evaluate(model, dataloader, criterion):\n","    model.eval()\n","    total_loss = 0\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for X_batch, y_batch in dataloader:\n","            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n","            outputs = model(X_batch)\n","            loss = criterion(outputs, y_batch)\n","            total_loss += loss.item()\n","\n","            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n","            labels = y_batch.cpu().numpy()\n","            all_preds.extend(preds)\n","            all_labels.extend(labels)\n","\n","    avg_loss = total_loss / len(dataloader)\n","    acc = accuracy_score(all_labels, all_preds)\n","    mse = mean_squared_error(all_labels, all_preds)\n","    performance = ((1 - avg_loss) + acc) / 2\n","\n","    return avg_loss, acc, mse, performance\n"],"metadata":{"id":"tfhuVLgiCRd5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, mean_squared_error\n","\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","INPUT_DIM = X.shape[1]\n","OUTPUT_DIM = y_cat.shape[1]\n","DROPOUT = 0.5\n","N_EPOCHS = 10\n","\n","# --- RNN ---\n","rnn_model = RNN(input_dim=INPUT_DIM, hidden_dim=128, output_dim=OUTPUT_DIM, n_layers=2, dropout=DROPOUT).to(device)\n","optimizer_rnn = torch.optim.Adam(rnn_model.parameters())\n","criterion = nn.CrossEntropyLoss()\n","\n","print(\"Training RNN...\")\n","for epoch in range(N_EPOCHS):\n","    train_loss = train(rnn_model, train_loader, optimizer_rnn, criterion)\n","    print(f\"[RNN] Epoch {epoch+1}: Train Loss = {train_loss:.4f}\")\n","\n","print(\"\\nFinal Evaluation for RNN:\")\n","train_eval = evaluate(rnn_model, train_loader, criterion)\n","val_eval = evaluate(rnn_model, val_loader, criterion)\n","test_eval = evaluate(rnn_model, test_loader, criterion)\n","\n","print(f\"Train → Loss: {train_eval[0]:.4f}, Acc: {train_eval[1]:.4f}, MSE: {train_eval[2]:.4f}, Perf: {train_eval[3]:.4f}\")\n","print(f\"Val   → Loss: {val_eval[0]:.4f}, Acc: {val_eval[1]:.4f}, MSE: {val_eval[2]:.4f}, Perf: {val_eval[3]:.4f}\")\n","print(f\"Test  → Loss: {test_eval[0]:.4f}, Acc: {test_eval[1]:.4f}, MSE: {test_eval[2]:.4f}, Perf: {test_eval[3]:.4f}\")\n","\n","\n","# torch.save(rnn_model.state_dict(), '/content/drive/MyDrive/Data Science/rnn_model.pt')\n","# print(\"✅ RNN model saved.\")\n","\n","# --- CNN ---\n","cnn_model = CNN(input_dim=INPUT_DIM, output_dim=OUTPUT_DIM, dropout=DROPOUT).to(device)\n","optimizer_cnn = torch.optim.Adam(cnn_model.parameters())\n","\n","print(\"Training CNN...\")\n","for epoch in range(N_EPOCHS):\n","    train_loss = train(cnn_model, train_loader, optimizer_cnn, criterion)\n","    print(f\"[CNN] Epoch {epoch+1}: Train Loss = {train_loss:.4f}\")\n","\n","print(\"\\nFinal Evaluation for CNN:\")\n","train_eval = evaluate(cnn_model, train_loader, criterion)\n","val_eval = evaluate(cnn_model, val_loader, criterion)\n","test_eval = evaluate(cnn_model, test_loader, criterion)\n","\n","print(f\"Train → Loss: {train_eval[0]:.4f}, Acc: {train_eval[1]:.4f}, MSE: {train_eval[2]:.4f}, Perf: {train_eval[3]:.4f}\")\n","print(f\"Val   → Loss: {val_eval[0]:.4f}, Acc: {val_eval[1]:.4f}, MSE: {val_eval[2]:.4f}, Perf: {val_eval[3]:.4f}\")\n","print(f\"Test  → Loss: {test_eval[0]:.4f}, Acc: {test_eval[1]:.4f}, MSE: {test_eval[2]:.4f}, Perf: {test_eval[3]:.4f}\")\n","\n","\n","\n","# torch.save(cnn_model.state_dict(), '/content/drive/MyDrive/Data Science/cnn_model.pt')\n","# print(\"✅ CNN model saved.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iV508Ea3LnLi","executionInfo":{"status":"ok","timestamp":1747338334142,"user_tz":-180,"elapsed":1505364,"user":{"displayName":"Habiba Habiba Mohamed Darwish Sayed","userId":"06249506762316426256"}},"outputId":"e563e95d-41ae-4bc3-e858-b0df16a94671"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training RNN...\n","[RNN] Epoch 1: Train Loss = 0.4737\n","[RNN] Epoch 2: Train Loss = 0.4508\n","[RNN] Epoch 3: Train Loss = 0.4457\n","[RNN] Epoch 4: Train Loss = 0.4424\n","[RNN] Epoch 5: Train Loss = 0.4399\n","[RNN] Epoch 6: Train Loss = 0.4377\n","[RNN] Epoch 7: Train Loss = 0.4355\n","[RNN] Epoch 8: Train Loss = 0.4326\n","[RNN] Epoch 9: Train Loss = 0.4303\n","[RNN] Epoch 10: Train Loss = 0.4288\n","\n","Final Evaluation for RNN:\n","Train → Loss: 0.4180, Acc: 0.8073, MSE: 0.1927, Perf: 0.6946\n","Val   → Loss: 0.4187, Acc: 0.8073, MSE: 0.1927, Perf: 0.6943\n","Test  → Loss: 0.4241, Acc: 0.8033, MSE: 0.1967, Perf: 0.6896\n","✅ RNN model saved.\n","Training CNN...\n","[CNN] Epoch 1: Train Loss = 0.6840\n","[CNN] Epoch 2: Train Loss = 0.6757\n","[CNN] Epoch 3: Train Loss = 0.6733\n","[CNN] Epoch 4: Train Loss = 0.6719\n","[CNN] Epoch 5: Train Loss = 0.6715\n","[CNN] Epoch 6: Train Loss = 0.6711\n","[CNN] Epoch 7: Train Loss = 0.6707\n","[CNN] Epoch 8: Train Loss = 0.6702\n","[CNN] Epoch 9: Train Loss = 0.6706\n","[CNN] Epoch 10: Train Loss = 0.6701\n","\n","Final Evaluation for CNN:\n","Train → Loss: 0.6649, Acc: 0.5987, MSE: 0.4013, Perf: 0.4669\n","Val   → Loss: 0.6662, Acc: 0.5963, MSE: 0.4037, Perf: 0.4650\n","Test  → Loss: 0.6657, Acc: 0.5975, MSE: 0.4025, Perf: 0.4659\n","✅ CNN model saved.\n"]}]},{"cell_type":"code","source":["!pip install praw"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jI-DArS8ke2N","executionInfo":{"status":"ok","timestamp":1747339841840,"user_tz":-180,"elapsed":6777,"user":{"displayName":"Habiba Habiba Mohamed Darwish Sayed","userId":"06249506762316426256"}},"outputId":"e445232a-5406-47ac-d535-494ecfe0829c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting praw\n","  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n","Collecting prawcore<3,>=2.4 (from praw)\n","  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n","Collecting update_checker>=0.18 (from praw)\n","  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n","Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.11/dist-packages (from praw) (1.8.0)\n","Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.4.26)\n","Downloading praw-7.8.1-py3-none-any.whl (189 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.3/189.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n","Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n","Installing collected packages: update_checker, prawcore, praw\n","Successfully installed praw-7.8.1 prawcore-2.4.0 update_checker-0.18.0\n"]}]}]}