{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d866fb9dd3f4aca",
   "metadata": {},
   "source": [
    "# English Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T09:47:10.725332Z",
     "start_time": "2025-04-17T09:47:10.717295Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ftfy import fix_text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from langdetect import detect, LangDetectException\n",
    "from tqdm import tqdm\n",
    "import html\n",
    "import string\n",
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587490a91fc2722a",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc809941825a26b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:18:41.629111Z",
     "start_time": "2025-04-17T00:18:39.344305Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0           1                             2         3                4  \\\n",
       "0  0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_   \n",
       "1  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton   \n",
       "2  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus   \n",
       "3  0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF   \n",
       "4  0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli   \n",
       "\n",
       "                                                   5  \n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1  is upset that he can't update his Facebook by ...  \n",
       "2  @Kenichan I dived many times for the ball. Man...  \n",
       "3    my whole body feels itchy and like its on fire   \n",
       "4  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\Shosh\\Desktop\\Univeristy\\Semester 6\\Data Science\\Project\\english dataset.csv\", encoding=\"ISO-8859-1\", header=None)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e38b3515fa67caf5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:18:41.671910Z",
     "start_time": "2025-04-17T00:18:41.660056Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                              tweet  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"tweet\"]\n",
    "df.columns= columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b4478288dd39a24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:18:41.893873Z",
     "start_time": "2025-04-17T00:18:41.742387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count    Dtype \n",
      "---  ------  --------------    ----- \n",
      " 0   target  1600000 non-null  int64 \n",
      " 1   ids     1600000 non-null  int64 \n",
      " 2   date    1600000 non-null  object\n",
      " 3   flag    1600000 non-null  object\n",
      " 4   user    1600000 non-null  object\n",
      " 5   tweet   1600000 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 73.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "573dcc4d92a0b4e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:18:43.368729Z",
     "start_time": "2025-04-17T00:18:41.942519Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e22781cde4f7f45c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:18:43.512998Z",
     "start_time": "2025-04-17T00:18:43.371278Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target    0\n",
       "ids       0\n",
       "date      0\n",
       "flag      0\n",
       "user      0\n",
       "tweet     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a9c32f375589697",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:18:43.656256Z",
     "start_time": "2025-04-17T00:18:43.557956Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  target\n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...       0\n",
       "1  is upset that he can't update his Facebook by ...       0\n",
       "2  @Kenichan I dived many times for the ball. Man...       0\n",
       "3    my whole body feels itchy and like its on fire        0\n",
       "4  @nationwideclass no, it's not behaving at all....       0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract el cols el me7taga\n",
    "data = df[['tweet','target']]\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aeb1af3f88a7fd0",
   "metadata": {},
   "source": [
    "## Tweet Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8adefcfbacbf026c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:18:43.747005Z",
     "start_time": "2025-04-17T00:18:43.739673Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tweet'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e82eb169e8c9430",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:18:43.865787Z",
     "start_time": "2025-04-17T00:18:43.856372Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_tweet(text):\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove @mentions\n",
    "    text = re.sub(r'#\\w+', '', text)  # Remove hashtags\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with one\n",
    "    return text.strip()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c49599100f3b9d49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:18:43.965377Z",
     "start_time": "2025-04-17T00:18:43.960018Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"- A that's a bummer. You shoulda got David Carr of Third Day to do it. ;D\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_tweet(data['tweet'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20642f9c5f18c187",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:18:50.833263Z",
     "start_time": "2025-04-17T00:18:44.024724Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shosh\\AppData\\Local\\Temp\\ipykernel_17212\\2139904303.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['tweet'] = data['tweet'].apply(clean_tweet)\n"
     ]
    }
   ],
   "source": [
    "data['tweet'] = data['tweet'].apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10a812b9fb28f0ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:18:50.861622Z",
     "start_time": "2025-04-17T00:18:50.851871Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- A that's a bummer. You shoulda got David Car...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I dived many times for the ball. Managed to sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no, it's not behaving at all. i'm mad. why am ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>not the whole crew</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Need a hug</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hey long time no see! Yes.. Rains a bit ,only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nope they didn't have it</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>que me muera ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  target\n",
       "0  - A that's a bummer. You shoulda got David Car...       0\n",
       "1  is upset that he can't update his Facebook by ...       0\n",
       "2  I dived many times for the ball. Managed to sa...       0\n",
       "3     my whole body feels itchy and like its on fire       0\n",
       "4  no, it's not behaving at all. i'm mad. why am ...       0\n",
       "5                                 not the whole crew       0\n",
       "6                                         Need a hug       0\n",
       "7  hey long time no see! Yes.. Rains a bit ,only ...       0\n",
       "8                           nope they didn't have it       0\n",
       "9                                     que me muera ?       0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2f9ca5b16320e7",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9f04f8572e9f219",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:18:50.921645Z",
     "start_time": "2025-04-17T00:18:50.914621Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Preserve sentiment punctuation (! ? . ,) and remove the rest\n",
    "    text = re.sub(r\"[^a-z0-9\\s.,!?']\", '', text)\n",
    "\n",
    "    # Normalize repeated punctuation (e.g., \"!!!\" → \"!\")\n",
    "    text = re.sub(r'([!?.,])\\1+', r'\\1', text)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d67cb2a2cf18845b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:18:57.286167Z",
     "start_time": "2025-04-17T00:18:50.960080Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shosh\\AppData\\Local\\Temp\\ipykernel_17212\\881401672.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['normalized_tweet'] = data['tweet'].apply(normalize_text)\n"
     ]
    }
   ],
   "source": [
    "data['normalized_tweet'] = data['tweet'].apply(normalize_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20af54bb0fe97d73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:18:57.305067Z",
     "start_time": "2025-04-17T00:18:57.288832Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>target</th>\n",
       "      <th>normalized_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- A that's a bummer. You shoulda got David Car...</td>\n",
       "      <td>0</td>\n",
       "      <td>a that's a bummer. you shoulda got david carr ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I dived many times for the ball. Managed to sa...</td>\n",
       "      <td>0</td>\n",
       "      <td>i dived many times for the ball. managed to sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no, it's not behaving at all. i'm mad. why am ...</td>\n",
       "      <td>0</td>\n",
       "      <td>no, it's not behaving at all. i'm mad. why am ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>not the whole crew</td>\n",
       "      <td>0</td>\n",
       "      <td>not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Need a hug</td>\n",
       "      <td>0</td>\n",
       "      <td>need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hey long time no see! Yes.. Rains a bit ,only ...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey long time no see! yes. rains a bit ,only a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nope they didn't have it</td>\n",
       "      <td>0</td>\n",
       "      <td>nope they didn't have it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>que me muera ?</td>\n",
       "      <td>0</td>\n",
       "      <td>que me muera ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  target  \\\n",
       "0  - A that's a bummer. You shoulda got David Car...       0   \n",
       "1  is upset that he can't update his Facebook by ...       0   \n",
       "2  I dived many times for the ball. Managed to sa...       0   \n",
       "3     my whole body feels itchy and like its on fire       0   \n",
       "4  no, it's not behaving at all. i'm mad. why am ...       0   \n",
       "5                                 not the whole crew       0   \n",
       "6                                         Need a hug       0   \n",
       "7  hey long time no see! Yes.. Rains a bit ,only ...       0   \n",
       "8                           nope they didn't have it       0   \n",
       "9                                     que me muera ?       0   \n",
       "\n",
       "                                    normalized_tweet  \n",
       "0  a that's a bummer. you shoulda got david carr ...  \n",
       "1  is upset that he can't update his facebook by ...  \n",
       "2  i dived many times for the ball. managed to sa...  \n",
       "3     my whole body feels itchy and like its on fire  \n",
       "4  no, it's not behaving at all. i'm mad. why am ...  \n",
       "5                                 not the whole crew  \n",
       "6                                         need a hug  \n",
       "7  hey long time no see! yes. rains a bit ,only a...  \n",
       "8                           nope they didn't have it  \n",
       "9                                     que me muera ?  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51d5557d356b8277",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:19:22.528556Z",
     "start_time": "2025-04-17T00:18:57.349125Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shosh\\AppData\\Local\\Temp\\ipykernel_17212\\76191462.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"normalized_tweet\"] = data[\"normalized_tweet\"].apply(fix_text)\n"
     ]
    }
   ],
   "source": [
    "import ftfy  # fixes text for you!\n",
    "\n",
    "# install it first if needed: pip install ftfy\n",
    "\n",
    "data[\"normalized_tweet\"] = data[\"normalized_tweet\"].apply(fix_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b9c46c4bed3f0f",
   "metadata": {},
   "source": [
    "## Duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ab9f7c096a7211a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:19:22.912258Z",
     "start_time": "2025-04-17T00:19:22.552215Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    1535027\n",
      "True       64973\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "duplicates=data.duplicated(subset='normalized_tweet')\n",
    "counts= duplicates.value_counts()\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c5e7308d5338441",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:19:23.970697Z",
     "start_time": "2025-04-17T00:19:22.912258Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(47969)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10cda91b7b8cb052",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:19:24.612800Z",
     "start_time": "2025-04-17T00:19:24.022727Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     normalized_tweet              Conflicting Targets\n",
      "0                                                      [0, 0, 0, 0, 0]... (total 3198)\n",
      "1                                                   !    [0, 0, 0, 0, 0]... (total 28)\n",
      "2                                                   ,     [0, 0, 4, 4, 4]... (total 6)\n",
      "3                                                 , ?     [0, 0, 4, 4, 4]... (total 9)\n",
      "4   , i guess that this will happen again this yea...                           [0, 4]\n",
      "5   , your twitter page looks like on all resoluti...                     [0, 0, 0, 4]\n",
      "6   , your twitter page looks like on big resoluti...                           [0, 4]\n",
      "7                                                   .    [0, 0, 0, 0, 0]... (total 37)\n",
      "8   . . . . i forgot the password for twitter . . ...                           [0, 4]\n",
      "9                                                 . ?     [0, 4, 4, 4, 4]... (total 6)\n",
      "10  . again! but you just cleaned it up o ! i woul...                           [0, 4]\n",
      "11  . home from temecula. ergh i wanted to go to t...                           [0, 4]\n",
      "12                   . i'm still joining in however .                           [0, 4]\n",
      "13                                           . thanks                     [0, 4, 4, 4]\n",
      "14  . what's everyones up to? really tired! watchi...                           [0, 4]\n",
      "15  .and a big bag to take up as well as a rain co...                           [0, 4]\n",
      "16  .i got called out 4 not caring that lfo is get...                           [0, 4]\n",
      "17  .kathontai kai klaine. tha pw olo to song.yey ...                           [0, 4]\n",
      "18  .must have been a gnarly ny show.no tweets fro...                           [0, 4]\n",
      "19  .vacation? i don't know? i hope he's not gone ...                           [0, 4]\n",
      "\n",
      "Total conflicting tweets: 3658\n"
     ]
    }
   ],
   "source": [
    "#Find duplicated tweets\n",
    "conflicts = data[data.duplicated(subset='normalized_tweet', keep=False)]\n",
    "\n",
    "#Group by tweet and collect targets\n",
    "conflicting_tweets = conflicts.groupby('normalized_tweet')['target'].apply(list)\n",
    "\n",
    "#Filter only tweets that have conflicting targets\n",
    "conflicting_tweets = conflicting_tweets[conflicting_tweets.apply(lambda x: len(set(x)) > 1)]\n",
    "\n",
    "#Create a DataFrame for display\n",
    "conflicting_tweets_df = pd.DataFrame({\n",
    "    'normalized_tweet': conflicting_tweets.index,\n",
    "    'Conflicting Targets': conflicting_tweets.values\n",
    "})\n",
    "\n",
    "#Truncate long lists in 'Conflicting Targets' (show first 5 elements)\n",
    "conflicting_tweets_df['Conflicting Targets'] = conflicting_tweets_df['Conflicting Targets'].apply(\n",
    "    lambda x: f\"{x[:5]}... (total {len(x)})\" if len(x) > 5 else x\n",
    ")\n",
    "\n",
    "#Set display options\n",
    "pd.set_option('display.max_colwidth', 50)  # Limit tweet width\n",
    "pd.set_option('display.width', 1000)       # Ensure table fits horizontally\n",
    "\n",
    "# Display first 10 rows\n",
    "print(conflicting_tweets_df.head(20))\n",
    "\n",
    "# Optional: Show total count\n",
    "print(f\"\\nTotal conflicting tweets: {len(conflicting_tweets_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc1abb811fb761b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:19:24.817724Z",
     "start_time": "2025-04-17T00:19:24.619872Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved all 3658 conflicting tweets to 'conflicting_tweets_full1.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame for export (without truncation)\n",
    "conflicting_tweets_df = pd.DataFrame({\n",
    "    'normalized_tweet': conflicting_tweets.index,\n",
    "    'Conflicting Targets': conflicting_tweets.values,\n",
    "    'Number of Conflicts': conflicting_tweets.apply(len)  # Add count of conflicts\n",
    "})\n",
    "\n",
    "# Export to Excel (full content, no truncation)\n",
    "output_file = \"conflicting_tweets_full1.xlsx\"\n",
    "conflicting_tweets_df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Successfully saved all {len(conflicting_tweets_df)} conflicting tweets to '{output_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f21bfcdf331aab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:19:24.865441Z",
     "start_time": "2025-04-17T00:19:24.860121Z"
    }
   },
   "outputs": [],
   "source": [
    "# def resolve_conflict(labels, tweet):\n",
    "#     # Threshold for short tweets (adjust as needed)\n",
    "#     word_count = len(tweet.split())\n",
    "#     if word_count < 2:\n",
    "#         return 'drop'\n",
    "#\n",
    "#     label_counts = pd.Series(labels).value_counts()\n",
    "#\n",
    "#     if len(label_counts) == 1:\n",
    "#         return label_counts.index[0]\n",
    "#     elif label_counts.get(0, 0) == label_counts.get(4, 0):\n",
    "#         return 2  # Equal → Neutral\n",
    "#     elif abs(label_counts.get(0, 0) - label_counts.get(4, 0)) == 1:\n",
    "#         return 'drop'  # Too close, too risky\n",
    "#     else:\n",
    "#         return label_counts.idxmax()  # Strong majority\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af508cd4133d4153",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:19:24.875625Z",
     "start_time": "2025-04-17T00:19:24.865441Z"
    }
   },
   "outputs": [],
   "source": [
    "def resolve_conflict(labels, tweet):\n",
    "    # Threshold for short tweets (adjust as needed)\n",
    "    word_count = len(tweet.split())\n",
    "    if word_count < 2:\n",
    "        return 'drop'\n",
    "\n",
    "    label_counts = pd.Series(labels).value_counts()\n",
    "\n",
    "    if len(label_counts) == 1:\n",
    "        return label_counts.index[0]\n",
    "    elif label_counts.get(0, 0) == label_counts.get(4, 0):\n",
    "        return 'drop'  # Previously neutral, now drop\n",
    "    elif abs(label_counts.get(0, 0) - label_counts.get(4, 0)) == 1:\n",
    "        return 'drop'  # Too close, too risky\n",
    "    else:\n",
    "        return label_counts.idxmax()  # Strong majority\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0e5055e4e0696d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:21:58.484940Z",
     "start_time": "2025-04-17T00:19:24.897466Z"
    }
   },
   "outputs": [],
   "source": [
    "#Group by normalized tweet\n",
    "grouped = data.groupby(\"normalized_tweet\")[\"target\"].apply(list).reset_index(name=\"labels\")\n",
    "\n",
    "#Apply the conflict resolution function\n",
    "grouped[\"resolved_label\"] = grouped.apply(\n",
    "    lambda row: resolve_conflict(row[\"labels\"], row[\"normalized_tweet\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Merge resolved labels back to main dataframe\n",
    "data = data.merge(grouped[[\"normalized_tweet\", \"resolved_label\"]], on=\"normalized_tweet\", how=\"left\")\n",
    "\n",
    "#Drop rows marked as 'drop'\n",
    "data = data[data[\"resolved_label\"] != \"drop\"].copy()\n",
    "\n",
    "#Update target column\n",
    "data[\"target\"] = data[\"resolved_label\"]\n",
    "\n",
    "#Drop the helper column\n",
    "data.drop(columns=[\"resolved_label\"], inplace=True)\n",
    "\n",
    "# Optional: Reset index\n",
    "data.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "942fff56dd9b1ce9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:21:58.850276Z",
     "start_time": "2025-04-17T00:21:58.521739Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total remaining duplicated normalized tweets: 63199\n"
     ]
    }
   ],
   "source": [
    "dups = data[data.duplicated(subset=\"normalized_tweet\", keep=False)]\n",
    "print(f\"Total remaining duplicated normalized tweets: {len(dups)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d244310d3a639f68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:22:02.004043Z",
     "start_time": "2025-04-17T00:21:58.854039Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets with conflicting labels after cleanup: 0\n"
     ]
    }
   ],
   "source": [
    "conflicts_check = data.groupby(\"normalized_tweet\")[\"target\"].nunique()\n",
    "conflicting_final = conflicts_check[conflicts_check > 1]\n",
    "print(f\"Tweets with conflicting labels after cleanup: {len(conflicting_final)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a64d26c6cbaff988",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:22:02.903801Z",
     "start_time": "2025-04-17T00:22:02.022360Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total remaining duplicated normalized tweets: 0\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicate tweets after resolving conflicts\n",
    "data = data.drop_duplicates(subset=[\"normalized_tweet\"]).reset_index(drop=True)\n",
    "\n",
    "# Check again for duplicates\n",
    "dups = data[data.duplicated(subset=\"normalized_tweet\", keep=False)]\n",
    "print(f\"Total remaining duplicated normalized tweets: {len(dups)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b41a16f6b465d9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:22:04.170006Z",
     "start_time": "2025-04-17T00:22:02.935146Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e685faeea7c6b1",
   "metadata": {},
   "source": [
    "## Additional Text Cleaning\n",
    "1. Remove unnecessary punctuation to enhance textual clarity.\n",
    "2. Correct spelling errors to improve accuracy. If a word remains nonsensical after correction, it is classified as an outlier for removal.\n",
    "3. Apply the TF-IDF model, which requires properly spelled words, to effectively identify and filter out low-value or meaningless terms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1ef304454a9d6efb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:22:04.192007Z",
     "start_time": "2025-04-17T00:22:04.185846Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1527194, 3)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "de136c0e3db03b27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:22:04.231829Z",
     "start_time": "2025-04-17T00:22:04.222900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>target</th>\n",
       "      <th>normalized_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- A that's a bummer. You shoulda got David Car...</td>\n",
       "      <td>0</td>\n",
       "      <td>a that's a bummer. you shoulda got david carr ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I dived many times for the ball. Managed to sa...</td>\n",
       "      <td>0</td>\n",
       "      <td>i dived many times for the ball. managed to sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no, it's not behaving at all. i'm mad. why am ...</td>\n",
       "      <td>0</td>\n",
       "      <td>no, it's not behaving at all. i'm mad. why am ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet target                                   normalized_tweet\n",
       "0  - A that's a bummer. You shoulda got David Car...      0  a that's a bummer. you shoulda got david carr ...\n",
       "1  is upset that he can't update his Facebook by ...      0  is upset that he can't update his facebook by ...\n",
       "2  I dived many times for the ball. Managed to sa...      0  i dived many times for the ball. managed to sa...\n",
       "3     my whole body feels itchy and like its on fire      0     my whole body feels itchy and like its on fire\n",
       "4  no, it's not behaving at all. i'm mad. why am ...      0  no, it's not behaving at all. i'm mad. why am ..."
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d2657498ae7b2639",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:23:34.510997Z",
     "start_time": "2025-04-17T00:22:04.281417Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets with meaningless punctuation tokens: 40403\n",
      "12     i couldn't bear to watch it. and i thought the...\n",
      "79     wonders why someone that u like so much can ma...\n",
      "100    body of missing northern calif. girl found pol...\n",
      "184      is watching the hill . . .and its making me sad\n",
      "220                       now your leaving me . gets sad\n",
      "248    damn i am so late at filling this appraisal fo...\n",
      "298    amazon s3 plugin not worked in my website . it...\n",
      "528    fell asleep . . . really didn't mean too chris...\n",
      "641    kutnerrrr! why? why? and to think that 13 is s...\n",
      "643           i so hate homeworks . my head hurts so bad\n",
      "Name: normalized_tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Define punctuation considered meaningless (not including ? and !)\n",
    "meaningless_punctuations = [\"_\", \"-\", \"___\", \"....\", \"....\", \"...\", \"--\"]\n",
    "\n",
    "# Check if a token is meaningless punctuation\n",
    "def contains_meaningless_punct(text):\n",
    "    tokens = text.split()\n",
    "    return any(\n",
    "        re.fullmatch(rf\"[{re.escape(punct)}]+\", token) for punct in meaningless_punctuations for token in tokens\n",
    "    )\n",
    "\n",
    "# Detect tweets with meaningless punctuation tokens\n",
    "punct_token_tweets = data[data[\"normalized_tweet\"].apply(contains_meaningless_punct)]\n",
    "\n",
    "# Show how many tweets contain these meaningless punctuations\n",
    "print(f\"Tweets with meaningless punctuation tokens: {len(punct_token_tweets)}\")\n",
    "\n",
    "# Display the first 10 tweets with meaningless punctuation\n",
    "print(punct_token_tweets[\"normalized_tweet\"].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b26c53b699032ea4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:23:34.565623Z",
     "start_time": "2025-04-17T00:23:34.562151Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_meaningless_punct_tokens(text):\n",
    "    tokens = text.split()\n",
    "    cleaned = [\n",
    "        token for token in tokens\n",
    "        if not re.fullmatch(rf\"[{re.escape(string.punctuation)}]+\", token)\n",
    "    ]\n",
    "    return \" \".join(cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a76b0f94957ec59e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:24:15.846776Z",
     "start_time": "2025-04-17T00:23:34.565623Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply the function\n",
    "data[\"normalized_tweet\"] = data[\"normalized_tweet\"].apply(remove_meaningless_punct_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "55bc9e9e19ae2d29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:25:37.839014Z",
     "start_time": "2025-04-17T00:24:15.865472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining tweets with meaningless punctuation tokens: 0\n"
     ]
    }
   ],
   "source": [
    "# Count the tweets with meaningless punctuation after removal\n",
    "remaining_punct_tweets = data[data[\"normalized_tweet\"].apply(contains_meaningless_punct)]\n",
    "print(f\"Remaining tweets with meaningless punctuation tokens: {len(remaining_punct_tweets)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "af8771a55a08836e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:25:37.880531Z",
     "start_time": "2025-04-17T00:25:37.873624Z"
    }
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "# from autocorrect import Speller\n",
    "\n",
    "# # Initialize the spell checker once (faster than re-creating each time)\n",
    "# spell = Speller(lang='en')  # You can set `fast=True` optionally for speed\n",
    "\n",
    "# # Function to correct spelling for a single tweet\n",
    "# def correct_spelling_autocorrect(text):\n",
    "#     if isinstance(text, str):  # Ensure input is string\n",
    "#         return spell(text)\n",
    "#     return text  # Return unchanged if not string (for robustness)\n",
    "\n",
    "# # Batch processing for large datasets\n",
    "# def batch_process_spell_check(df, batch_size=10000):\n",
    "#     corrected_texts = []\n",
    "#     for start in tqdm(range(0, len(df), batch_size), desc=\"Spell Checking\"):\n",
    "#         batch = df[\"normalized_tweet\"].iloc[start:start + batch_size]\n",
    "#         corrected_batch = batch.apply(correct_spelling_autocorrect)\n",
    "#         corrected_texts.extend(corrected_batch)\n",
    "#     return corrected_texts\n",
    "\n",
    "# # Apply to your DataFrame\n",
    "# corrected_tweets = batch_process_spell_check(data)\n",
    "# data[\"corrected_tweet\"] = corrected_tweets\n",
    "\n",
    "# # Save corrected version\n",
    "# # data.to_csv(\"corrected_tweets_autocorrect.csv\", index=False)\n",
    "\n",
    "\n",
    "# print(data[[\"normalized_tweet\", \"corrected_tweet\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e35257069289287f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:25:38.141247Z",
     "start_time": "2025-04-17T00:25:37.932130Z"
    }
   },
   "outputs": [],
   "source": [
    "data[\"normalized_tweet\"] = data[\"normalized_tweet\"].apply(html.unescape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7a97cf2a09ca6a37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:25:38.577201Z",
     "start_time": "2025-04-17T00:25:38.160796Z"
    }
   },
   "outputs": [],
   "source": [
    "data[\"normalized_tweet\"] = data[\"normalized_tweet\"].str.replace(r\"ï½\", \"'\", regex=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40959e49578d11d4",
   "metadata": {},
   "source": [
    "## Filtering\n",
    "things I noticed:\n",
    "1. numbers only without text\n",
    "2. one/two words 3amla conflict gamed \n",
    "3. punctuation bas without text "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd01dff96d516d6",
   "metadata": {},
   "source": [
    "i will first detect then decide what needs to be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5ed0c08006b62de0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:25:41.427964Z",
     "start_time": "2025-04-17T00:25:38.626180Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short tweets (2 tokens): 884\n",
      "10921                     what\n",
      "17466                 hangover\n",
      "19497                     true\n",
      "23118       yeah,unfortunately\n",
      "30656                         \n",
      "41854                whatever.\n",
      "44900                         \n",
      "46076                         \n",
      "52014                   ?quot?\n",
      "52982    borrrrrreeeeeeeeeeeed\n",
      "Name: normalized_tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Detect Short Tweets (possible outliers)\n",
    "short_outliers = data[data[\"normalized_tweet\"].str.split().apply(len) < 2]\n",
    "print(f\"Short tweets (2 tokens): {len(short_outliers)}\")\n",
    "print(short_outliers[\"normalized_tweet\"].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "73e5b1a26a000225",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:25:44.095995Z",
     "start_time": "2025-04-17T00:25:41.450757Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop short tweets with less than 2 tokens\n",
    "data = data[data[\"normalized_tweet\"].str.split().apply(len) >= 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0be73b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>target</th>\n",
       "      <th>normalized_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- A that's a bummer. You shoulda got David Car...</td>\n",
       "      <td>0</td>\n",
       "      <td>a that's a bummer. you shoulda got david carr ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I dived many times for the ball. Managed to sa...</td>\n",
       "      <td>0</td>\n",
       "      <td>i dived many times for the ball. managed to sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no, it's not behaving at all. i'm mad. why am ...</td>\n",
       "      <td>0</td>\n",
       "      <td>no, it's not behaving at all. i'm mad. why am ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet target                                   normalized_tweet\n",
       "0  - A that's a bummer. You shoulda got David Car...      0  a that's a bummer. you shoulda got david carr ...\n",
       "1  is upset that he can't update his Facebook by ...      0  is upset that he can't update his facebook by ...\n",
       "2  I dived many times for the ball. Managed to sa...      0  i dived many times for the ball. managed to sa...\n",
       "3     my whole body feels itchy and like its on fire      0     my whole body feels itchy and like its on fire\n",
       "4  no, it's not behaving at all. i'm mad. why am ...      0  no, it's not behaving at all. i'm mad. why am ..."
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9e1e5488faf8fcde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:25:46.703959Z",
     "start_time": "2025-04-17T00:25:44.104612Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long tweets (≥100 tokens): 0\n",
      "Series([], Name: normalized_tweet, dtype: object)\n"
     ]
    }
   ],
   "source": [
    "#Detect Long Tweets\n",
    "long_outliers = data[data[\"normalized_tweet\"].str.split().apply(len) >= 100]\n",
    "print(f\"Long tweets (≥100 tokens): {len(long_outliers)}\")\n",
    "print(long_outliers[\"normalized_tweet\"].head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9b3bf9be3fd9a198",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:25:47.655216Z",
     "start_time": "2025-04-17T00:25:46.738835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets with non-ASCII (possibly garbled): 0\n",
      "Series([], Name: normalized_tweet, dtype: object)\n"
     ]
    }
   ],
   "source": [
    "# Regex pattern for detecting non-ASCII characters\n",
    "import re\n",
    "\n",
    "def is_garbled(text):\n",
    "    return bool(re.search(r\"[^\\x00-\\x7F]\", text))\n",
    "\n",
    "# Apply the check\n",
    "weird_tweets = data[data[\"normalized_tweet\"].apply(is_garbled)]\n",
    "\n",
    "print(f\"Tweets with non-ASCII (possibly garbled): {len(weird_tweets)}\")\n",
    "print(weird_tweets[\"normalized_tweet\"].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b97e2cc85d942b08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:25:48.524688Z",
     "start_time": "2025-04-17T00:25:47.655216Z"
    }
   },
   "outputs": [],
   "source": [
    "# Replace common problematic characters\n",
    "replacements = {\n",
    "    '½': '0.5',\n",
    "    '¼': '0.25',\n",
    "    '¾': '0.75',\n",
    "    'â€™': \"'\",\n",
    "    'â€œ': '\"',\n",
    "    'â€': '\"',\n",
    "    'â€“': '-',  # en-dash\n",
    "    'â€”': '-',  # em-dash\n",
    "    'ï': '',     # junk\n",
    "    'á': '',     # remove accented duplicates\n",
    "    'ãª': '',    # junk\n",
    "    'â': '',     # catch-all\n",
    "}\n",
    "\n",
    "def clean_garbled(text):\n",
    "    for bad, good in replacements.items():\n",
    "        text = text.replace(bad, good)\n",
    "    return text\n",
    "\n",
    "data[\"normalized_tweet\"] = data[\"normalized_tweet\"].apply(clean_garbled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bcf86adb5f1a1a35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:25:49.446436Z",
     "start_time": "2025-04-17T00:25:48.543132Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets with non-ASCII (possibly garbled): 0\n",
      "Series([], Name: normalized_tweet, dtype: object)\n"
     ]
    }
   ],
   "source": [
    "def is_garbled(text):\n",
    "    return bool(re.search(r\"[^\\x00-\\x7F]\", text))\n",
    "\n",
    "# Apply the check\n",
    "weird_tweets = data[data[\"normalized_tweet\"].apply(is_garbled)]\n",
    "\n",
    "print(f\"Tweets with non-ASCII (possibly garbled): {len(weird_tweets)}\")\n",
    "print(weird_tweets[\"normalized_tweet\"].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "325622ec0537781",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T01:21:33.938694Z",
     "start_time": "2025-04-17T00:25:49.466818Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153/153 [1:42:01<00:00, 40.01s/it]\n"
     ]
    }
   ],
   "source": [
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == \"en\"\n",
    "    except LangDetectException:\n",
    "        return False\n",
    "\n",
    "# Prepare\n",
    "batch_size = 10000\n",
    "english_flags = []\n",
    "\n",
    "# Loop over the data in batches\n",
    "for start in tqdm(range(0, len(data), batch_size)):\n",
    "    end = start + batch_size\n",
    "    batch = data[\"normalized_tweet\"].iloc[start:end]\n",
    "    flags = [is_english(text) for text in batch]\n",
    "    english_flags.extend(flags)\n",
    "\n",
    "# Add results to DataFrame\n",
    "data[\"is_english\"] = english_flags\n",
    "\n",
    "# Filter English tweets\n",
    "data = data[data[\"is_english\"]].drop(columns=[\"is_english\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "de7055339977dc9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T01:21:34.951486Z",
     "start_time": "2025-04-17T01:21:33.996629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Garbled tweets found: 0\n",
      "Series([], Name: normalized_tweet, dtype: object)\n"
     ]
    }
   ],
   "source": [
    "def is_garbled(text):\n",
    "    return bool(re.search(r\"[^\\x00-\\x7F]\", text))\n",
    "\n",
    "# Step 1: Find garbled tweets\n",
    "garbled_mask = data[\"normalized_tweet\"].apply(is_garbled)\n",
    "garbled_tweets = data[garbled_mask]\n",
    "\n",
    "# Step 2: Preview how many and what they look like\n",
    "print(f\"Garbled tweets found: {len(garbled_tweets)}\")\n",
    "print(garbled_tweets[\"normalized_tweet\"].head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5f04e7f3b91e32fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T01:21:35.085295Z",
     "start_time": "2025-04-17T01:21:34.971181Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop anything still garbled\n",
    "data = data[~garbled_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2d020413e8883d7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T01:21:37.385546Z",
     "start_time": "2025-04-17T01:21:35.086915Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gibberish-like tweets: 26978\n",
      "69        agreed, i saw the failwhale allllll day today.\n",
      "111    i'm sooo sad! they killed off kutner on house ...\n",
      "130    haha its so cooooold in the d! and no but you ...\n",
      "168    i had on my page for sooooo long! until it got...\n",
      "252    ooooooh! sealclap see, i download shitloads of...\n",
      "263    poor socks luvvvvv the golden retriever! i wan...\n",
      "265                          aaaaand the nausea is back.\n",
      "270    oooooooo who with? im not neither but thats be...\n",
      "313    oh! did i mention it? quotgooooood moooorniiii...\n",
      "348    stupid movies we watched. mirrors ugggggh. sto...\n",
      "Name: normalized_tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Detect Gibberish / Repeated Characters\n",
    "def is_gibberish(text):\n",
    "    return bool(re.search(r\"(.)\\1{4,}\", text))  # e.g. \"sooooo happy\" or \"aaaaaaah\"\n",
    "\n",
    "gibberish_tweets = data[data[\"normalized_tweet\"].apply(is_gibberish)]\n",
    "print(f\"Gibberish-like tweets: {len(gibberish_tweets)}\")\n",
    "print(gibberish_tweets[\"normalized_tweet\"].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "74f89490485a6626",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T01:21:40.135386Z",
     "start_time": "2025-04-17T01:21:37.403226Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_repeats(text):\n",
    "    return re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", text)  # replace 3+ same chars with 2\n",
    "\n",
    "# Apply it to the dataset\n",
    "data[\"normalized_tweet\"] = data[\"normalized_tweet\"].apply(normalize_repeats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9a6dd874dd956541",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T01:21:42.598801Z",
     "start_time": "2025-04-17T01:21:40.152776Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gibberish-like tweets: 0\n",
      "Series([], Name: normalized_tweet, dtype: object)\n"
     ]
    }
   ],
   "source": [
    "gibberish_tweets = data[data[\"normalized_tweet\"].apply(is_gibberish)]\n",
    "print(f\"Gibberish-like tweets: {len(gibberish_tweets)}\")\n",
    "print(gibberish_tweets[\"normalized_tweet\"].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "50e729324fdc5ecd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T02:11:53.111524Z",
     "start_time": "2025-04-17T01:21:42.614585Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Detect Non-English Tweets, already handled bas bent2kd\n",
    "# def is_non_english(text):\n",
    "#     try:\n",
    "#         return detect(text) != \"en\"\n",
    "#     except LangDetectException:\n",
    "#         return True\n",
    "\n",
    "# data[\"non_english\"] = data[\"corrected_tweet\"].apply(is_non_english)\n",
    "# non_english_tweets = data[data[\"non_english\"] == True]\n",
    "# print(f\"Non-English tweets: {len(non_english_tweets)}\")\n",
    "# print(non_english_tweets[\"corrected_tweet\"].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d55a89b5ebb527a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T02:11:55.072695Z",
     "start_time": "2025-04-17T02:11:53.200925Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 457104.99it/s]s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 463889.58it/s] 9.38it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 393336.46it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 523738.08it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 388908.83it/s]18.48it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 407463.21it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 543860.17it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 390654.77it/s]21.08it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 339729.79it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 370544.47it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 382235.10it/s] 21.22it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 476316.93it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 402111.46it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 425433.26it/s] 21.00it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 365421.15it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 406874.26it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 382583.76it/s] 21.61it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 345272.72it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 393436.08it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 379726.23it/s] 21.38it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 403931.55it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 425912.79it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 389352.89it/s] 20.82it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 367373.57it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 404789.18it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 427619.31it/s] 21.42it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 344179.08it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 440171.27it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 385296.94it/s] 21.91it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 394665.16it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 445302.47it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 488186.60it/s] 22.17it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 451587.98it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 450071.25it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 338873.41it/s] 22.82it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 436679.23it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 518853.01it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 460866.95it/s] 23.40it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 403612.81it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 439028.64it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 409992.38it/s] 24.31it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 423107.20it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 423256.64it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 404418.39it/s] 24.19it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 413823.10it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 458113.50it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 447908.42it/s] 24.39it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 438835.72it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 438877.04it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 393831.36it/s] 24.53it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 443189.81it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 426983.74it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 419388.46it/s] 24.46it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 441682.36it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 475151.41it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 379575.02it/s] 24.51it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 389244.49it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 555846.17it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 353869.08it/s] 23.76it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 680363.35it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 407502.79it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 452797.01it/s] 23.82it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 387873.05it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 516921.86it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 396317.18it/s] 23.98it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 447077.68it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 395696.52it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 429102.37it/s] 23.80it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 441966.26it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 394579.77it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 542137.89it/s] 23.70it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 504165.49it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 415351.65it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 470672.52it/s] 23.95it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 426914.21it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 543493.71it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 508375.84it/s] 24.35it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 394412.80it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 494279.08it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 538449.21it/s] 24.55it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 414924.32it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 469902.64it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 473830.93it/s] 24.89it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 417169.34it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 312597.19it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 373447.78it/s] 23.83it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 401487.91it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 430516.19it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 413998.74it/s] 23.58it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 495880.26it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 493296.64it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 595824.14it/s] 23.71it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 426254.74it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 502865.91it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 485693.57it/s] 24.58it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 589750.28it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 467743.64it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 547530.68it/s] 24.64it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 506417.78it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 448843.09it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 446625.42it/s], 24.90it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 422625.45it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 642805.21it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 356291.91it/s], 25.16it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 395144.80it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 373943.87it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 459438.29it/s], 24.28it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 384989.26it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 415466.85it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 539397.88it/s], 24.29it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 453865.15it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 467144.54it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 424972.04it/s], 25.27it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 416027.30it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 541801.74it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 414047.78it/s], 25.51it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 442171.27it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 373225.13it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 383854.74it/s], 25.19it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 487539.70it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 425282.28it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 380528.93it/s], 25.18it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 432322.25it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 447459.25it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 453933.92it/s], 25.32it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 481478.54it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 348546.50it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 517879.24it/s], 25.61it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 550022.16it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 498864.61it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 360298.25it/s], 26.49it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 429462.65it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 477129.69it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 416489.98it/s], 25.65it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 439253.93it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 382078.41it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 433972.83it/s], 25.35it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 407210.03it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 430980.68it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 421851.83it/s], 25.39it/s]\n",
      "100%|██████████| 72/72 [00:00<00:00, 46253.62it/s]\n",
      "Processing batches: 100%|██████████| 141/141 [00:05<00:00, 23.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Found 0 tweets with only numbers.\n",
      "Series([], Name: normalized_tweet, dtype: object)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Check tweets that contain ONLY numbers (possibly with spaces)\n",
    "# Enable progress_apply for tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Define regex pattern to match ONLY numbers (and optional spaces between them)\n",
    "number_only_pattern = re.compile(r\"^\\d+(?:\\s+\\d+)*$\")\n",
    "\n",
    "# Function to check for number-only tweets\n",
    "def is_only_numbers(text):\n",
    "    return bool(number_only_pattern.match(text))\n",
    "\n",
    "# OPTIONAL: define batch size if your dataset is very large\n",
    "batch_size = 10000\n",
    "\n",
    "# Container for all matching tweets\n",
    "only_numbers = pd.DataFrame()\n",
    "\n",
    "# If dataset is small, you can skip batching and just use this:\n",
    "# only_numbers = data[data[\"normalized_tweet\"].progress_apply(is_only_numbers)]\n",
    "\n",
    "# Batch processing\n",
    "for i in tqdm(range(0, len(data), batch_size), desc=\"Processing batches\"):\n",
    "    batch = data.iloc[i:i + batch_size].copy()\n",
    "    matched = batch[batch[\"normalized_tweet\"].progress_apply(is_only_numbers)]\n",
    "    only_numbers = pd.concat([only_numbers, matched], ignore_index=True)\n",
    "\n",
    "print(f\"\\n✅ Found {len(only_numbers)} tweets with only numbers.\")\n",
    "print(only_numbers[\"normalized_tweet\"].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7652d0ec1c756d34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T02:11:57.270679Z",
     "start_time": "2025-04-17T02:11:55.076214Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove any digit sequences from within the tweets\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r\"\\b\\d+\\b\", \"\", text)  # remove standalone numbers\n",
    "\n",
    "data[\"normalized_tweet\"] = data[\"normalized_tweet\"].apply(remove_numbers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "854830947792af02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T02:11:59.033587Z",
     "start_time": "2025-04-17T02:11:57.287720Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/141 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 141/141 [00:07<00:00, 19.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets with both text and numbers: 77265\n",
      "0    some1 hacked my account on aim now i have to m...\n",
      "1                       sorry! bed time came here gmt1\n",
      "2    i miss my ps3, it's out of commission wutcha p...\n",
      "3    i had such a nice day. too bad the rain comes ...\n",
      "4    pray for me please, the ex is threatening to s...\n",
      "Name: normalized_tweet, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Enable progress_apply for tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Define the batch size (adjust as needed)\n",
    "batch_size = 10000\n",
    "\n",
    "# Regex patterns for detecting numbers and text\n",
    "number_pattern = r\"\\d\"\n",
    "text_pattern = r\"[a-zA-Z]\"\n",
    "\n",
    "# Container to hold matching tweets\n",
    "numbers_with_text = pd.DataFrame()\n",
    "\n",
    "# Batch processing\n",
    "for i in tqdm(range(0, len(data), batch_size), desc=\"Processing batches\"):\n",
    "    batch = data.iloc[i:i + batch_size].copy()\n",
    "\n",
    "    # Filter tweets containing both numbers and letters\n",
    "    matched_batch = batch[batch[\"normalized_tweet\"].str.contains(number_pattern) &\n",
    "                          batch[\"normalized_tweet\"].str.contains(text_pattern)]\n",
    "\n",
    "    # Concatenate matched tweets\n",
    "    numbers_with_text = pd.concat([numbers_with_text, matched_batch], ignore_index=True)\n",
    "\n",
    "# Output the number of matched tweets\n",
    "print(f\"Tweets with both text and numbers: {len(numbers_with_text)}\")\n",
    "print(numbers_with_text[\"normalized_tweet\"].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "49632d852056f637",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T02:12:00.326015Z",
     "start_time": "2025-04-17T02:11:59.058324Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove digits that appear inside words like \"cool99\"\n",
    "data[\"normalized_tweet\"] = data[\"normalized_tweet\"].str.replace(r\"\\d+\", \"\", regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1103dc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 141/141 [00:05<00:00, 26.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets with both text and numbers: 0\n",
      "Series([], Name: normalized_tweet, dtype: object)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Enable progress_apply for tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Define the batch size (adjust as needed)\n",
    "batch_size = 10000\n",
    "\n",
    "# Regex patterns for detecting numbers and text\n",
    "number_pattern = r\"\\d\"\n",
    "text_pattern = r\"[a-zA-Z]\"\n",
    "\n",
    "# Container to hold matching tweets\n",
    "numbers_with_text = pd.DataFrame()\n",
    "\n",
    "# Batch processing\n",
    "for i in tqdm(range(0, len(data), batch_size), desc=\"Processing batches\"):\n",
    "    batch = data.iloc[i:i + batch_size].copy()\n",
    "\n",
    "    # Filter tweets containing both numbers and letters\n",
    "    matched_batch = batch[batch[\"normalized_tweet\"].str.contains(number_pattern) &\n",
    "                          batch[\"normalized_tweet\"].str.contains(text_pattern)]\n",
    "\n",
    "    # Concatenate matched tweets\n",
    "    numbers_with_text = pd.concat([numbers_with_text, matched_batch], ignore_index=True)\n",
    "\n",
    "# Output the number of matched tweets\n",
    "print(f\"Tweets with both text and numbers: {len(numbers_with_text)}\")\n",
    "print(numbers_with_text[\"normalized_tweet\"].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c3c5138bceb5744e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T02:12:00.855185Z",
     "start_time": "2025-04-17T02:12:00.343177Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop empty tweets\n",
    "data = data[data[\"normalized_tweet\"].str.strip().astype(bool)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bb40801d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>target</th>\n",
       "      <th>normalized_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- A that's a bummer. You shoulda got David Car...</td>\n",
       "      <td>0</td>\n",
       "      <td>a that's a bummer. you shoulda got david carr ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I dived many times for the ball. Managed to sa...</td>\n",
       "      <td>0</td>\n",
       "      <td>i dived many times for the ball. managed to sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no, it's not behaving at all. i'm mad. why am ...</td>\n",
       "      <td>0</td>\n",
       "      <td>no, it's not behaving at all. i'm mad. why am ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>not the whole crew</td>\n",
       "      <td>0</td>\n",
       "      <td>not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Need a hug</td>\n",
       "      <td>0</td>\n",
       "      <td>need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hey long time no see! Yes.. Rains a bit ,only ...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey long time no see! yes. rains a bit ,only a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nope they didn't have it</td>\n",
       "      <td>0</td>\n",
       "      <td>nope they didn't have it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>spring break in plain city... it's snowing</td>\n",
       "      <td>0</td>\n",
       "      <td>spring break in plain city. it's snowing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I just re-pierced my ears</td>\n",
       "      <td>0</td>\n",
       "      <td>i just repierced my ears</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I couldn't bear to watch it. And I thought the...</td>\n",
       "      <td>0</td>\n",
       "      <td>i couldn't bear to watch it. and i thought the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>It it counts, idk why I did either. you never ...</td>\n",
       "      <td>0</td>\n",
       "      <td>it it counts, idk why i did either. you never ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>i would've been the first, but i didn't have a...</td>\n",
       "      <td>0</td>\n",
       "      <td>i would've been the first, but i didn't have a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I wish I got to watch it with you!! I miss you...</td>\n",
       "      <td>0</td>\n",
       "      <td>i wish i got to watch it with you! i miss you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Hollis' death scene will hurt me severely to w...</td>\n",
       "      <td>0</td>\n",
       "      <td>hollis' death scene will hurt me severely to w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>about to file taxes</td>\n",
       "      <td>0</td>\n",
       "      <td>about to file taxes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ahh ive always wanted to see rent love the sou...</td>\n",
       "      <td>0</td>\n",
       "      <td>ahh ive always wanted to see rent love the sou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Oh dear. Were you drinking out of the forgotte...</td>\n",
       "      <td>0</td>\n",
       "      <td>oh dear. were you drinking out of the forgotte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>i was out most of the day so didn't get much done</td>\n",
       "      <td>0</td>\n",
       "      <td>i was out most of the day so didn't get much done</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweet target                                   normalized_tweet\n",
       "0   - A that's a bummer. You shoulda got David Car...      0  a that's a bummer. you shoulda got david carr ...\n",
       "1   is upset that he can't update his Facebook by ...      0  is upset that he can't update his facebook by ...\n",
       "2   I dived many times for the ball. Managed to sa...      0  i dived many times for the ball. managed to sa...\n",
       "3      my whole body feels itchy and like its on fire      0     my whole body feels itchy and like its on fire\n",
       "4   no, it's not behaving at all. i'm mad. why am ...      0  no, it's not behaving at all. i'm mad. why am ...\n",
       "5                                  not the whole crew      0                                 not the whole crew\n",
       "6                                          Need a hug      0                                         need a hug\n",
       "7   hey long time no see! Yes.. Rains a bit ,only ...      0  hey long time no see! yes. rains a bit ,only a...\n",
       "8                            nope they didn't have it      0                           nope they didn't have it\n",
       "10         spring break in plain city... it's snowing      0           spring break in plain city. it's snowing\n",
       "11                          I just re-pierced my ears      0                           i just repierced my ears\n",
       "12  I couldn't bear to watch it. And I thought the...      0  i couldn't bear to watch it. and i thought the...\n",
       "13  It it counts, idk why I did either. you never ...      0  it it counts, idk why i did either. you never ...\n",
       "14  i would've been the first, but i didn't have a...      0  i would've been the first, but i didn't have a...\n",
       "15  I wish I got to watch it with you!! I miss you...      0  i wish i got to watch it with you! i miss you ...\n",
       "16  Hollis' death scene will hurt me severely to w...      0  hollis' death scene will hurt me severely to w...\n",
       "17                                about to file taxes      0                                about to file taxes\n",
       "18  ahh ive always wanted to see rent love the sou...      0  ahh ive always wanted to see rent love the sou...\n",
       "19  Oh dear. Were you drinking out of the forgotte...      0  oh dear. were you drinking out of the forgotte...\n",
       "20  i was out most of the day so didn't get much done      0  i was out most of the day so didn't get much done"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e519e9653fc02c7",
   "metadata": {},
   "source": [
    "## Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5f7ecae217b41378",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T09:47:58.857243Z",
     "start_time": "2025-04-17T09:47:58.792876Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Your existing pie chart data\n",
    "plt.pie(\n",
    "    data['target'].value_counts(),\n",
    "    labels=['Negative', 'Positive'],\n",
    "    autopct=\"%1.0f%%\",\n",
    "    colors=['#A16D98', '#FFCFF6']  # Custom pastel colors\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b605b5c1da608d",
   "metadata": {},
   "source": [
    "## NLP (tokenization , stopwords and stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "daf29595f41b250b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T02:12:52.439332Z",
     "start_time": "2025-04-17T02:12:00.895808Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Shosh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Example: Tokenize a single tweet\n",
    "data[\"tokens\"] = data[\"normalized_tweet\"].apply(word_tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "917d5e212ef3d8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T02:12:58.771205Z",
     "start_time": "2025-04-17T02:12:56.023983Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Shosh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "NEGATIONS = {\"no\", \"nor\", \"not\", \"n't\", \"never\", \"none\", \"nobody\", \"nothing\", \"nowhere\", \"hardly\", \"scarcely\", \"barely\", \"wouldn't\", \"couldn't\", \"shouldn't\", \"won't\", \"can't\", \"don't\", \"doesn't\", \"didn't\", \"isn't\", \"aren't\", \"wasn't\", \"weren't\", \"haven't\", \"hasn't\", \"hadn't\", \"without\"}\n",
    "\n",
    "CUSTOM_STOPWORDS = set(stopwords.words('english'))\n",
    "STOP_WORDS = CUSTOM_STOPWORDS - NEGATIONS\n",
    "data[\"tokens\"] = data[\"tokens\"].apply(lambda x: [word for word in x if word not in STOP_WORDS])  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "57e6b92d1493067c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T02:14:43.490728Z",
     "start_time": "2025-04-17T02:12:58.808636Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "stemmer = PorterStemmer()\n",
    "data[\"tokens\"] = data[\"tokens\"].apply(lambda x: [stemmer.stem(word) for word in x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c4a36f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.util import ngrams\n",
    "\n",
    "# def trigram_tokenizer(text):\n",
    "#     # Tokenize using a better tokenizer (NLTK here; spaCy works too)\n",
    "#     tokens = word_tokenize(text.lower())\n",
    "#     # Create trigrams\n",
    "#     trigrams = [' '.join(gram) for gram in ngrams(tokens, 3)]\n",
    "#     return trigrams\n",
    "\n",
    "# tfidf = TfidfVectorizer(tokenizer=trigram_tokenizer, lowercase=False, max_features=5000, preprocessor=None)\n",
    "# X_tfidf = tfidf.fit_transform(data[\"normalized_tweet\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e131038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open(\"tfidf_vectorizer.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(tfidf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "51ddb42b97be7bd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T02:14:57.624676Z",
     "start_time": "2025-04-17T02:14:43.539257Z"
    }
   },
   "outputs": [],
   "source": [
    "# from scipy.sparse import csr_matrix\n",
    "#\n",
    "# # Apply TF-IDF to your tweet data\n",
    "# tfidf = TfidfVectorizer(max_features=5000)  # Adjust max_features if needed\n",
    "# X_tfidf = tfidf.fit_transform(data[\"normalized_tweet\"])\n",
    "#\n",
    "# # We can work with the sparse matrix directly without converting it to a dense DataFrame\n",
    "# # If needed, we can convert the sparse matrix to a DataFrame later for analysis or export\n",
    "#\n",
    "# # For example, getting the feature names:\n",
    "# feature_names = tfidf.get_feature_names_out()\n",
    "#\n",
    "# # If needed, you can convert it to a sparse DataFrame\n",
    "# tfidf_sparse_df = pd.DataFrame.sparse.from_spmatrix(X_tfidf, columns=feature_names)\n",
    "#\n",
    "# # Show first 5 rows (or any sample)\n",
    "# print(tfidf_sparse_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "64bb0aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>target</th>\n",
       "      <th>normalized_tweet</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- A that's a bummer. You shoulda got David Car...</td>\n",
       "      <td>0</td>\n",
       "      <td>a that's a bummer. you shoulda got david carr ...</td>\n",
       "      <td>['s, bummer, ., shoulda, got, david, carr, thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his facebook by ...</td>\n",
       "      <td>[upset, ca, n't, updat, facebook, text, ., mig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I dived many times for the ball. Managed to sa...</td>\n",
       "      <td>0</td>\n",
       "      <td>i dived many times for the ball. managed to sa...</td>\n",
       "      <td>[dive, mani, time, ball, ., manag, save, rest,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[whole, bodi, feel, itchi, like, fire]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no, it's not behaving at all. i'm mad. why am ...</td>\n",
       "      <td>0</td>\n",
       "      <td>no, it's not behaving at all. i'm mad. why am ...</td>\n",
       "      <td>[no, ,, 's, not, behav, ., 'm, mad, ., ?, ca, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet target                                   normalized_tweet                                             tokens\n",
       "0  - A that's a bummer. You shoulda got David Car...      0  a that's a bummer. you shoulda got david carr ...  ['s, bummer, ., shoulda, got, david, carr, thi...\n",
       "1  is upset that he can't update his Facebook by ...      0  is upset that he can't update his facebook by ...  [upset, ca, n't, updat, facebook, text, ., mig...\n",
       "2  I dived many times for the ball. Managed to sa...      0  i dived many times for the ball. managed to sa...  [dive, mani, time, ball, ., manag, save, rest,...\n",
       "3     my whole body feels itchy and like its on fire      0     my whole body feels itchy and like its on fire             [whole, bodi, feel, itchi, like, fire]\n",
       "4  no, it's not behaving at all. i'm mad. why am ...      0  no, it's not behaving at all. i'm mad. why am ...  [no, ,, 's, not, behav, ., 'm, mad, ., ?, ca, ..."
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c1202f2e7a3d1844",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T11:01:02.321408Z",
     "start_time": "2025-04-17T10:59:45.198737Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Shosh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top trigrams:\n",
      "              Trigram  Frequency\n",
      "460      (i, do, n't)      21178\n",
      "82       (i, ca, n't)      18685\n",
      "71         (., i, 'm)      15925\n",
      "284     (i, have, to)      10964\n",
      "347     (i, want, to)      10049\n",
      "349      (to, go, to)       9596\n",
      "122       (., it, 's)       9319\n",
      "600        (!, i, 'm)       8964\n",
      "1701  (ca, n't, wait)       8961\n",
      "4835       (,, i, 'm)       8816\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Initialize stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Download tokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to extract trigrams\n",
    "def extract_trigrams(text):\n",
    "    tokens = word_tokenize(text.lower())  # tokenize and lowercase\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return list(ngrams(stemmed_tokens, 3))\n",
    "\n",
    "# Extract trigrams from all tweets\n",
    "data['trigrams_token'] = data['normalized_tweet'].apply(extract_trigrams)\n",
    "\n",
    "# Flatten all trigrams into a single list\n",
    "all_trigrams = [trigram for tweet_trigrams in data['trigrams_token'] for trigram in tweet_trigrams]\n",
    "\n",
    "# Count trigram frequencies\n",
    "trigram_freq = Counter(all_trigrams)\n",
    "\n",
    "# Convert to DataFrame for better viewing\n",
    "trigram_df = pd.DataFrame(trigram_freq.items(), columns=['Trigram', 'Frequency']).sort_values(by='Frequency', ascending=False)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nTop trigrams:\")\n",
    "print(trigram_df.head(10))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "25e17db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert trigrams to space-separated strings\n",
    "# text_data = data['trigrams_token'].apply(lambda trigrams: ' '.join(['_'.join(trigram) for trigram in trigrams]))\n",
    "\n",
    "# # Initialize and fit TF-IDF vectorizer\n",
    "# vectorizer = TfidfVectorizer(max_features=5000)\n",
    "# vectorizer.fit(text_data)\n",
    "\n",
    "# # Save the fitted TF-IDF vectorizer\n",
    "# with open(\"fitted_tfidf.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(vectorizer, f)\n",
    "\n",
    "# print(\"TF-IDF Vectorizer fitted and saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "35c6ef21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Convert trigrams to space-separated strings\n",
    "# text_data = data['trigrams_token'].apply(lambda trigrams: ' '.join(['_'.join(trigram) for trigram in trigrams]))\n",
    "\n",
    "# # Initialize and fit TF-IDF vectorizer\n",
    "# vectorizer = TfidfVectorizer(max_features=5000)\n",
    "# vectorizer.fit(text_data)\n",
    "\n",
    "# # Print a few transformed examples\n",
    "# tfidf_transformed = vectorizer.transform(text_data)\n",
    "\n",
    "# # Get feature names (trigrams in the vocabulary)\n",
    "# feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# # Convert TF-IDF matrix to a DataFrame for better readability\n",
    "# tfidf_df = pd.DataFrame(tfidf_transformed.toarray(), columns=feature_names)\n",
    "\n",
    "# # Display the first few rows to review the vectorized output\n",
    "# print(\"Sample vectorized output:\")\n",
    "# print(tfidf_df.head())\n",
    "\n",
    "# # Save the fitted TF-IDF vectorizer\n",
    "# with open(\"fitted_tfidf.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(vectorizer, f)\n",
    "\n",
    "# print(\"TF-IDF Vectorizer fitted and saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1a417f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # Save the function and stemmer\n",
    "# with open(\"tokenizer.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(extract_trigrams, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2f3691c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>target</th>\n",
       "      <th>normalized_tweet</th>\n",
       "      <th>tokens</th>\n",
       "      <th>trigrams_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- A that's a bummer. You shoulda got David Car...</td>\n",
       "      <td>0</td>\n",
       "      <td>a that's a bummer. you shoulda got david carr ...</td>\n",
       "      <td>['s, bummer, ., shoulda, got, david, carr, thi...</td>\n",
       "      <td>[(a, that, 's), (that, 's, a), ('s, a, bummer)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his facebook by ...</td>\n",
       "      <td>[upset, ca, n't, updat, facebook, text, ., mig...</td>\n",
       "      <td>[(is, upset, that), (upset, that, he), (that, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I dived many times for the ball. Managed to sa...</td>\n",
       "      <td>0</td>\n",
       "      <td>i dived many times for the ball. managed to sa...</td>\n",
       "      <td>[dive, mani, time, ball, ., manag, save, rest,...</td>\n",
       "      <td>[(i, dive, mani), (dive, mani, time), (mani, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[whole, bodi, feel, itchi, like, fire]</td>\n",
       "      <td>[(my, whole, bodi), (whole, bodi, feel), (bodi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no, it's not behaving at all. i'm mad. why am ...</td>\n",
       "      <td>0</td>\n",
       "      <td>no, it's not behaving at all. i'm mad. why am ...</td>\n",
       "      <td>[no, ,, 's, not, behav, ., 'm, mad, ., ?, ca, ...</td>\n",
       "      <td>[(no, ,, it), (,, it, 's), (it, 's, not), ('s,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>not the whole crew</td>\n",
       "      <td>0</td>\n",
       "      <td>not the whole crew</td>\n",
       "      <td>[not, whole, crew]</td>\n",
       "      <td>[(not, the, whole), (the, whole, crew)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Need a hug</td>\n",
       "      <td>0</td>\n",
       "      <td>need a hug</td>\n",
       "      <td>[need, hug]</td>\n",
       "      <td>[(need, a, hug)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hey long time no see! Yes.. Rains a bit ,only ...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey long time no see! yes. rains a bit ,only a...</td>\n",
       "      <td>[hey, long, time, no, see, !, ye, ., rain, bit...</td>\n",
       "      <td>[(hey, long, time), (long, time, no), (time, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nope they didn't have it</td>\n",
       "      <td>0</td>\n",
       "      <td>nope they didn't have it</td>\n",
       "      <td>[nope, n't]</td>\n",
       "      <td>[(nope, they, did), (they, did, n't), (did, n'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>spring break in plain city... it's snowing</td>\n",
       "      <td>0</td>\n",
       "      <td>spring break in plain city. it's snowing</td>\n",
       "      <td>[spring, break, plain, citi, ., 's, snow]</td>\n",
       "      <td>[(spring, break, in), (break, in, plain), (in,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I just re-pierced my ears</td>\n",
       "      <td>0</td>\n",
       "      <td>i just repierced my ears</td>\n",
       "      <td>[repierc, ear]</td>\n",
       "      <td>[(i, just, repierc), (just, repierc, my), (rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I couldn't bear to watch it. And I thought the...</td>\n",
       "      <td>0</td>\n",
       "      <td>i couldn't bear to watch it. and i thought the...</td>\n",
       "      <td>[could, n't, bear, watch, ., thought, ua, loss...</td>\n",
       "      <td>[(i, could, n't), (could, n't, bear), (n't, be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>It it counts, idk why I did either. you never ...</td>\n",
       "      <td>0</td>\n",
       "      <td>it it counts, idk why i did either. you never ...</td>\n",
       "      <td>[count, ,, idk, either, ., never, talk, anymor]</td>\n",
       "      <td>[(it, it, count), (it, count, ,), (count, ,, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>i would've been the first, but i didn't have a...</td>\n",
       "      <td>0</td>\n",
       "      <td>i would've been the first, but i didn't have a...</td>\n",
       "      <td>[would, 've, first, ,, n't, gun, ., not, reall...</td>\n",
       "      <td>[(i, would, 've), (would, 've, been), ('ve, be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I wish I got to watch it with you!! I miss you...</td>\n",
       "      <td>0</td>\n",
       "      <td>i wish i got to watch it with you! i miss you ...</td>\n",
       "      <td>[wish, got, watch, !, miss, premier, ?, !]</td>\n",
       "      <td>[(i, wish, i), (wish, i, got), (i, got, to), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Hollis' death scene will hurt me severely to w...</td>\n",
       "      <td>0</td>\n",
       "      <td>hollis' death scene will hurt me severely to w...</td>\n",
       "      <td>[holli, ', death, scene, hurt, sever, watch, f...</td>\n",
       "      <td>[(holli, ', death), (', death, scene), (death,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>about to file taxes</td>\n",
       "      <td>0</td>\n",
       "      <td>about to file taxes</td>\n",
       "      <td>[file, tax]</td>\n",
       "      <td>[(about, to, file), (to, file, tax)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ahh ive always wanted to see rent love the sou...</td>\n",
       "      <td>0</td>\n",
       "      <td>ahh ive always wanted to see rent love the sou...</td>\n",
       "      <td>[ahh, ive, alway, want, see, rent, love, sound...</td>\n",
       "      <td>[(ahh, ive, alway), (ive, alway, want), (alway...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Oh dear. Were you drinking out of the forgotte...</td>\n",
       "      <td>0</td>\n",
       "      <td>oh dear. were you drinking out of the forgotte...</td>\n",
       "      <td>[oh, dear, ., drink, forgotten, tabl, drink, ?]</td>\n",
       "      <td>[(oh, dear, .), (dear, ., were), (., were, you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>i was out most of the day so didn't get much done</td>\n",
       "      <td>0</td>\n",
       "      <td>i was out most of the day so didn't get much done</td>\n",
       "      <td>[day, n't, get, much, done]</td>\n",
       "      <td>[(i, wa, out), (wa, out, most), (out, most, of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweet target                                   normalized_tweet                                             tokens                                     trigrams_token\n",
       "0   - A that's a bummer. You shoulda got David Car...      0  a that's a bummer. you shoulda got david carr ...  ['s, bummer, ., shoulda, got, david, carr, thi...  [(a, that, 's), (that, 's, a), ('s, a, bummer)...\n",
       "1   is upset that he can't update his Facebook by ...      0  is upset that he can't update his facebook by ...  [upset, ca, n't, updat, facebook, text, ., mig...  [(is, upset, that), (upset, that, he), (that, ...\n",
       "2   I dived many times for the ball. Managed to sa...      0  i dived many times for the ball. managed to sa...  [dive, mani, time, ball, ., manag, save, rest,...  [(i, dive, mani), (dive, mani, time), (mani, t...\n",
       "3      my whole body feels itchy and like its on fire      0     my whole body feels itchy and like its on fire             [whole, bodi, feel, itchi, like, fire]  [(my, whole, bodi), (whole, bodi, feel), (bodi...\n",
       "4   no, it's not behaving at all. i'm mad. why am ...      0  no, it's not behaving at all. i'm mad. why am ...  [no, ,, 's, not, behav, ., 'm, mad, ., ?, ca, ...  [(no, ,, it), (,, it, 's), (it, 's, not), ('s,...\n",
       "5                                  not the whole crew      0                                 not the whole crew                                 [not, whole, crew]            [(not, the, whole), (the, whole, crew)]\n",
       "6                                          Need a hug      0                                         need a hug                                        [need, hug]                                   [(need, a, hug)]\n",
       "7   hey long time no see! Yes.. Rains a bit ,only ...      0  hey long time no see! yes. rains a bit ,only a...  [hey, long, time, no, see, !, ye, ., rain, bit...  [(hey, long, time), (long, time, no), (time, n...\n",
       "8                            nope they didn't have it      0                           nope they didn't have it                                        [nope, n't]  [(nope, they, did), (they, did, n't), (did, n'...\n",
       "10         spring break in plain city... it's snowing      0           spring break in plain city. it's snowing          [spring, break, plain, citi, ., 's, snow]  [(spring, break, in), (break, in, plain), (in,...\n",
       "11                          I just re-pierced my ears      0                           i just repierced my ears                                     [repierc, ear]  [(i, just, repierc), (just, repierc, my), (rep...\n",
       "12  I couldn't bear to watch it. And I thought the...      0  i couldn't bear to watch it. and i thought the...  [could, n't, bear, watch, ., thought, ua, loss...  [(i, could, n't), (could, n't, bear), (n't, be...\n",
       "13  It it counts, idk why I did either. you never ...      0  it it counts, idk why i did either. you never ...    [count, ,, idk, either, ., never, talk, anymor]  [(it, it, count), (it, count, ,), (count, ,, i...\n",
       "14  i would've been the first, but i didn't have a...      0  i would've been the first, but i didn't have a...  [would, 've, first, ,, n't, gun, ., not, reall...  [(i, would, 've), (would, 've, been), ('ve, be...\n",
       "15  I wish I got to watch it with you!! I miss you...      0  i wish i got to watch it with you! i miss you ...         [wish, got, watch, !, miss, premier, ?, !]  [(i, wish, i), (wish, i, got), (i, got, to), (...\n",
       "16  Hollis' death scene will hurt me severely to w...      0  hollis' death scene will hurt me severely to w...  [holli, ', death, scene, hurt, sever, watch, f...  [(holli, ', death), (', death, scene), (death,...\n",
       "17                                about to file taxes      0                                about to file taxes                                        [file, tax]               [(about, to, file), (to, file, tax)]\n",
       "18  ahh ive always wanted to see rent love the sou...      0  ahh ive always wanted to see rent love the sou...  [ahh, ive, alway, want, see, rent, love, sound...  [(ahh, ive, alway), (ive, alway, want), (alway...\n",
       "19  Oh dear. Were you drinking out of the forgotte...      0  oh dear. were you drinking out of the forgotte...    [oh, dear, ., drink, forgotten, tabl, drink, ?]  [(oh, dear, .), (dear, ., were), (., were, you...\n",
       "20  i was out most of the day so didn't get much done      0  i was out most of the day so didn't get much done                        [day, n't, get, much, done]  [(i, wa, out), (wa, out, most), (out, most, of..."
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a168c477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                   tweet  target\n",
      "800000                                                                                                      I LOVE @Health4UandPets u guys r the best!!        4\n",
      "800001                                                                          im meeting up with one of my besties tonight! Cant wait!!  - GIRL TALK!!       4\n",
      "800002         @DaRealSunisaKim Thanks for the Twitter add, Sunisa! I got to meet you once at a HIN show here in the DC area and you were a sweetheart.        4\n",
      "800003                                          Being sick can be really cheap when it hurts too much to eat real food  Plus, your friends make you soup       4\n",
      "800004                                                                                                   @LovesBrooklyn2 he has that effect on everyone        4\n",
      "800005             @ProductOfFear You can tell him that I just burst out laughing really loud because of that  Thanks for making me come out of my sulk!       4\n",
      "800006                                                                             @r_keith_hill Thans for your response. Ihad already find this answer        4\n",
      "800007                                @KeepinUpWKris I am so jealous, hope you had a great time in vegas! how did you like the ACM's?! LOVE YOUR SHOW!!        4\n",
      "800008                                                                                   @tommcfly ah, congrats mr fletcher for finally joining twitter        4\n",
      "800009                                                                               @e4VoIP I RESPONDED  Stupid cat is helping me type. Forgive errors        4\n",
      "800010                  crazy day of school. there for 10 hours straiiight. about to watch the hills. @spencerpratt told me too! ha. happy birthday JB!        4\n",
      "800011                                                               @naughtyhaughty HOW DID I FORGET ABOUT TWO AND A HALF MEN?!?!? I LOVE THAT SHOW!!!        4\n",
      "800012                                                                                   @nileyjileyluver Haha, don't worry! You'll get the hang of it!        4\n",
      "800013                   @soundwav2010 At least I won't be the only one feeling lost! This may cause me many later than usual nights, already addicting        4\n",
      "800014  @LutheranLucciol Make sure you DM me if you post a link to that video! &lt;LOL&gt;So I don't miss it   Better get permission and blessing first?       4\n",
      "800015                                                                                                              Just added tweetie to my new iPhone        4\n",
      "800016                        @michellardi i really don't know. i think its Globe!  yeah! sana gumaling na ko para alam ko na din kung makakasama ako! )       4\n",
      "800017                                                                                                        @nicolerichie: your picture is very sweet        4\n",
      "800018                             Catching Up on Emails, RSS and Random BACN. Then I'm cutting out early tonight (11:30PM) to have Dinner with @lauraw        4\n",
      "800019                                                      Dancing around the room in Pjs, jamming to my ipod. Getting dizzy. Well twitter, you asked!        4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ensure full text is displayed\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Filter for positive class where target == 4\n",
    "df_positive = df[df['target'] == 4]\n",
    "\n",
    "# Display first 20 rows without truncation\n",
    "print(df_positive[['tweet', 'target']].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97777d20a2192b5",
   "metadata": {},
   "source": [
    "## bazbt el target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a81cf385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>target</th>\n",
       "      <th>normalized_tweet</th>\n",
       "      <th>tokens</th>\n",
       "      <th>trigrams_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- A that's a bummer. You shoulda got David Carr of Third Day to do it. ;D</td>\n",
       "      <td>0</td>\n",
       "      <td>a that's a bummer. you shoulda got david carr of third day to do it. d</td>\n",
       "      <td>['s, bummer, ., shoulda, got, david, carr, third, day, .]</td>\n",
       "      <td>[(a, that, 's), (that, 's, a), ('s, a, bummer), (a, bummer, .), (bummer, ., you), (., you, shoulda), (you, shoulda, got), (shoulda, got, david), (got, david, carr), (david, carr, of), (carr, of, third), (of, third, day), (third, day, to), (day, to, do), (to, do, it), (do, it, .), (it, ., d)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can't update his Facebook by texting it... and might cry as a result School today also. Blah!</td>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his facebook by texting it. and might cry as a result school today also. blah!</td>\n",
       "      <td>[upset, ca, n't, updat, facebook, text, ., might, cri, result, school, today, also, ., blah, !]</td>\n",
       "      <td>[(is, upset, that), (upset, that, he), (that, he, ca), (he, ca, n't), (ca, n't, updat), (n't, updat, hi), (updat, hi, facebook), (hi, facebook, by), (facebook, by, text), (by, text, it), (text, it, .), (it, ., and), (., and, might), (and, might, cri), (might, cri, as), (cri, as, a), (as, a, result), (a, result, school), (result, school, today), (school, today, also), (today, also, .), (also, ., blah), (., blah, !)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I dived many times for the ball. Managed to save 50% The rest go out of bounds</td>\n",
       "      <td>0</td>\n",
       "      <td>i dived many times for the ball. managed to save  the rest go out of bounds</td>\n",
       "      <td>[dive, mani, time, ball, ., manag, save, rest, go, bound]</td>\n",
       "      <td>[(i, dive, mani), (dive, mani, time), (mani, time, for), (time, for, the), (for, the, ball), (the, ball, .), (ball, ., manag), (., manag, to), (manag, to, save), (to, save, the), (save, the, rest), (the, rest, go), (rest, go, out), (go, out, of), (out, of, bound)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[whole, bodi, feel, itchi, like, fire]</td>\n",
       "      <td>[(my, whole, bodi), (whole, bodi, feel), (bodi, feel, itchi), (feel, itchi, and), (itchi, and, like), (and, like, it), (like, it, on), (it, on, fire)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.</td>\n",
       "      <td>0</td>\n",
       "      <td>no, it's not behaving at all. i'm mad. why am i here? because i can't see you all over there.</td>\n",
       "      <td>[no, ,, 's, not, behav, ., 'm, mad, ., ?, ca, n't, see, .]</td>\n",
       "      <td>[(no, ,, it), (,, it, 's), (it, 's, not), ('s, not, behav), (not, behav, at), (behav, at, all), (at, all, .), (all, ., i), (., i, 'm), (i, 'm, mad), ('m, mad, .), (mad, ., whi), (., whi, am), (whi, am, i), (am, i, here), (i, here, ?), (here, ?, becaus), (?, becaus, i), (becaus, i, ca), (i, ca, n't), (ca, n't, see), (n't, see, you), (see, you, all), (you, all, over), (all, over, there), (over, there, .)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                            tweet target                                                                                              normalized_tweet                                                                                           tokens                                                                                                                                                                                                                                                                                                                                                                                                                      trigrams_token\n",
       "0                                       - A that's a bummer. You shoulda got David Carr of Third Day to do it. ;D      0                                        a that's a bummer. you shoulda got david carr of third day to do it. d                                        ['s, bummer, ., shoulda, got, david, carr, third, day, .]                                                                                                                                [(a, that, 's), (that, 's, a), ('s, a, bummer), (a, bummer, .), (bummer, ., you), (., you, shoulda), (you, shoulda, got), (shoulda, got, david), (got, david, carr), (david, carr, of), (carr, of, third), (of, third, day), (third, day, to), (day, to, do), (to, do, it), (do, it, .), (it, ., d)]\n",
       "1  is upset that he can't update his Facebook by texting it... and might cry as a result School today also. Blah!      0  is upset that he can't update his facebook by texting it. and might cry as a result school today also. blah!  [upset, ca, n't, updat, facebook, text, ., might, cri, result, school, today, also, ., blah, !]  [(is, upset, that), (upset, that, he), (that, he, ca), (he, ca, n't), (ca, n't, updat), (n't, updat, hi), (updat, hi, facebook), (hi, facebook, by), (facebook, by, text), (by, text, it), (text, it, .), (it, ., and), (., and, might), (and, might, cri), (might, cri, as), (cri, as, a), (as, a, result), (a, result, school), (result, school, today), (school, today, also), (today, also, .), (also, ., blah), (., blah, !)]\n",
       "2                                  I dived many times for the ball. Managed to save 50% The rest go out of bounds      0                                   i dived many times for the ball. managed to save  the rest go out of bounds                                        [dive, mani, time, ball, ., manag, save, rest, go, bound]                                                                                                                                                            [(i, dive, mani), (dive, mani, time), (mani, time, for), (time, for, the), (for, the, ball), (the, ball, .), (ball, ., manag), (., manag, to), (manag, to, save), (to, save, the), (save, the, rest), (the, rest, go), (rest, go, out), (go, out, of), (out, of, bound)]\n",
       "3                                                                  my whole body feels itchy and like its on fire      0                                                                my whole body feels itchy and like its on fire                                                           [whole, bodi, feel, itchi, like, fire]                                                                                                                                                                                                                                                                              [(my, whole, bodi), (whole, bodi, feel), (bodi, feel, itchi), (feel, itchi, and), (itchi, and, like), (and, like, it), (like, it, on), (it, on, fire)]\n",
       "4                   no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.      0                 no, it's not behaving at all. i'm mad. why am i here? because i can't see you all over there.                                       [no, ,, 's, not, behav, ., 'm, mad, ., ?, ca, n't, see, .]              [(no, ,, it), (,, it, 's), (it, 's, not), ('s, not, behav), (not, behav, at), (behav, at, all), (at, all, .), (all, ., i), (., i, 'm), (i, 'm, mad), ('m, mad, .), (mad, ., whi), (., whi, am), (whi, am, i), (am, i, here), (i, here, ?), (here, ?, becaus), (?, becaus, i), (becaus, i, ca), (i, ca, n't), (ca, n't, see), (n't, see, you), (see, you, all), (you, all, over), (all, over, there), (over, there, .)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9ac23122bf439be1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T02:14:57.832101Z",
     "start_time": "2025-04-17T02:14:57.739309Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.int64(0) np.int64(4)]\n"
     ]
    }
   ],
   "source": [
    "print(data[\"target\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3d4db1eace5e1b4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T02:14:58.688495Z",
     "start_time": "2025-04-17T02:14:57.839608Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 4]\n"
     ]
    }
   ],
   "source": [
    "# Convert target to integers if they are valid\n",
    "data[\"target\"] = data[\"target\"].apply(lambda x: int(x) if isinstance(x, (int, float)) else x)\n",
    "\n",
    "# Check unique values again to see if they are consistent\n",
    "print(data[\"target\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a88be3253fc62e6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T02:14:58.785204Z",
     "start_time": "2025-04-17T02:14:58.779038Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Convert target values according to the specified mapping\n",
    "# data[\"target\"] = data[\"target\"].replace({0: -1, 2: 0, 4: 1})\n",
    "#\n",
    "# # Check unique values again to see if they are consistent\n",
    "# print(data[\"target\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "28ff7228d50830e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T02:14:59.171298Z",
     "start_time": "2025-04-17T02:14:58.869176Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  1]\n"
     ]
    }
   ],
   "source": [
    "# Convert target values: 0 → -1, 4 → 1\n",
    "data[\"target\"] = data[\"target\"].replace({0: -1, 4: 1})\n",
    "\n",
    "# Check unique values again to ensure correctness\n",
    "print(data[\"target\"].unique())\n",
    "# Drop neutral tweets if still present\n",
    "data = data[data[\"target\"] != 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "75405276bd96ea6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T09:43:43.885415Z",
     "start_time": "2025-04-17T09:43:43.873112Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>target</th>\n",
       "      <th>normalized_tweet</th>\n",
       "      <th>tokens</th>\n",
       "      <th>trigrams_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- A that's a bummer. You shoulda got David Carr of Third Day to do it. ;D</td>\n",
       "      <td>-1</td>\n",
       "      <td>a that's a bummer. you shoulda got david carr of third day to do it. d</td>\n",
       "      <td>['s, bummer, ., shoulda, got, david, carr, third, day, .]</td>\n",
       "      <td>[(a, that, 's), (that, 's, a), ('s, a, bummer), (a, bummer, .), (bummer, ., you), (., you, shoulda), (you, shoulda, got), (shoulda, got, david), (got, david, carr), (david, carr, of), (carr, of, third), (of, third, day), (third, day, to), (day, to, do), (to, do, it), (do, it, .), (it, ., d)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can't update his Facebook by texting it... and might cry as a result School today also. Blah!</td>\n",
       "      <td>-1</td>\n",
       "      <td>is upset that he can't update his facebook by texting it. and might cry as a result school today also. blah!</td>\n",
       "      <td>[upset, ca, n't, updat, facebook, text, ., might, cri, result, school, today, also, ., blah, !]</td>\n",
       "      <td>[(is, upset, that), (upset, that, he), (that, he, ca), (he, ca, n't), (ca, n't, updat), (n't, updat, hi), (updat, hi, facebook), (hi, facebook, by), (facebook, by, text), (by, text, it), (text, it, .), (it, ., and), (., and, might), (and, might, cri), (might, cri, as), (cri, as, a), (as, a, result), (a, result, school), (result, school, today), (school, today, also), (today, also, .), (also, ., blah), (., blah, !)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I dived many times for the ball. Managed to save 50% The rest go out of bounds</td>\n",
       "      <td>-1</td>\n",
       "      <td>i dived many times for the ball. managed to save  the rest go out of bounds</td>\n",
       "      <td>[dive, mani, time, ball, ., manag, save, rest, go, bound]</td>\n",
       "      <td>[(i, dive, mani), (dive, mani, time), (mani, time, for), (time, for, the), (for, the, ball), (the, ball, .), (ball, ., manag), (., manag, to), (manag, to, save), (to, save, the), (save, the, rest), (the, rest, go), (rest, go, out), (go, out, of), (out, of, bound)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>-1</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[whole, bodi, feel, itchi, like, fire]</td>\n",
       "      <td>[(my, whole, bodi), (whole, bodi, feel), (bodi, feel, itchi), (feel, itchi, and), (itchi, and, like), (and, like, it), (like, it, on), (it, on, fire)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.</td>\n",
       "      <td>-1</td>\n",
       "      <td>no, it's not behaving at all. i'm mad. why am i here? because i can't see you all over there.</td>\n",
       "      <td>[no, ,, 's, not, behav, ., 'm, mad, ., ?, ca, n't, see, .]</td>\n",
       "      <td>[(no, ,, it), (,, it, 's), (it, 's, not), ('s, not, behav), (not, behav, at), (behav, at, all), (at, all, .), (all, ., i), (., i, 'm), (i, 'm, mad), ('m, mad, .), (mad, ., whi), (., whi, am), (whi, am, i), (am, i, here), (i, here, ?), (here, ?, becaus), (?, becaus, i), (becaus, i, ca), (i, ca, n't), (ca, n't, see), (n't, see, you), (see, you, all), (you, all, over), (all, over, there), (over, there, .)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                            tweet  target                                                                                              normalized_tweet                                                                                           tokens                                                                                                                                                                                                                                                                                                                                                                                                                      trigrams_token\n",
       "0                                       - A that's a bummer. You shoulda got David Carr of Third Day to do it. ;D      -1                                        a that's a bummer. you shoulda got david carr of third day to do it. d                                        ['s, bummer, ., shoulda, got, david, carr, third, day, .]                                                                                                                                [(a, that, 's), (that, 's, a), ('s, a, bummer), (a, bummer, .), (bummer, ., you), (., you, shoulda), (you, shoulda, got), (shoulda, got, david), (got, david, carr), (david, carr, of), (carr, of, third), (of, third, day), (third, day, to), (day, to, do), (to, do, it), (do, it, .), (it, ., d)]\n",
       "1  is upset that he can't update his Facebook by texting it... and might cry as a result School today also. Blah!      -1  is upset that he can't update his facebook by texting it. and might cry as a result school today also. blah!  [upset, ca, n't, updat, facebook, text, ., might, cri, result, school, today, also, ., blah, !]  [(is, upset, that), (upset, that, he), (that, he, ca), (he, ca, n't), (ca, n't, updat), (n't, updat, hi), (updat, hi, facebook), (hi, facebook, by), (facebook, by, text), (by, text, it), (text, it, .), (it, ., and), (., and, might), (and, might, cri), (might, cri, as), (cri, as, a), (as, a, result), (a, result, school), (result, school, today), (school, today, also), (today, also, .), (also, ., blah), (., blah, !)]\n",
       "2                                  I dived many times for the ball. Managed to save 50% The rest go out of bounds      -1                                   i dived many times for the ball. managed to save  the rest go out of bounds                                        [dive, mani, time, ball, ., manag, save, rest, go, bound]                                                                                                                                                            [(i, dive, mani), (dive, mani, time), (mani, time, for), (time, for, the), (for, the, ball), (the, ball, .), (ball, ., manag), (., manag, to), (manag, to, save), (to, save, the), (save, the, rest), (the, rest, go), (rest, go, out), (go, out, of), (out, of, bound)]\n",
       "3                                                                  my whole body feels itchy and like its on fire      -1                                                                my whole body feels itchy and like its on fire                                                           [whole, bodi, feel, itchi, like, fire]                                                                                                                                                                                                                                                                              [(my, whole, bodi), (whole, bodi, feel), (bodi, feel, itchi), (feel, itchi, and), (itchi, and, like), (and, like, it), (like, it, on), (it, on, fire)]\n",
       "4                   no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.      -1                 no, it's not behaving at all. i'm mad. why am i here? because i can't see you all over there.                                       [no, ,, 's, not, behav, ., 'm, mad, ., ?, ca, n't, see, .]              [(no, ,, it), (,, it, 's), (it, 's, not), ('s, not, behav), (not, behav, at), (behav, at, all), (at, all, .), (all, ., i), (., i, 'm), (i, 'm, mad), ('m, mad, .), (mad, ., whi), (., whi, am), (whi, am, i), (am, i, here), (i, here, ?), (here, ?, becaus), (?, becaus, i), (becaus, i, ca), (i, ca, n't), (ca, n't, see), (n't, see, you), (see, you, all), (you, all, over), (all, over, there), (over, there, .)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5a0af51dd024c7",
   "metadata": {},
   "source": [
    "## Saving data to new csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9d4e6c580e21bbc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T09:53:45.041178Z",
     "start_time": "2025-04-17T09:53:39.611799Z"
    }
   },
   "outputs": [],
   "source": [
    "#columns: normalized_tweet, tokens, target\n",
    "selected_cols = data[['normalized_tweet', 'tokens','trigrams_token', 'target']]\n",
    "\n",
    "# Save to CSV\n",
    "selected_cols.to_csv('English_cleaned.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
