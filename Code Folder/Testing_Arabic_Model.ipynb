{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["B8ow6kLuk7vF","1EHzz0Ix_6VC","PjaphrZVAI9O"],"authorship_tag":"ABX9TyPlRcCnS7HTLk6433GVsmpf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Clean_Text"],"metadata":{"id":"B8ow6kLuk7vF"}},{"cell_type":"code","source":["!pip install gensim pyarabic emoji farasapy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BIzMNXOn-SCd","executionInfo":{"status":"ok","timestamp":1747883436085,"user_tz":-180,"elapsed":18285,"user":{"displayName":"Habiba Habiba Mohamed Darwish Sayed","userId":"06249506762316426256"}},"outputId":"b4be0651-ebcf-4c0a-d61c-fb76c52ca4f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gensim\n","  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n","Collecting pyarabic\n","  Downloading PyArabic-0.6.15-py3-none-any.whl.metadata (10 kB)\n","Collecting emoji\n","  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n","Collecting farasapy\n","  Downloading farasapy-0.0.14-py3-none-any.whl.metadata (8.9 kB)\n","Collecting numpy<2.0,>=1.18.5 (from gensim)\n","  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n","  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from pyarabic) (1.17.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from farasapy) (2.32.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from farasapy) (4.67.1)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->farasapy) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->farasapy) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->farasapy) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->farasapy) (2025.4.26)\n","Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading PyArabic-0.6.15-py3-none-any.whl (126 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.4/126.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading emoji-2.14.1-py3-none-any.whl (590 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading farasapy-0.0.14-py3-none-any.whl (11 kB)\n","Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pyarabic, numpy, emoji, scipy, farasapy, gensim\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.0.2\n","    Uninstalling numpy-2.0.2:\n","      Successfully uninstalled numpy-2.0.2\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.15.3\n","    Uninstalling scipy-1.15.3:\n","      Successfully uninstalled scipy-1.15.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n","tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed emoji-2.14.1 farasapy-0.0.14 gensim-4.3.3 numpy-1.26.4 pyarabic-0.6.15 scipy-1.13.1\n"]}]},{"cell_type":"code","source":["!pip install numpy==1.26.4"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iEPU1krv_VE4","executionInfo":{"status":"ok","timestamp":1747883441801,"user_tz":-180,"elapsed":3268,"user":{"displayName":"Habiba Habiba Mohamed Darwish Sayed","userId":"06249506762316426256"}},"outputId":"22f5400d-05e3-4f61-b799-ceb92e1d5165"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n"]}]},{"cell_type":"code","source":["\n","\n","# --- Download NLTK Data ---\n","# Ensure necessary nltk data is downloaded for text processing\n","import nltk\n","try:\n","    nltk.data.find('tokenizers/punkt')\n","except (nltk.downloader.DownloadError, LookupError):\n","    print(\"Downloading punkt...\")\n","    nltk.download('punkt') # Using the standard 'punkt'\n","    print(\"Download complete.\")\n","\n","try:\n","    nltk.data.find('corpora/stopwords')\n","except (nltk.downloader.DownloadError, LookupError):\n","    print(\"Downloading stopwords...\")\n","    nltk.download('stopwords')\n","    print(\"Download complete.\")\n","\n","\n","# --- Mount Google Drive ---\n","# Necessary for accessing files stored in Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# --- Import Libraries ---\n","# Standard Libraries\n","import os\n","import pickle\n","import re\n","import string\n","import sys # Useful for path manipulation if needed later\n","\n","# Data Manipulation and Analysis\n","import pandas as pd\n","import numpy as np # Should now be compatible\n","import scipy # Should now be compatible\n","\n","# Machine Learning - General\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import accuracy_score, mean_squared_error, f1_score\n","\n","# Deep Learning - TensorFlow/Keras\n","import tensorflow as tf\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.layers import Concatenate, Embedding, Input, LSTM, Dense, Dropout\n","from tensorflow.keras.models import Model, Sequential, load_model\n","from tensorflow.keras.optimizers import Adam, Nadam, RMSprop\n","from tensorflow.keras.utils import to_categorical\n","\n","# Deep Learning - PyTorch\n","import torch\n","from torch.utils.data import TensorDataset, DataLoader\n","import torch.nn as nn # Neural network modules\n","import torch.optim # For optimizers\n","\n","# Natural Language Processing (NLP) and Text Processing\n","from tqdm import tqdm # For progress bars\n","import gensim # For word embedding models like FastText\n","from transformers import AutoTokenizer # For Transformer-based tokenizers\n","from pyarabic import araby as ar # For Arabic text processing functions\n","from nltk.corpus import stopwords # Now imported directly from nltk\n","from nltk.stem.isri import ISRIStemmer # Now imported directly from nltk.stem\n","from nltk.tokenize import word_tokenize # Now imported directly from nltk.tokenize\n","import emoji # For handling emojis\n","import arabic_reshaper # For reshaping Arabic text\n","from farasa.segmenter import FarasaSegmenter # For Farasa tools\n","from farasa.stemmer import FarasaStemmer\n","\n","# API Interactions\n","import praw # For Reddit API\n","\n","# --- Other Initializations ---\n","# Define the device for PyTorch models\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Initialize Farasa tools (moved to a later cell if preferred, but grouped here)\n","# Note: Requires Java to be installed (handled by !apt-get install openjdk-8-jdk earlier)\n","try:\n","    # Ensure Java is installed first if running this cell stand alone\n","    print(\"Installing OpenJDK...\")\n","    !apt-get install -y openjdk-8-jdk -qq > /dev/null\n","    os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\" # Set JAVA_HOME\n","    print(\"Initializing Farasa segmenter and stemmer...\")\n","    farasa_segmenter = FarasaSegmenter(interactive=True)\n","    farasa_stemmer = FarasaStemmer(interactive=True)\n","    print(\"Farasa initialization complete.\")\n","except Exception as e:\n","    print(f\"Warning: Could not initialize Farasa. Make sure Java is installed. Error: {e}\")\n","\n","\n","# Define common paths (adjust if necessary)\n","DATA_DIR = \"/content/drive/MyDrive/Data Science/Arabic Sentiment/\"\n","MODELS_DIR = os.path.join(DATA_DIR, \"Models2/\")\n","FEATURES_DIR = os.path.join(DATA_DIR, \"Features/\")\n","DATASETS_DIR = os.path.join(DATA_DIR, \"Datasets2/\")\n","\n","# Ensure necessary directories exist\n","for dir_path in [MODELS_DIR, FEATURES_DIR, DATASETS_DIR]:\n","    os.makedirs(dir_path, exist_ok=True)\n","\n","print(\"✅ All necessary libraries installed and imported.\")\n","print(f\"Using device: {device}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"yvY6Jpj2b364","outputId":"d0c457bb-25f8-42b9-a1ec-9feba34ac88f","executionInfo":{"status":"error","timestamp":1747883465086,"user_tz":-180,"elapsed":1841,"user":{"displayName":"Habiba Habiba Mohamed Darwish Sayed","userId":"06249506762316426256"}}},"execution_count":null,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"module 'nltk.downloader' has no attribute 'DownloadError'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-83581e491ee6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDownloadError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-83581e491ee6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDownloadError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Downloading punkt...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'punkt'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Using the standard 'punkt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'nltk.downloader' has no attribute 'DownloadError'"]}]},{"cell_type":"code","source":["\n","import pandas as pd\n","import numpy as np\n","import re\n","import os\n","import pickle\n","from tqdm import tqdm\n","import gensim\n","from transformers import AutoTokenizer\n","import nltk\n","from pyarabic import araby as ar\n","# from Stemmer import Stemmer\n","# import Stemmer\n","\n","# NLTK\n","from nltk.corpus import stopwords\n","from nltk.stem.isri import ISRIStemmer\n","from nltk.tokenize import word_tokenize\n","\n","# Standard Library\n","# !pip install numpy==1.26.4 pandas==2.2.2 scipy==1.13.1 gensim==4.3.2 tensorflow==2.18.0\n","\n","\n","import os\n","import pickle\n","import re\n","import string\n","import emoji\n","import nltk\n","import scipy\n","import numpy as np\n","import pandas as pd\n","import gensim\n","import tensorflow as tf\n","from pyarabic import araby as ar\n","# from Stemmer import Stemmer\n","# import Stemmer\n","\n","# NLTK\n","from nltk.corpus import stopwords\n","from nltk.stem.isri import ISRIStemmer\n","from nltk.tokenize import word_tokenize\n","\n","# Scikit-Learn\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics import accuracy_score, f1_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","\n","# TensorFlow/Keras\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.layers import Concatenate, Embedding, Input, LSTM\n","from tensorflow.keras.layers import Dense, Dropout\n","from tensorflow.keras.layers import Input\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.optimizers import Adam, Nadam, RMSprop\n","from tensorflow.keras.utils import to_categorical\n","\n","# Transformers\n","from transformers import AutoTokenizer"],"metadata":{"id":"P0opvL6-W53U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk # Import nltk\n","from nltk.corpus import stopwords\n","import string # Import string for punctuation - Added this import here\n","\n","nltk.download('punkt_tab')\n","nltk.download('stopwords')\n","\n","# Define Arabic and English punctuations\n","arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n","english_punctuations = string.punctuation\n","punctuations_list = arabic_punctuations + english_punctuations\n","\n","\n","# Load and filter Arabic stopwords\n","stop_words = set(stopwords.words('arabic'))\n","negation_words = {'لا', 'لم', 'لن', 'ما', 'ليس', 'بدون', 'غير'}\n","stop_words = stop_words.difference(negation_words)\n","\n","\n","def remove_stopwords(text):\n","    words = text.split()\n","    filtered_words = [word for word in words if word.lower() not in stop_words]\n","    return ' '.join(filtered_words)\n","\n","def remove_gibberish(text):\n","    arabic_pattern = re.compile(r'[^\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FF\\uFB50-\\uFDFF\\uFE70-\\uFEFF\\s.,!?؟]')\n","    return arabic_pattern.sub('', text)\n","\n","def remove_diacritics(text):\n","    return ar.strip_tashkeel(text)\n","\n","def remove_tatweel(text):\n","    return ar.strip_tatweel(text)\n","\n","def remove_emoji(text):\n","    return emoji.replace_emoji(text, replace='')\n","\n","def normalize_elongation(text):\n","    return re.sub(r'(.)\\1+', r'\\1\\1', text)\n","\n","def normalize_letters(text):\n","    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n","    text = re.sub(\"ى\", \"ي\", text)\n","    text = re.sub(\"ؤ\", \"و\", text)\n","    text = re.sub(\"ئ\", \"ي\", text)\n","    return text\n","\n","def clean_punctuation(text):\n","    keep = '،.؟!'  # punctuation to keep\n","    # Replace all punctuation (excluding \"keep\") with space\n","    pattern = rf'[{\"\".join(re.escape(c) for c in punctuations_list if c not in keep)}]'\n","    text = re.sub(pattern, ' ', text)\n","    text = re.sub(r'\\.{2,}', '.', text)  # Normalize multiple dots\n","    text = re.sub(r'\\s+', ' ', text).strip()  # Normalize spaces\n","    return text\n","\n","def remove_noise(text):\n","    text = re.sub(r'http\\S+|www.\\S+', '', text)  # URLs\n","    text = re.sub(r'@\\w+', '', text)            # Mentions\n","    text = re.sub(r'#\\w+', '', text)            # Hashtags\n","    text = re.sub(r'\\d+', '', text)             # Numbers\n","    text = re.sub(r'[a-zA-Z]', '', text)        # English letters\n","    text = re.sub(r'\\s+', ' ', text).strip()    # Extra spaces\n","    return text\n","\n","\n","def clean_text(text):\n","    if not isinstance(text, str):\n","        return \"\"\n","\n","    text = text.strip()\n","    text = remove_emoji(text)\n","    text = remove_noise(text)\n","    text = clean_punctuation(text)\n","    text = remove_gibberish(text)\n","    text = remove_diacritics(text)\n","    text = remove_tatweel(text)\n","    text = normalize_letters(text)\n","    text = normalize_elongation(text)\n","    text = remove_stopwords(text)\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","\n","    return text"],"metadata":{"id":"msDxmtXUlI4W","executionInfo":{"status":"ok","timestamp":1747883557032,"user_tz":-180,"elapsed":733,"user":{"displayName":"Habiba Habiba Mohamed Darwish Sayed","userId":"06249506762316426256"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"098a3b8f-5e9d-434a-a52d-c6e770969051"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]},{"cell_type":"code","source":["!apt-get install -y openjdk-8-jdk\n","!pip install farasapy\n"],"metadata":{"id":"zelIO4rBl63H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from farasa.segmenter import FarasaSegmenter\n","from farasa.stemmer import FarasaStemmer\n","from farasa.pos import FarasaPOSTagger\n","from farasa.ner import FarasaNamedEntityRecognizer"],"metadata":{"id":"qzHQIIQRmDSD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","import os\n","import pickle\n","from tqdm import tqdm\n","import gensim\n","from transformers import AutoTokenizer"],"metadata":{"id":"s02QJFqNmHcR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["farasa_segmenter = FarasaSegmenter(interactive=True)\n","farasa_stemmer = FarasaStemmer(interactive=True)\n","\n","# Initialize paths\n","DATA_DIR = \"/content/drive/MyDrive/Data Science/Arabic Sentiment/\"\n","MODELS_DIR = os.path.join(DATA_DIR, \"Models2/\")\n","FEATURES_DIR = os.path.join(DATA_DIR, \"Features/\")\n","DATASETS_DIR = os.path.join(DATA_DIR, \"Datasets2/\")\n","\n","# Ensure directories exist\n","for dir_path in [MODELS_DIR, FEATURES_DIR, DATASETS_DIR]:\n","    os.makedirs(dir_path, exist_ok=True)\n","\n","\n","# 1. Function to clean and tokenize text using Farasa\n","def clean_and_tokenize(texts):\n","    print(\"Cleaning and tokenizing texts...\")\n","    cleaned_texts = []\n","    tokenized_texts = []\n","\n","    for text in tqdm(texts):\n","        # Clean first\n","        cleaned = clean_text(text)\n","        cleaned_texts.append(cleaned if cleaned else \"\")\n","\n","        # Then tokenize using Farasa\n","        if cleaned:\n","            # Get segments using Farasa\n","            segments = farasa_segmenter.segment(cleaned).split()\n","            tokenized_texts.append(segments)\n","        else:\n","            tokenized_texts.append([])\n","\n","    return cleaned_texts, tokenized_texts\n","# 3. Function to stem tokenized text using Farasa\n","def stem_with_farasa(tokenized_texts):\n","    print(\"Stemming tokens with Farasa...\")\n","    stemmed_token_lists = []\n","\n","    for tokens in tqdm(tokenized_texts):\n","        if tokens:\n","            # Stem each token individually\n","            stemmed_tokens = []\n","            for token in tokens:\n","                stemmed = farasa_stemmer.stem(token)\n","                stemmed_tokens.append(stemmed)\n","            stemmed_token_lists.append(stemmed_tokens)\n","        else:\n","            stemmed_token_lists.append([])\n","\n","    return stemmed_token_lists"],"metadata":{"id":"xMSltxF7mKC4","executionInfo":{"status":"ok","timestamp":1747884035329,"user_tz":-180,"elapsed":463417,"user":{"displayName":"Habiba Habiba Mohamed Darwish Sayed","userId":"06249506762316426256"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"8610e79b-ae00-4570-e8c9-2fd2e03e2c88"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'farasa-api.qcri.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["100%|██████████| 241M/241M [07:30<00:00, 536kiB/s]\n"]},{"output_type":"stream","name":"stderr","text":["[2025-05-22 03:20:12,406 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n","[2025-05-22 03:20:17,307 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n"]}]},{"cell_type":"code","source":["import os\n","import numpy as np\n","import pickle\n","import gensim\n","from tqdm import tqdm\n","\n","# Define the paths\n","DATA_DIR = \"/content/drive/MyDrive/Data Science/Arabic Sentiment/\"\n","FEATURES_DIR = os.path.join(DATA_DIR, \"Features/\")\n","fasttext_model_path = os.path.join(DATA_DIR, \"Models/cc.ar.300.vec.gz\")\n","stemmed_tokens_path = os.path.join(FEATURES_DIR, \"stemmed_tokens.pkl\")\n","embeddings_path = os.path.join(FEATURES_DIR, \"fasttext_embeddings.npy\")"],"metadata":{"id":"mQY08zzpqfWt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#API_Request"],"metadata":{"id":"1EHzz0Ix_6VC"}},{"cell_type":"code","source":["# Reddit API Credentials\n","CLIENT_ID=\"CuUuQtvb_KjZLGsPX2NTvA\",\n","CLIENT_SECRET=\"F75_nPTomFNnYPGTFD_uEBmgr70vZg\",\n","USER_AGENT=\"script:RedditSentimentBot:v1.0 (by u/Right_Border_2666)\""],"metadata":{"id":"he_KNk4j7dF5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install praw"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5Sy4ZavfHZA4","executionInfo":{"status":"ok","timestamp":1747681529314,"user_tz":-180,"elapsed":7996,"user":{"displayName":"Habiba Habiba Mohamed Darwish Sayed","userId":"06249506762316426256"}},"outputId":"ed25e848-b3e0-4248-93fc-31546df30afb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting praw\n","  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n","Collecting prawcore<3,>=2.4 (from praw)\n","  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n","Collecting update_checker>=0.18 (from praw)\n","  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n","Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.11/dist-packages (from praw) (1.8.0)\n","Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.4.26)\n","Downloading praw-7.8.1-py3-none-any.whl (189 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.3/189.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n","Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n","Installing collected packages: update_checker, prawcore, praw\n","Successfully installed praw-7.8.1 prawcore-2.4.0 update_checker-0.18.0\n"]}]},{"cell_type":"code","source":["import praw\n","\n","# Extracted credentials\n","reddit = praw.Reddit(\n","    client_id=\"CuUuQtvb_KjZLGsPX2NTvA\",\n","    client_secret=\"F75_nPTomFNnYPGTFD_uEBmgr70vZg\",\n","    user_agent=\"script:RedditSentimentBot:v1.0 (by u/Right_Border_2666)\"\n",")\n","\n","def test_connection():\n","    try:\n","        print(\"Testing Reddit API connection...\")\n","\n","        # Test API access\n","        subreddit = reddit.subreddit(\"worldnews\")\n","        for post in subreddit.hot(limit=5):\n","            print(f\"Title: {post.title[:50]}... | Upvotes: {post.score}\")\n","\n","        print(\"\\n✅ Connection successful!\")\n","        return True\n","\n","    except Exception as e:\n","        print(f\"\\n❌ Connection failed. Error: {e}\")\n","        print(\"\\nTroubleshooting:\")\n","        print(\"1. Verify credentials are correct\")\n","        print(\"2. Check if app is set to 'script' type\")\n","        print(\"3. Ensure credentials haven't been revoked\")\n","        return False\n","\n","if __name__ == \"__main__\":\n","    test_connection()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6lLBgOxEHyxT","executionInfo":{"status":"ok","timestamp":1747681539899,"user_tz":-180,"elapsed":916,"user":{"displayName":"Habiba Habiba Mohamed Darwish Sayed","userId":"06249506762316426256"}},"outputId":"b928a542-4c78-4c39-b5be-fe311364e986"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n","It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n","See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n","\n"]},{"output_type":"stream","name":"stdout","text":["Testing Reddit API connection...\n","\n","❌ Connection failed. Error: received 403 HTTP response\n","\n","Troubleshooting:\n","1. Verify credentials are correct\n","2. Check if app is set to 'script' type\n","3. Ensure credentials haven't been revoked\n"]}]},{"cell_type":"markdown","source":["# Test_Bilstm_Models"],"metadata":{"id":"PjaphrZVAI9O"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.models import load_model\n","\n","# Define the path to your saved model\n","model_path = \"/content/drive/MyDrive/Data Science/Arabic Sentiment/Models/best_training_model.h5\"\n","\n","# Load the model\n","try:\n","    loaded_model = load_model(model_path)\n","    print(\"Model loaded successfully!\")\n","    loaded_model.summary() # Print the model summary to verify\n","except Exception as e:\n","    print(f\"Error loading the model: {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":474},"id":"SMLpiT-zkBJT","executionInfo":{"status":"ok","timestamp":1747884036259,"user_tz":-180,"elapsed":902,"user":{"displayName":"Habiba Habiba Mohamed Darwish Sayed","userId":"06249506762316426256"}},"outputId":"09cff553-1ccb-4b7a-b2e6-93cca64b572a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"output_type":"stream","name":"stdout","text":["Model loaded successfully!\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"functional\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m77,056\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m16,448\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m130\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">77,056</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m93,636\u001b[0m (365.77 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">93,636</span> (365.77 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m93,634\u001b[0m (365.76 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">93,634</span> (365.76 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","source":["\n","# !pip install --upgrade numpy scipy\n","# !pip install gensim\n","\n","import torch\n","import numpy as np\n","import gensim\n","from tqdm import tqdm\n","\n","# Assumes these are already implemented\n","# from your_utils import clean_and_tokenize, stem_with_farasa\n","\n","def process_texts_for_fasttext_model(text_list, fasttext_model_path, dim=300):\n","    \"\"\"\n","    Processes and embeds Arabic text using FastText for model inference.\n","\n","    Args:\n","        text_list (List[str]): Raw text list (e.g., from Reddit).\n","        fasttext_model_path (str): Path to .vec FastText model file.\n","        dim (int): Embedding dimensionality (default 300).\n","\n","    Returns:\n","        torch.Tensor: Tensor of shape (N, dim) ready for CNN/RNN models.\n","    \"\"\"\n","    print(\"Cleaning and tokenizing...\")\n","    cleaned, tokenized = clean_and_tokenize(text_list)\n","\n","    print(\"Stemming with Farasa...\")\n","    stemmed = stem_with_farasa(tokenized)\n","\n","    # Join tokens into strings (space-separated) for embedding\n","    stemmed_text = [' '.join(tokens) for tokens in stemmed]\n","\n","    print(\"Loading FastText model...\")\n","    fasttext_model = gensim.models.KeyedVectors.load_word2vec_format(fasttext_model_path)\n","\n","    def fasttext_embed(text_list, model, dim):\n","        embedded = []\n","        for text in text_list:\n","            tokens = text.strip().split()\n","            vectors = [model[word] for word in tokens if word in model]\n","            if vectors:\n","                vec = np.mean(vectors, axis=0)\n","            else:\n","                vec = np.zeros(dim)\n","            embedded.append(vec)\n","        return torch.tensor(np.array(embedded), dtype=torch.float32)\n","\n","    print(\"Creating embeddings...\")\n","    embeddings_tensor = fasttext_embed(stemmed_text, fasttext_model, dim)\n","\n","    return embeddings_tensor"],"metadata":{"id":"n1Cg7Yv1BU41"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !pip install emoji\n","import re\n","import emoji\n","# Install the arabic-reshaper library if you haven't already\n","# !pip install arabic-reshaper\n","\n","# Import the necessary library and assign it to 'ar'"],"metadata":{"id":"2j5meU5VPIw_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install praw"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2QfvvxoyoY8y","executionInfo":{"status":"ok","timestamp":1747883374509,"user_tz":-180,"elapsed":4559,"user":{"displayName":"Habiba Habiba Mohamed Darwish Sayed","userId":"06249506762316426256"}},"outputId":"3c1b399c-714c-459d-d46f-f210e38e37f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting praw\n","  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n","Collecting prawcore<3,>=2.4 (from praw)\n","  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n","Collecting update_checker>=0.18 (from praw)\n","  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n","Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.11/dist-packages (from praw) (1.8.0)\n","Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.4.26)\n","Downloading praw-7.8.1-py3-none-any.whl (189 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.3/189.3 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n","Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n","Installing collected packages: update_checker, prawcore, praw\n","Successfully installed praw-7.8.1 prawcore-2.4.0 update_checker-0.18.0\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.models import load_model\n","import praw\n","import pandas as pd\n","import numpy as np  # Import numpy for Keras prediction\n","\n","# Your existing processing function (assumed imported)\n","# from your_module import process_texts_for_fasttext_model\n","\n","def predict_reddit_sentiment_keras(\n","    reddit_client_id, reddit_client_secret, reddit_user_agent,\n","    subreddit_name, limit, fasttext_model_path,\n","    keras_model_path\n","):\n","    \"\"\"\n","    Fetch Reddit comments, process text, and predict sentiment with a saved Keras model.\n","\n","    Args:\n","        reddit_client_id, reddit_client_secret, reddit_user_agent: Reddit API credentials.\n","        subreddit_name (str): Subreddit to fetch comments from.\n","        limit (int): Number of comments to fetch.\n","        fasttext_model_path (str): Path to FastText .vec model.\n","        keras_model_path (str): Path to saved Keras model (.h5).\n","\n","    Returns:\n","        pd.DataFrame with comments and predicted labels.\n","    \"\"\"\n","\n","    # Initialize Reddit API\n","    reddit = praw.Reddit(\n","        client_id = \"5ePdG6FzGRkDSNdjW6L2AA\",\n","        client_secret = \"pcADpQzSpsQn5SuErSnbJfa4G6oOoA\",\n","        user_agent = \"script:RedditSentimentBot:v1.0 (by u/CorgiNegative621)\"\n","    )\n","\n","    # Fetch comments\n","    subreddit = reddit.subreddit(subreddit_name)\n","    comments = []\n","    for comment in subreddit.comments(limit=limit):\n","        if comment.body in (\"[removed]\", \"[deleted]\", \"\"):\n","            continue\n","        comments.append(comment.body)\n","\n","    if not comments:\n","        print(\"No valid comments fetched.\")\n","        return pd.DataFrame()\n","\n","    print(f\"Fetched {len(comments)} comments. Processing...\")\n","\n","    # Process text into embeddings (assuming process_texts_for_fasttext_model returns numpy array for Keras)\n","    # If it returns a torch tensor, convert it to numpy: embeddings.cpu().numpy()\n","    embeddings = process_texts_for_fasttext_model(\n","        text_list=comments,\n","        fasttext_model_path=fasttext_model_path,\n","        dim=300\n","    )\n","\n","    # Load the saved Keras model\n","    try:\n","        loaded_model = load_model(keras_model_path)\n","        print(\"Keras model loaded successfully!\")\n","    except Exception as e:\n","        print(f\"Error loading the Keras model: {e}\")\n","        return pd.DataFrame() # Return empty DataFrame if model loading fails\n","\n","\n","    # Predict with Keras model\n","    predictions = loaded_model.predict(embeddings)\n","\n","    # Get predicted labels (assuming one-hot encoding or similar output from Keras)\n","    # If your Keras model outputs probabilities for each class, use np.argmax\n","    predicted_labels = np.argmax(predictions, axis=1).tolist()\n","\n","\n","    # Map numeric labels to strings (customize as needed)\n","    # Make sure this matches the labels your Keras model was trained on\n","    label_map = {0: 'Negative', 1: 'Positive'}\n","    predicted_labels_str = [label_map.get(lbl, \"Unknown\") for lbl in predicted_labels]\n","\n","    # Return results as DataFrame\n","    df = pd.DataFrame({\n","        'comment': comments,\n","        'predicted_label': predicted_labels_str\n","    })\n","\n","    return df\n","\n","# Example usage (replace with your actual credentials and paths)\n","if __name__ == \"__main__\":\n","    client_id = \"5ePdG6FzGRkDSNdjW6L2AA\",\n","    client_secret = \"pcADpQzSpsQn5SuErSnbJfa4G6oOoA\",\n","    user_agent = \"script:RedditSentimentBot:v1.0 (by u/CorgiNegative621)\"\n","    KERAS_MODEL_PATH = \"/content/drive/MyDrive/Data Science/Arabic Sentiment/Models/best_training_model.h5\"\n","\n","\n","    df_results = predict_reddit_sentiment_keras(\n","        reddit_client_id=client_id,\n","        reddit_client_secret=client_secret,\n","        reddit_user_agent=user_agent,\n","        subreddit_name=\"arabic\",\n","        limit=20,\n","        fasttext_model_path=fasttext_model_path, # Assuming fasttext_model_path is defined elsewhere\n","        keras_model_path=KERAS_MODEL_PATH\n","    )\n","    print(df_results.head(20))"],"metadata":{"id":"O6fvkNFVBR52","executionInfo":{"status":"ok","timestamp":1747884369769,"user_tz":-180,"elapsed":333467,"user":{"displayName":"Habiba Habiba Mohamed Darwish Sayed","userId":"06249506762316426256"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9472a4cd-88e1-4829-dace-77fe95df706c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n","It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n","See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n","\n"]},{"output_type":"stream","name":"stdout","text":["Fetched 20 comments. Processing...\n","Cleaning and tokenizing...\n","Cleaning and tokenizing texts...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 20/20 [00:00<00:00, 50.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Stemming with Farasa...\n","Stemming tokens with Farasa...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 20/20 [00:00<00:00, 64.72it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loading FastText model...\n","Creating embeddings...\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"output_type":"stream","name":"stdout","text":["Keras model loaded successfully!\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 442ms/step\n","                                              comment predicted_label\n","0   يعني واللة بجد انا جربت بيع الصور \\nبيع الشعار...        Negative\n","1                                    شكرًا لأخلاقك ❤️        Positive\n","2   حبيبي الغالي وفقك الله وشكرا جدا علي كلماتك ال...        Positive\n","3   يعني اظلم بنت ملهاش ذنب عشان تمشي الحياه ؟ انا...        Negative\n","4   هيك حال الدنيا..\\nلو ما حبيت بعدها، بتضل عالق ...        Positive\n","5   اخاف أتجوز وأظلم البنت، ماهي بتكون عذرا وبريئه...        Positive\n","6   اسمحلي انتقدك، صراحة جدًا جميله وكلماتها معبره...        Positive\n","7                     وعليكم السلام، حب حلالاً بعدها.        Positive\n","8                                                 انا        Positive\n","9                                     طيب انصحنا بكتب        Positive\n","10                             عندك حق \\nشكرا لتعليقك        Positive\n","11  ألا يوجد تطبيقات لتذكيرك وتنبيهك بمهامك غير ال...        Positive\n","12  أنت تسمع فقط عن الأشخاص الرابحين لأن هذه القصص...        Positive\n","13  تبدو الكثير من الروايات مفتعلة أو أن أفكارها ت...        Negative\n","14  تحلي بالصبر وخذي الأمر ببطء، فلا بد أنه يخشى خ...        Positive\n","15  شكرًا، ذكرتني بالتوقف عن قضاء الكثير من الوقت ...        Positive\n","16                كيف؟ مجرد تحفيز أم تنويم مغناطيسي؟؟        Negative\n","17  انا البنت يلي بحياة هاد الرجل\\nكيف فيني اساعده...        Positive\n","18  بما أنك رجل مؤمن وكل أمرك لله وامضي قدمًا أنت ...        Positive\n","19  عن رأي شخصي ما حدا بيقدر يساعد حدا بموضوع متعل...        Positive\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Assuming df_results is your DataFrame\n","\n","# Set pandas display options to show all rows and columns\n","pd.set_option('display.max_rows', None)\n","pd.set_option('display.max_colwidth', None) # Set to None to show full width\n","\n","# Print the entire DataFrame\n","print(df_results)\n","\n","# Reset display options to default if you don't need them anymore\n","# pd.reset_option('display.max_rows')\n","# pd.reset_option('display.max_colwidth')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IG7rITSTvoBF","executionInfo":{"status":"ok","timestamp":1747884370120,"user_tz":-180,"elapsed":114,"user":{"displayName":"Habiba Habiba Mohamed Darwish Sayed","userId":"06249506762316426256"}},"outputId":"4fc96577-c950-433b-8675-26c8e48e51f5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                                                                                                                                                                                                                                                                                                                                                                                                                                  comment  \\\n","0                                                                                                                                                                                                                                        يعني واللة بجد انا جربت بيع الصور \\nبيع الشعارات \\nالتسويق بالعمولة  \\nالطباعة عند الطلب \\nبيع المقالات \\nبس مكسبتش ولا جنية لحد دلوك ضيعت وقت علي الفاضي حد جرب الطرق دي ونفعت معاه وجربها إزاي   \n","1                                                                                                                                                                                                                                                                                                                                                                                                                        شكرًا لأخلاقك ❤️   \n","2                                                                                                                                                                                                                                                                     حبيبي الغالي وفقك الله وشكرا جدا علي كلماتك الجميله ونصيحتك وسام لي قبل نقدك بل بالعكس اخذ نقدك واضعه ببرواز من ذهب يشبعك شكرا من القلب و ب اذن الله اكون احسن ابشر   \n","3                                                                                                                                                                                                                                                                                                                                         يعني اظلم بنت ملهاش ذنب عشان تمشي الحياه ؟ انا عايز احل مشكلتي مش ادخل طرف فيها\\nلا اقصد التهجم   \n","4                                                                                                                                                                                                                                                                                                                                                                                  هيك حال الدنيا..\\nلو ما حبيت بعدها، بتضل عالق في مكانك   \n","5                                                                                                                                                                                                                                                                                                                                            اخاف أتجوز وأظلم البنت، ماهي بتكون عذرا وبريئه واول جوازه بالنهايه انا اكون لسه منستش الاولى   \n","6                                                                                                                                                                                                                                                                                      اسمحلي انتقدك، صراحة جدًا جميله وكلماتها معبره لكن مارتبتها بالطريقه الصح\\nفي كلمات زائده ومكانها غلط، اقدر اسميها نثر اثر من قصيده\\nممتاز والله 💯   \n","7                                                                                                                                                                                                                                                                                                                                                                                                         وعليكم السلام، حب حلالاً بعدها.   \n","8                                                                                                                                                                                                                                                                                                                                                                                                                                     انا   \n","9                                                                                                                                                                                                                                                                                                                                                                                                                         طيب انصحنا بكتب   \n","10                                                                                                                                                                                                                                                                                                                                                                                                                 عندك حق \\nشكرا لتعليقك   \n","11                                                                                                                                                                                                                                                                                                                                                                                   ألا يوجد تطبيقات لتذكيرك وتنبيهك بمهامك غير المنجزة؟   \n","12                                                                                                                                                                                                              أنت تسمع فقط عن الأشخاص الرابحين لأن هذه القصص يتم مشاركتها على نطاق واسع والترويج لها من الشركات المعنية، لكن الغالبية العظمى من الناس خسروا ويخجلون جدا من التحدث عنه عبر الإنترنت، لذلك لا تسمع إلا جانبا فقط من القصة   \n","13                                                                                                                                                                                                                                                                                                                                                                                    تبدو الكثير من الروايات مفتعلة أو أن أفكارها تجارية   \n","14                                                                                                                                                                                                                                                                                                         تحلي بالصبر وخذي الأمر ببطء، فلا بد أنه يخشى خسارتك ويريد وقت للاطمئنان تجاهك و التأكد انك صادقة بمشاعرك ولم يفقد الحب من جديد   \n","15                                                                                                                                                                                                                                                                                                                            شكرًا، ذكرتني بالتوقف عن قضاء الكثير من الوقت على وسائل التواصل الاجتماعي، والإعجاب بمنشورات مثل منشورك هذا   \n","16                                                                                                                                                                                                                                                                                                                                                                                                    كيف؟ مجرد تحفيز أم تنويم مغناطيسي؟؟   \n","17                                                                                                                                                                                                                                                                                                                                        انا البنت يلي بحياة هاد الرجل\\nكيف فيني اساعده يتطمن ويحس بالامان \\nويعرف انه مو شفقة ولا تضحية   \n","18                                                                                                                                                                                                                                                                                                                                                                    بما أنك رجل مؤمن وكل أمرك لله وامضي قدمًا أنت تستحق أن تُحِب وتُحَب   \n","19  عن رأي شخصي ما حدا بيقدر يساعد حدا بموضوع متعلق فيو، يعني لو هو بدو يعطي فرصة جديدة لحالو وللحياة لازم هالشي يكون نابع من جواتو، القصة مو سهلة ابدا لهيك هو متأذي نفسياً كتير الحل الوحيد انو تحالو تخلو يطلع من يلي هو فيو وتلهو عن الفكرة، وتغيرولو شكل بكل الطرق وبعدين بتجي فكرة الشخص الجديد يعني هو بحاجة يتعافى نفسياً بأنو يشوف جوانب الحياة الحلوة التانية وانو الحياة فيا كتير نعم وقصص حلوة وبعدا بفكر بالارتباط مرة تانية   \n","\n","   predicted_label  \n","0         Negative  \n","1         Positive  \n","2         Positive  \n","3         Negative  \n","4         Positive  \n","5         Positive  \n","6         Positive  \n","7         Positive  \n","8         Positive  \n","9         Positive  \n","10        Positive  \n","11        Positive  \n","12        Positive  \n","13        Negative  \n","14        Positive  \n","15        Positive  \n","16        Negative  \n","17        Positive  \n","18        Positive  \n","19        Positive  \n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Assuming df_results is the DataFrame from the previous cell\n","# with 'comment' and 'predicted_label' columns\n","\n","# Define the output CSV file path\n","output_csv_path = \"/content/drive/MyDrive/Data Science/Arabic Sentiment/Reddit_Sentiment_Predictions(arabic).csv\"\n","\n","# Save the DataFrame to a CSV file\n","df_results.to_csv(output_csv_path, index=False)\n","\n","print(f\"Sentiment predictions saved to {output_csv_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U7Loi-7IscKT","executionInfo":{"status":"ok","timestamp":1747884370144,"user_tz":-180,"elapsed":22,"user":{"displayName":"Habiba Habiba Mohamed Darwish Sayed","userId":"06249506762316426256"}},"outputId":"1fff636b-8264-4b22-da0d-9e646558f7e4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sentiment predictions saved to /content/drive/MyDrive/Data Science/Arabic Sentiment/Reddit_Sentiment_Predictions(arabic).csv\n"]}]}]}