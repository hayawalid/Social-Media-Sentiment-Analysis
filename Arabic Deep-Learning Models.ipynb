{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b9b7ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Dense...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Social Media Sentiment Analysis\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "375/375 - 9s - 23ms/step - accuracy: 0.7569 - loss: 0.4764 - val_accuracy: 0.8365 - val_loss: 0.3575\n",
      "Epoch 2/5\n",
      "375/375 - 6s - 15ms/step - accuracy: 0.8749 - loss: 0.3050 - val_accuracy: 0.8369 - val_loss: 0.3539\n",
      "Epoch 3/5\n",
      "375/375 - 6s - 15ms/step - accuracy: 0.9087 - loss: 0.2380 - val_accuracy: 0.8384 - val_loss: 0.3920\n",
      "Epoch 4/5\n",
      "375/375 - 6s - 15ms/step - accuracy: 0.9329 - loss: 0.1832 - val_accuracy: 0.8316 - val_loss: 0.4212\n",
      "\u001b[1m417/417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Dense Accuracy: 0.8479\n",
      "Dense F1 Score: 0.8477\n",
      "\n",
      "Training CNN...\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Social Media Sentiment Analysis\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 - 34s - 90ms/step - accuracy: 0.7888 - loss: 0.4208 - val_accuracy: 0.8526 - val_loss: 0.3298\n",
      "Epoch 2/5\n",
      "375/375 - 31s - 83ms/step - accuracy: 0.8850 - loss: 0.2777 - val_accuracy: 0.8583 - val_loss: 0.3234\n",
      "Epoch 3/5\n",
      "375/375 - 31s - 83ms/step - accuracy: 0.9212 - loss: 0.2051 - val_accuracy: 0.8481 - val_loss: 0.3576\n",
      "Epoch 4/5\n",
      "375/375 - 32s - 86ms/step - accuracy: 0.9463 - loss: 0.1472 - val_accuracy: 0.8423 - val_loss: 0.4151\n",
      "\u001b[1m417/417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step\n",
      "CNN Accuracy: 0.8575\n",
      "CNN F1 Score: 0.8574\n",
      "\n",
      "Training BiLSTM...\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Social Media Sentiment Analysis\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 - 105s - 280ms/step - accuracy: 0.7567 - loss: 0.4652 - val_accuracy: 0.8461 - val_loss: 0.3420\n",
      "Epoch 2/5\n",
      "375/375 - 136s - 364ms/step - accuracy: 0.8789 - loss: 0.2889 - val_accuracy: 0.8487 - val_loss: 0.3349\n",
      "Epoch 3/5\n",
      "375/375 - 139s - 370ms/step - accuracy: 0.9025 - loss: 0.2378 - val_accuracy: 0.8528 - val_loss: 0.3473\n",
      "Epoch 4/5\n",
      "375/375 - 96s - 256ms/step - accuracy: 0.9262 - loss: 0.1890 - val_accuracy: 0.8463 - val_loss: 0.3922\n",
      "\u001b[1m417/417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 32ms/step\n",
      "BiLSTM Accuracy: 0.8522\n",
      "BiLSTM F1 Score: 0.8515\n",
      "\n",
      "=== Arabic Model Results Summary ===\n",
      "CNN: Accuracy = 0.8575, F1 Score = 0.8574\n",
      "BiLSTM: Accuracy = 0.8522, F1 Score = 0.8515\n",
      "Dense: Accuracy = 0.8479, F1 Score = 0.8477\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, Dropout, GlobalMaxPool1D, Conv1D, LSTM, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ====== Load and Preprocess Arabic Dataset ======\n",
    "df = pd.read_csv(\"Arabic_cleaned.csv\")\n",
    "\n",
    "# Drop any rows with missing or empty cleanedtext\n",
    "df['cleanedtext'] = df['cleanedtext'].astype(str).str.strip()\n",
    "df = df[df['cleanedtext'] != '']\n",
    "df = df.dropna(subset=['cleanedtext'])\n",
    "\n",
    "# ====== Tokenization and Padding ======\n",
    "max_words = 10000\n",
    "max_len = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df['cleanedtext'])\n",
    "X = tokenizer.texts_to_sequences(df['cleanedtext'])\n",
    "X = pad_sequences(X, maxlen=max_len)\n",
    "\n",
    "# ====== Labels ======\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['label'])  # Encode -1 and 1 to 0 and 1\n",
    "y_cat = to_categorical(y)\n",
    "\n",
    "# ====== Train-test Split ======\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_cat, test_size=0.2, random_state=42, stratify=y_cat)\n",
    "\n",
    "# ====== Early Stopping ======\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "# ====== Models ======\n",
    "def create_dense_model():\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=max_words, output_dim=64, input_length=max_len),\n",
    "        GlobalMaxPool1D(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(2, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_cnn_model():\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=max_words, output_dim=64, input_length=max_len),\n",
    "        Conv1D(128, 5, activation='relu'),\n",
    "        GlobalMaxPool1D(),\n",
    "        Dropout(0.5),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(2, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_bilstm_model():\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=max_words, output_dim=64, input_length=max_len),\n",
    "        Bidirectional(LSTM(64)),\n",
    "        Dropout(0.5),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(2, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ====== Training Function ======\n",
    "def train_and_evaluate(model_fn, name):\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    model = model_fn()\n",
    "    model.fit(X_train, y_train, epochs=5, batch_size=128, validation_split=0.1, callbacks=[early_stop], verbose=2)\n",
    "\n",
    "    y_pred_probs = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    print(f\"{name} Accuracy: {acc:.4f}\")\n",
    "    print(f\"{name} F1 Score: {f1:.4f}\")\n",
    "    return model, acc, f1\n",
    "\n",
    "# ====== Train All Models and Save the Best ======\n",
    "models = [\n",
    "    (\"Dense\", create_dense_model),\n",
    "    (\"CNN\", create_cnn_model),\n",
    "    (\"BiLSTM\", create_bilstm_model)\n",
    "]\n",
    "\n",
    "results = []\n",
    "best_model = None\n",
    "best_score = 0\n",
    "\n",
    "for name, fn in models:\n",
    "    model, acc, f1 = train_and_evaluate(fn, name)\n",
    "    results.append((name, acc, f1))\n",
    "    if acc > best_score:\n",
    "        best_score = acc\n",
    "        best_model = model\n",
    "        best_model.save(\"best_arabic_model.keras\")\n",
    "\n",
    "# ====== Results Summary ======\n",
    "print(\"\\n=== Arabic Model Results Summary ===\")\n",
    "for name, acc, f1 in sorted(results, key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{name}: Accuracy = {acc:.4f}, F1 Score = {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b489b0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_arabic_text(sample_text, model, tokenizer, label_encoder, threshold=0.1):\n",
    "    sequence = tokenizer.texts_to_sequences([sample_text])\n",
    "    padded = pad_sequences(sequence, maxlen=max_len)\n",
    "    pred_prob = model.predict(padded, verbose=0)[0]\n",
    "    pred_class = np.argmax(pred_prob)\n",
    "    confidence_gap = sorted(pred_prob, reverse=True)[0] - sorted(pred_prob, reverse=True)[1]\n",
    "\n",
    "    if confidence_gap < threshold:\n",
    "        label = \"neutral\"\n",
    "    else:\n",
    "        original_label = label_encoder.inverse_transform([pred_class])[0]\n",
    "        label = \"positive\" if original_label == 1 else \"negative\"\n",
    "\n",
    "    print(f\"Input: {sample_text}\")\n",
    "    print(f\"Probabilities: {pred_prob}\")\n",
    "    print(f\"Predicted Sentiment: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a4d974c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: الخدمة كانت مقبولة ولكن ليست ممتازة.\n",
      "Probabilities: [0.64340633 0.35659367]\n",
      "Predicted Sentiment: negative\n",
      "--------------------------------------------------\n",
      "Input: هذا أفضل منتج استخدمته في حياتي!\n",
      "Probabilities: [0.4053409 0.5946591]\n",
      "Predicted Sentiment: positive\n",
      "--------------------------------------------------\n",
      "Input: لا أنصح به على الإطلاق، تجربة سيئة.\n",
      "Probabilities: [0.8505613  0.14943871]\n",
      "Predicted Sentiment: negative\n",
      "--------------------------------------------------\n",
      "Input: أداء رائع وسرعة في التوصيل، شكراً لكم!\n",
      "Probabilities: [0.05260722 0.9473928 ]\n",
      "Predicted Sentiment: positive\n",
      "--------------------------------------------------\n",
      "Input: تعامل راقٍ واحترافي، أنصح بالتجربة.\n",
      "Probabilities: [0.5902418 0.4097582]\n",
      "Predicted Sentiment: negative\n",
      "--------------------------------------------------\n",
      "Input: تجربة سيئة جدًا، المنتج لا يعمل كما يجب.\n",
      "Probabilities: [0.66115725 0.33884272]\n",
      "Predicted Sentiment: negative\n",
      "--------------------------------------------------\n",
      "Input: خدمة سيئة وسعر مرتفع بلا داعٍ.\n",
      "Probabilities: [0.5158217  0.48417825]\n",
      "Predicted Sentiment: neutral\n",
      "--------------------------------------------------\n",
      "Input: المنتج متوسط، ليس الأفضل ولا الأسوأ.\n",
      "Probabilities: [0.73885626 0.2611438 ]\n",
      "Predicted Sentiment: negative\n",
      "--------------------------------------------------\n",
      "Input: المنتج فاق توقعاتي بكل صراحة.\n",
      "Probabilities: [0.81643033 0.18356967]\n",
      "Predicted Sentiment: negative\n",
      "--------------------------------------------------\n",
      "Input: وصل المنتج تالفًا وبحجم مختلف عن المطلوب.\n",
      "Probabilities: [0.46884724 0.53115284]\n",
      "Predicted Sentiment: neutral\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load model and test\n",
    "arabic_model = load_model(\"best_arabic_model.keras\")\n",
    "\n",
    "test_texts = [\n",
    "    \"الخدمة كانت مقبولة ولكن ليست ممتازة.\",\n",
    "    \"هذا أفضل منتج استخدمته في حياتي!\",\n",
    "    \"لا أنصح به على الإطلاق، تجربة سيئة.\",\n",
    "    \"أداء رائع وسرعة في التوصيل، شكراً لكم!\",\n",
    "    \"تعامل راقٍ واحترافي، أنصح بالتجربة.\",\n",
    "    \"تجربة سيئة جدًا، المنتج لا يعمل كما يجب.\",\n",
    "    \"خدمة سيئة وسعر مرتفع بلا داعٍ.\",\n",
    "    \"المنتج متوسط، ليس الأفضل ولا الأسوأ.\",\n",
    "    \"المنتج فاق توقعاتي بكل صراحة.\",\n",
    "    \"وصل المنتج تالفًا وبحجم مختلف عن المطلوب.\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    predict_arabic_text(text, arabic_model, tokenizer, label_encoder)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46051e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Social Media Sentiment Analysis\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Social Media Sentiment Analysis\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 - 232s - 309ms/step - accuracy: 0.8179 - loss: 0.3841 - val_accuracy: 0.8556 - val_loss: 0.3224\n",
      "Epoch 2/5\n",
      "750/750 - 227s - 303ms/step - accuracy: 0.9079 - loss: 0.2350 - val_accuracy: 0.8507 - val_loss: 0.3596\n",
      "Epoch 3/5\n",
      "750/750 - 234s - 312ms/step - accuracy: 0.9525 - loss: 0.1352 - val_accuracy: 0.8419 - val_loss: 0.4458\n",
      "\u001b[1m417/417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 33ms/step\n",
      "\n",
      "BiLSTM Accuracy: 0.8592\n",
      "BiLSTM F1 Score: 0.8592\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from transformers import AutoTokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "# ====== Load Data ======\n",
    "df = pd.read_csv(\"Arabic_cleaned.csv\")\n",
    "df = df[df['cleanedtext'].str.strip() != '']\n",
    "df = df.dropna(subset=['cleanedtext'])\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "df['label'] = le.fit_transform(df['label'])  # -1 → 0, 1 → 1\n",
    "y = to_categorical(df['label'])\n",
    "\n",
    "# ====== AraBERT Tokenizer ======\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv02\")\n",
    "\n",
    "# Tokenize text\n",
    "def tokenize(texts, max_len=100):\n",
    "    encodings = tokenizer(\n",
    "        texts.tolist(),\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=max_len,\n",
    "        return_tensors='np'\n",
    "    )\n",
    "    return encodings['input_ids']\n",
    "\n",
    "X = tokenize(df['cleanedtext'], max_len=100)\n",
    "\n",
    "# ====== Train-Test Split ======\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# ====== Early Stopping ======\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "# ====== Custom BiLSTM Model ======\n",
    "vocab_size = tokenizer.vocab_size  # Get AraBERT tokenizer vocab size\n",
    "embedding_dim = 128\n",
    "max_len = X.shape[1]\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# ====== Train ======\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.1, callbacks=[early_stop], verbose=2)\n",
    "\n",
    "# ====== Evaluate ======\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "print(f\"\\nBiLSTM Accuracy: {acc:.4f}\")\n",
    "print(f\"BiLSTM F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41308b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict2_arabic_text(text, model, tokenizer, le, max_len=100):\n",
    "    # Tokenize the input text using AraBERT tokenizer\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=max_len,\n",
    "        return_tensors='np'\n",
    "    )\n",
    "\n",
    "    # Get input_ids\n",
    "    input_ids = encoding['input_ids']\n",
    "\n",
    "    # Predict probabilities\n",
    "    prediction = model.predict(input_ids)\n",
    "\n",
    "    # Get predicted class index\n",
    "    pred_class_idx = np.argmax(prediction, axis=1)[0]\n",
    "\n",
    "    # Decode original label (-1 or 1)\n",
    "    label = le.inverse_transform([pred_class_idx])[0]\n",
    "\n",
    "    # Confidence score\n",
    "    confidence = float(np.max(prediction))\n",
    "\n",
    "    sentiment = \"Positive\" if label == 1 else \"Negative\"\n",
    "\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Predicted Sentiment: {sentiment}\")\n",
    "    print(f\"Confidence: {confidence:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "595bd89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "Text: الخدمة كانت مقبولة ولكن ليست ممتازة.\n",
      "Predicted Sentiment: Negative\n",
      "Confidence: 0.80\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Text: هذا أفضل منتج استخدمته في حياتي!\n",
      "Predicted Sentiment: Positive\n",
      "Confidence: 0.55\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Text: لا أنصح به على الإطلاق، تجربة سيئة.\n",
      "Predicted Sentiment: Negative\n",
      "Confidence: 0.85\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
      "Text: أداء رائع وسرعة في التوصيل، شكراً لكم!\n",
      "Predicted Sentiment: Positive\n",
      "Confidence: 0.97\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
      "Text: تعامل راقٍ واحترافي، أنصح بالتجربة.\n",
      "Predicted Sentiment: Positive\n",
      "Confidence: 0.70\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "Text: تجربة سيئة جدًا، المنتج لا يعمل كما يجب.\n",
      "Predicted Sentiment: Negative\n",
      "Confidence: 0.54\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "Text: خدمة سيئة وسعر مرتفع بلا داعٍ.\n",
      "Predicted Sentiment: Positive\n",
      "Confidence: 0.56\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Text: المنتج متوسط، ليس الأفضل ولا الأسوأ.\n",
      "Predicted Sentiment: Negative\n",
      "Confidence: 0.89\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "Text: المنتج فاق توقعاتي بكل صراحة.\n",
      "Predicted Sentiment: Negative\n",
      "Confidence: 0.79\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Text: وصل المنتج تالفًا وبحجم مختلف عن المطلوب.\n",
      "Predicted Sentiment: Negative\n",
      "Confidence: 0.50\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\n",
    "    \"الخدمة كانت مقبولة ولكن ليست ممتازة.\",\n",
    "    \"هذا أفضل منتج استخدمته في حياتي!\",\n",
    "    \"لا أنصح به على الإطلاق، تجربة سيئة.\",\n",
    "    \"أداء رائع وسرعة في التوصيل، شكراً لكم!\",\n",
    "    \"تعامل راقٍ واحترافي، أنصح بالتجربة.\",\n",
    "    \"تجربة سيئة جدًا، المنتج لا يعمل كما يجب.\",\n",
    "    \"خدمة سيئة وسعر مرتفع بلا داعٍ.\",\n",
    "    \"المنتج متوسط، ليس الأفضل ولا الأسوأ.\",\n",
    "    \"المنتج فاق توقعاتي بكل صراحة.\",\n",
    "    \"وصل المنتج تالفًا وبحجم مختلف عن المطلوب.\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    predict2_arabic_text(text, model, tokenizer, le)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1064b53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "AraBERT_Tokenization_Model = model \n",
    "AraBERT_Tokenization_Model.save(\"araBERT_tokenization_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dc1e36dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Social Media Sentiment Analysis\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m663s\u001b[0m 432ms/step - accuracy: 0.7811 - loss: 0.4331 - val_accuracy: 0.8552 - val_loss: 0.3219\n",
      "Epoch 2/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m640s\u001b[0m 427ms/step - accuracy: 0.9259 - loss: 0.1956 - val_accuracy: 0.8519 - val_loss: 0.3628\n",
      "Epoch 3/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m645s\u001b[0m 430ms/step - accuracy: 0.9648 - loss: 0.1030 - val_accuracy: 0.8470 - val_loss: 0.4444\n",
      "Epoch 4/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m652s\u001b[0m 435ms/step - accuracy: 0.9790 - loss: 0.0631 - val_accuracy: 0.8449 - val_loss: 0.5601\n",
      "Epoch 5/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m649s\u001b[0m 432ms/step - accuracy: 0.9891 - loss: 0.0346 - val_accuracy: 0.8397 - val_loss: 0.7729\n",
      "\u001b[1m417/417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 110ms/step - accuracy: 0.8367 - loss: 0.7640\n",
      "Test Loss: 0.7531 - Accuracy: 83.95%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "\n",
    "# === Load Dataset ===\n",
    "df = pd.read_csv(\"Arabic_cleaned.csv\")\n",
    "df = df.dropna(subset=[\"cleanedtext\"])\n",
    "df[\"cleanedtext\"] = df[\"cleanedtext\"].astype(str)\n",
    "\n",
    "# === Encode Labels (-1 to 0, 1 to 1) ===\n",
    "le = LabelEncoder()\n",
    "df[\"label\"] = le.fit_transform(df[\"label\"])\n",
    "\n",
    "# === Train-Test Split ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"cleanedtext\"], df[\"label\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# === Tokenization ===\n",
    "tokenizer = Tokenizer(num_words=50000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "max_len = 128\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "# === Build the Model ===\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=50000, output_dim=128, input_length=max_len),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    Dropout(0.5),\n",
    "    Bidirectional(LSTM(32)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# === Compile ===\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# === Train ===\n",
    "history = model.fit(X_train_pad, y_train, epochs=5, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# === Evaluate ===\n",
    "loss, accuracy = model.evaluate(X_test_pad, y_test)\n",
    "print(f\"Test Loss: {loss:.4f} - Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "480840ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'inspect' has no attribute 'ArgSpec'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m classification_report, accuracy_score\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msequence\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Social Media Sentiment Analysis\\venv\\Lib\\site-packages\\keras\\api\\_v2\\keras\\__init__.py:12\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[33;03m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m \u001b[33;03mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33;03m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[32m      7\u001b[39m \n\u001b[32m      8\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_sys\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_v2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_v2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m activations\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Social Media Sentiment Analysis\\venv\\Lib\\site-packages\\keras\\__init__.py:21\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[33;03m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[32m     16\u001b[39m \n\u001b[32m     17\u001b[39m \u001b[33;03mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[33;03m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minput_layer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Input\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msequential\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Social Media Sentiment Analysis\\venv\\Lib\\site-packages\\keras\\models\\__init__.py:18\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[33;03m\"\"\"Keras models API.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Functional\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msequential\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtraining\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Model\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Social Media Sentiment Analysis\\venv\\Lib\\site-packages\\keras\\engine\\functional.py:26\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m layout_map \u001b[38;5;28;01mas\u001b[39;00m layout_map_lib\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m base_layer\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Social Media Sentiment Analysis\\venv\\Lib\\site-packages\\keras\\backend.py:34\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backend_config\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistribute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distribute_coordinator_utils \u001b[38;5;28;01mas\u001b[39;00m dc\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras_tensor\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m control_flow_util\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m object_identity\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Social Media Sentiment Analysis\\venv\\Lib\\site-packages\\keras\\engine\\keras_tensor.py:19\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[33;03m\"\"\"Keras Input Tensor used to track functional API Topology.\"\"\"\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m object_identity\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# isort: off\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m structure\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Social Media Sentiment Analysis\\venv\\Lib\\site-packages\\keras\\utils\\__init__.py:17\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[33;03m\"\"\"Public Keras utilities.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msaving\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlegacy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mserialization\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deserialize_keras_object\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msaving\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlegacy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mserialization\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m serialize_keras_object\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Serialization related\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Social Media Sentiment Analysis\\venv\\Lib\\site-packages\\keras\\saving\\legacy\\serialization.py:24\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf_contextlib\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf_inspect\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# isort: off\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtf_export\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras_export\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Social Media Sentiment Analysis\\venv\\Lib\\site-packages\\keras\\utils\\tf_inspect.py:22\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01minspect\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_inspect\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m ArgSpec = \u001b[43m_inspect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mArgSpec\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(_inspect, \u001b[33m\"\u001b[39m\u001b[33mFullArgSpec\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     26\u001b[39m     FullArgSpec = _inspect.FullArgSpec\n",
      "\u001b[31mAttributeError\u001b[39m: module 'inspect' has no attribute 'ArgSpec'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, GRU, Conv1D, GlobalMaxPooling1D, Dropout, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# ============================\n",
    "# Step 1: Load and Clean Data\n",
    "# ============================\n",
    "\n",
    "# Replace with your own data\n",
    "# Dataset should have columns: ['text', 'label'] with label as 0/1\n",
    "df = pd.read_csv(\"arabic_sentiment_data.csv\")\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].astype(str).apply(clean_text)\n",
    "\n",
    "# ============================\n",
    "# Step 2: Tokenization\n",
    "# ============================\n",
    "\n",
    "max_len = 100\n",
    "vocab_size = 50000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(df['text'])\n",
    "\n",
    "X = tokenizer.texts_to_sequences(df['text'])\n",
    "X = pad_sequences(X, maxlen=max_len)\n",
    "\n",
    "y = df['label'].values\n",
    "\n",
    "X_train_pad, X_test_pad, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ============================\n",
    "# Step 3: Load AraVec Embeddings\n",
    "# ============================\n",
    "\n",
    "print(\"Loading AraVec Word2Vec embeddings...\")\n",
    "aravec_path = \"aravec_cbow.bin\"  # Path to AraVec CBOW binary\n",
    "aravec_model = KeyedVectors.load_word2vec_format(aravec_path, binary=True)\n",
    "\n",
    "embedding_dim = 300\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= vocab_size:\n",
    "        continue\n",
    "    if word in aravec_model:\n",
    "        embedding_matrix[i] = aravec_model[word]\n",
    "\n",
    "# ============================\n",
    "# Step 4: Build GRU + CNN Model\n",
    "# ============================\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size,\n",
    "              output_dim=embedding_dim,\n",
    "              input_length=max_len,\n",
    "              weights=[embedding_matrix],\n",
    "              trainable=False),\n",
    "    Bidirectional(GRU(64, return_sequences=True)),\n",
    "    Conv1D(128, kernel_size=5, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dropout(0.4),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# ============================\n",
    "# Step 5: Train Model\n",
    "# ============================\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_pad, y_train,\n",
    "    epochs=15,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# Step 6: Plot Performance\n",
    "# ============================\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Acc')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================\n",
    "# Step 7: Evaluate on Test Set\n",
    "# ============================\n",
    "\n",
    "y_pred = (model.predict(X_test_pad) > 0.5).astype(\"int32\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
