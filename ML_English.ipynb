{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Bqx7TUQLAbNw",
        "1RNB3zZoAbg8",
        "C1p0x2jUNhsr",
        "e7Z4WlJWWv4B"
      ],
      "authorship_tag": "ABX9TyPPmFElb79zYQdmP2jnmlq2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janaghoniem/Social-Media-Sentiment-Analysis/blob/main/ML_English.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "R9D8ufKNKGZD"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from scipy.sparse import vstack\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "import warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "import ast\n",
        "import re\n",
        "import pickle\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data Science/English preprocessing + pipeline & dataset/English_cleaned.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "collapsed": true,
        "id": "dWXsiNY8KT_L",
        "outputId": "cbae4886-5031-4ac3-fcd8-4501b527efd6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                    normalized_tweet  \\\n",
              "0  a that's a bummer. you shoulda got david carr ...   \n",
              "1  is upset that he can't update his facebook by ...   \n",
              "2  i dived many times for the ball. managed to sa...   \n",
              "3     my whole body feels itchy and like its on fire   \n",
              "4  no, it's not behaving at all. i'm mad. why am ...   \n",
              "\n",
              "                                              tokens  \\\n",
              "0  [\"'s\", 'bummer', '.', 'shoulda', 'got', 'david...   \n",
              "1  ['upset', 'ca', \"n't\", 'updat', 'facebook', 't...   \n",
              "2  ['dive', 'mani', 'time', 'ball', '.', 'manag',...   \n",
              "3  ['whole', 'bodi', 'feel', 'itchi', 'like', 'fi...   \n",
              "4  ['no', ',', \"'s\", 'not', 'behav', '.', \"'m\", '...   \n",
              "\n",
              "                                      trigrams_token  target  \n",
              "0  [('a', 'that', \"'s\"), ('that', \"'s\", 'a'), (\"'...      -1  \n",
              "1  [('is', 'upset', 'that'), ('upset', 'that', 'h...      -1  \n",
              "2  [('i', 'dive', 'mani'), ('dive', 'mani', 'time...      -1  \n",
              "3  [('my', 'whole', 'bodi'), ('whole', 'bodi', 'f...      -1  \n",
              "4  [('no', ',', 'it'), (',', 'it', \"'s\"), ('it', ...      -1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-59e72d29-b40b-4615-a6c9-bc9bf3df00c7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>normalized_tweet</th>\n",
              "      <th>tokens</th>\n",
              "      <th>trigrams_token</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a that's a bummer. you shoulda got david carr ...</td>\n",
              "      <td>[\"'s\", 'bummer', '.', 'shoulda', 'got', 'david...</td>\n",
              "      <td>[('a', 'that', \"'s\"), ('that', \"'s\", 'a'), (\"'...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>is upset that he can't update his facebook by ...</td>\n",
              "      <td>['upset', 'ca', \"n't\", 'updat', 'facebook', 't...</td>\n",
              "      <td>[('is', 'upset', 'that'), ('upset', 'that', 'h...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i dived many times for the ball. managed to sa...</td>\n",
              "      <td>['dive', 'mani', 'time', 'ball', '.', 'manag',...</td>\n",
              "      <td>[('i', 'dive', 'mani'), ('dive', 'mani', 'time...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "      <td>['whole', 'bodi', 'feel', 'itchi', 'like', 'fi...</td>\n",
              "      <td>[('my', 'whole', 'bodi'), ('whole', 'bodi', 'f...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>no, it's not behaving at all. i'm mad. why am ...</td>\n",
              "      <td>['no', ',', \"'s\", 'not', 'behav', '.', \"'m\", '...</td>\n",
              "      <td>[('no', ',', 'it'), (',', 'it', \"'s\"), ('it', ...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-59e72d29-b40b-4615-a6c9-bc9bf3df00c7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-59e72d29-b40b-4615-a6c9-bc9bf3df00c7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-59e72d29-b40b-4615-a6c9-bc9bf3df00c7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-b0c990d0-0487-4e16-aff0-fa48b089dfdb\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b0c990d0-0487-4e16-aff0-fa48b089dfdb')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-b0c990d0-0487-4e16-aff0-fa48b089dfdb button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) CatBoost ✅"
      ],
      "metadata": {
        "id": "ZmWPa8cFEkID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "urwser5zDU-h",
        "outputId": "f5da101e-071b-43ba-ffc1-0c4bcbcecd15"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.15.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.1.2)\n",
            "Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Keep the trigram list as is\n",
        "X_tokens = df['trigrams_token'].tolist()  # List of lists of trigram strings\n",
        "y = df['target']\n",
        "\n",
        "# Vectorize using CountVectorizer with a custom analyzer that expects pre-tokenized input\n",
        "vectorizer = CountVectorizer(analyzer=lambda x: x)\n",
        "X = vectorizer.fit_transform(X_tokens)\n",
        "\n",
        "# CatBoost requires dense float32 arrays\n",
        "X = X.toarray().astype(np.float32)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split into train (60%) and temp (40%)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y_encoded, test_size=0.4, stratify=y_encoded, random_state=42\n",
        ")\n",
        "\n",
        "# Split temp into validation (20%) and test (20%)\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
        ")\n",
        "\n",
        "# -------------------- Baseline CatBoost --------------------\n",
        "cat_model = CatBoostClassifier(verbose=0, random_state=42)\n",
        "cat_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions before tuning\n",
        "y_train_pred = cat_model.predict(X_train)\n",
        "y_val_pred = cat_model.predict(X_val)\n",
        "y_test_pred = cat_model.predict(X_test)\n",
        "\n",
        "print(f\"Baseline CatBoost Train Accuracy: {accuracy_score(y_train, y_train_pred):.4f}\")\n",
        "print(f\"Baseline CatBoost Validation Accuracy: {accuracy_score(y_val, y_val_pred):.4f}\")\n",
        "print(f\"Baseline CatBoost Test Accuracy: {accuracy_score(y_test, y_test_pred):.4f}\")\n",
        "\n",
        "print(\"\\n--- Baseline Classification Report (Test Set) ---\")\n",
        "print(classification_report(y_test, y_test_pred, target_names=label_encoder.classes_.astype(str)))\n",
        "\n",
        "# -------------------- Hyperparameter Tuning --------------------\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'depth': [4, 6, 8],\n",
        "    'iterations': [100, 200]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    CatBoostClassifier(verbose=0, random_state=42),\n",
        "    param_grid,\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit grid search only on training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "best_cat = grid_search.best_estimator_\n",
        "\n",
        "print(\"\\nBest hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Predictions after tuning\n",
        "y_train_pred_tuned = best_cat.predict(X_train)\n",
        "y_val_pred_tuned = best_cat.predict(X_val)\n",
        "y_test_pred_tuned = best_cat.predict(X_test)\n",
        "\n",
        "print(f\"\\nTuned CatBoost Train Accuracy: {accuracy_score(y_train, y_train_pred_tuned):.4f}\")\n",
        "print(f\"Tuned CatBoost Validation Accuracy: {accuracy_score(y_val, y_val_pred_tuned):.4f}\")\n",
        "print(f\"Tuned CatBoost Test Accuracy: {accuracy_score(y_test, y_test_pred_tuned):.4f}\")\n",
        "\n",
        "print(\"\\n--- Tuned Classification Report (Test Set) ---\")\n",
        "print(classification_report(y_test, y_test_pred_tuned, target_names=label_encoder.classes_.astype(str)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w389VFvGDZb_",
        "outputId": "5aa0592c-4987-4dd8-b69f-f74f6a303801"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline CatBoost Train Accuracy: 0.6515\n",
            "Baseline CatBoost Validation Accuracy: 0.6309\n",
            "Baseline CatBoost Test Accuracy: 0.6295\n",
            "\n",
            "--- Baseline Classification Report (Test Set) ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.63      0.67      0.65    141567\n",
            "           1       0.63      0.59      0.61    138451\n",
            "\n",
            "    accuracy                           0.63    280018\n",
            "   macro avg       0.63      0.63      0.63    280018\n",
            "weighted avg       0.63      0.63      0.63    280018\n",
            "\n",
            "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
            "\n",
            "Best hyperparameters: {'depth': 8, 'iterations': 200, 'learning_rate': 0.1}\n",
            "\n",
            "Tuned CatBoost Train Accuracy: 0.6256\n",
            "Tuned CatBoost Validation Accuracy: 0.6179\n",
            "Tuned CatBoost Test Accuracy: 0.6164\n",
            "\n",
            "--- Tuned Classification Report (Test Set) ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.61      0.67      0.64    141567\n",
            "           1       0.62      0.56      0.59    138451\n",
            "\n",
            "    accuracy                           0.62    280018\n",
            "   macro avg       0.62      0.62      0.62    280018\n",
            "weighted avg       0.62      0.62      0.62    280018\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) Ridge Classifier"
      ],
      "metadata": {
        "id": "UzO8JuLwN84K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# -------------------- Preprocessing --------------------\n",
        "# Tokenized input\n",
        "X_tokens = df['trigrams_token'].tolist()\n",
        "y = df['target']\n",
        "\n",
        "# Vectorize trigrams\n",
        "vectorizer = CountVectorizer(analyzer=lambda x: x)\n",
        "X = vectorizer.fit_transform(X_tokens)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split into train (60%) and temp (40%)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y_encoded, test_size=0.4, stratify=y_encoded, random_state=42\n",
        ")\n",
        "\n",
        "# Split temp into validation (20%) and test (20%)\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
        ")\n",
        "\n",
        "# -------------------- Baseline Ridge Classifier --------------------\n",
        "ridge_model = RidgeClassifier()\n",
        "ridge_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions before tuning\n",
        "y_train_pred = ridge_model.predict(X_train)\n",
        "y_val_pred = ridge_model.predict(X_val)\n",
        "y_test_pred = ridge_model.predict(X_test)\n",
        "\n",
        "print(f\"Baseline Ridge Train Accuracy: {accuracy_score(y_train, y_train_pred):.4f}\")\n",
        "print(f\"Baseline Ridge Validation Accuracy: {accuracy_score(y_val, y_val_pred):.4f}\")\n",
        "print(f\"Baseline Ridge Test Accuracy: {accuracy_score(y_test, y_test_pred):.4f}\")\n",
        "\n",
        "print(\"\\n--- Baseline Classification Report (Test Set) ---\")\n",
        "print(classification_report(y_test, y_test_pred, target_names=label_encoder.classes_.astype(str)))\n",
        "\n",
        "# -------------------- Hyperparameter Tuning --------------------\n",
        "param_grid = {\n",
        "    'alpha': [0.01, 0.1, 1.0, 10.0, 100.0]  # Regularization strength\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    RidgeClassifier(),\n",
        "    param_grid,\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "best_ridge = grid_search.best_estimator_\n",
        "\n",
        "print(\"\\nBest hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Predictions after tuning\n",
        "y_train_pred_tuned = best_ridge.predict(X_train)\n",
        "y_val_pred_tuned = best_ridge.predict(X_val)\n",
        "y_test_pred_tuned = best_ridge.predict(X_test)\n",
        "\n",
        "print(f\"\\nTuned Ridge Train Accuracy: {accuracy_score(y_train, y_train_pred_tuned):.4f}\")\n",
        "print(f\"Tuned Ridge Validation Accuracy: {accuracy_score(y_val, y_val_pred_tuned):.4f}\")\n",
        "print(f\"Tuned Ridge Test Accuracy: {accuracy_score(y_test, y_test_pred_tuned):.4f}\")\n",
        "\n",
        "print(\"\\n--- Tuned Classification Report (Test Set) ---\")\n",
        "print(classification_report(y_test, y_test_pred_tuned, target_names=label_encoder.classes_.astype(str)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJGA9oCNN9HJ",
        "outputId": "a9376144-2462-403d-b7dd-324d5d7c7cf0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Ridge Train Accuracy: 0.5983\n",
            "Baseline Ridge Validation Accuracy: 0.5983\n",
            "Baseline Ridge Test Accuracy: 0.5974\n",
            "\n",
            "--- Baseline Classification Report (Test Set) ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.60      0.60      0.60    141567\n",
            "           1       0.59      0.60      0.60    138451\n",
            "\n",
            "    accuracy                           0.60    280018\n",
            "   macro avg       0.60      0.60      0.60    280018\n",
            "weighted avg       0.60      0.60      0.60    280018\n",
            "\n",
            "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
            "\n",
            "Best hyperparameters: {'alpha': 0.01}\n",
            "\n",
            "Tuned Ridge Train Accuracy: 0.5983\n",
            "Tuned Ridge Validation Accuracy: 0.5983\n",
            "Tuned Ridge Test Accuracy: 0.5974\n",
            "\n",
            "--- Tuned Classification Report (Test Set) ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.60      0.60      0.60    141567\n",
            "           1       0.59      0.60      0.60    138451\n",
            "\n",
            "    accuracy                           0.60    280018\n",
            "   macro avg       0.60      0.60      0.60    280018\n",
            "weighted avg       0.60      0.60      0.60    280018\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) Logistic regression ✅"
      ],
      "metadata": {
        "id": "Bqx7TUQLAbNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Keep the trigram list as is\n",
        "X_tokens = df['trigrams_token'].tolist()  # List of lists of trigram strings\n",
        "y = df['target']\n",
        "\n",
        "# Vectorize using CountVectorizer with a custom analyzer that expects pre-tokenized input\n",
        "vectorizer = CountVectorizer(analyzer=lambda x: x)\n",
        "X = vectorizer.fit_transform(X_tokens)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split into train (60%) and temp (40%)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y_encoded, test_size=0.4, stratify=y_encoded, random_state=42\n",
        ")\n",
        "\n",
        "# Split temp into validation (20%) and test (20%)\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
        ")\n",
        "\n",
        "# Train baseline Logistic Regression model\n",
        "logreg_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "logreg_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions before tuning\n",
        "y_train_pred = logreg_model.predict(X_train)\n",
        "y_val_pred = logreg_model.predict(X_val)\n",
        "y_test_pred = logreg_model.predict(X_test)\n",
        "\n",
        "print(f\"Baseline Logistic Regression Train Accuracy: {accuracy_score(y_train, y_train_pred):.4f}\")\n",
        "print(f\"Baseline Logistic Regression Validation Accuracy: {accuracy_score(y_val, y_val_pred):.4f}\")\n",
        "print(f\"Baseline Logistic Regression Test Accuracy: {accuracy_score(y_test, y_test_pred):.4f}\")\n",
        "\n",
        "print(\"\\n--- Baseline Classification Report (Test Set) ---\")\n",
        "print(classification_report(y_test, y_test_pred, target_names=label_encoder.classes_.astype(str)))\n",
        "\n",
        "# -------------------- Hyperparameter Tuning --------------------\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],      # Inverse of regularization strength\n",
        "    'penalty': ['l2'],                # L2 penalty (default and preferred)\n",
        "    'solver': ['lbfgs'],              # Good solver for multiclass problems\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    LogisticRegression(max_iter=1000, random_state=42),\n",
        "    param_grid,\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit grid search only on training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "best_logreg = grid_search.best_estimator_\n",
        "\n",
        "print(\"\\nBest hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Predictions after tuning\n",
        "y_train_pred_tuned = best_logreg.predict(X_train)\n",
        "y_val_pred_tuned = best_logreg.predict(X_val)\n",
        "y_test_pred_tuned = best_logreg.predict(X_test)\n",
        "\n",
        "print(f\"\\nTuned Logistic Regression Train Accuracy: {accuracy_score(y_train, y_train_pred_tuned):.4f}\")\n",
        "print(f\"Tuned Logistic Regression Validation Accuracy: {accuracy_score(y_val, y_val_pred_tuned):.4f}\")\n",
        "print(f\"Tuned Logistic Regression Test Accuracy: {accuracy_score(y_test, y_test_pred_tuned):.4f}\")\n",
        "\n",
        "print(\"\\n--- Tuned Classification Report (Test Set) ---\")\n",
        "print(classification_report(y_test, y_test_pred_tuned, target_names=label_encoder.classes_.astype(str)))\n"
      ],
      "metadata": {
        "id": "EH9IjOm2AbYn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b82deaa9-22ab-4436-cc69-afdcafc9ed84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Logistic Regression Train Accuracy: 0.5984\n",
            "Baseline Logistic Regression Validation Accuracy: 0.5986\n",
            "Baseline Logistic Regression Test Accuracy: 0.5975\n",
            "\n",
            "--- Baseline Classification Report (Test Set) ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.60      0.60      0.60    141567\n",
            "           1       0.59      0.60      0.59    138451\n",
            "\n",
            "    accuracy                           0.60    280018\n",
            "   macro avg       0.60      0.60      0.60    280018\n",
            "weighted avg       0.60      0.60      0.60    280018\n",
            "\n",
            "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
            "\n",
            "Best hyperparameters: {'C': 1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
            "\n",
            "Tuned Logistic Regression Train Accuracy: 0.5984\n",
            "Tuned Logistic Regression Validation Accuracy: 0.5986\n",
            "Tuned Logistic Regression Test Accuracy: 0.5975\n",
            "\n",
            "--- Tuned Classification Report (Test Set) ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.60      0.60      0.60    141567\n",
            "           1       0.59      0.60      0.59    138451\n",
            "\n",
            "    accuracy                           0.60    280018\n",
            "   macro avg       0.60      0.60      0.60    280018\n",
            "weighted avg       0.60      0.60      0.60    280018\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4) Naive Bayes ✅"
      ],
      "metadata": {
        "id": "1RNB3zZoAbg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Keep the trigram list as is\n",
        "X_tokens = df['trigrams_token'].tolist()  # List of lists of trigram strings\n",
        "y = df['target']\n",
        "\n",
        "# Vectorize using CountVectorizer with a custom analyzer that expects pre-tokenized input\n",
        "vectorizer = CountVectorizer(analyzer=lambda x: x)\n",
        "X = vectorizer.fit_transform(X_tokens)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split into train (60%) and temp (40%)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y_encoded, test_size=0.4, stratify=y_encoded, random_state=42\n",
        ")\n",
        "\n",
        "# Split temp into validation (20%) and test (20%)\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
        ")\n",
        "\n",
        "# Train baseline Naive Bayes model\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions before tuning\n",
        "y_train_pred = nb_model.predict(X_train)\n",
        "y_val_pred = nb_model.predict(X_val)\n",
        "y_test_pred = nb_model.predict(X_test)\n",
        "\n",
        "print(f\"Baseline NB Train Accuracy: {accuracy_score(y_train, y_train_pred):.4f}\")\n",
        "print(f\"Baseline NB Validation Accuracy: {accuracy_score(y_val, y_val_pred):.4f}\")\n",
        "print(f\"Baseline NB Test Accuracy: {accuracy_score(y_test, y_test_pred):.4f}\")\n",
        "\n",
        "print(\"\\n--- Baseline Classification Report (Test Set) ---\")\n",
        "print(classification_report(y_test, y_test_pred, target_names=label_encoder.classes_.astype(str)))\n",
        "\n",
        "# -------------------- Hyperparameter Tuning --------------------\n",
        "param_grid = {\n",
        "    'alpha': [0.001, 0.01, 0.1, 1.0, 10.0]  # Laplace smoothing parameter\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    MultinomialNB(),\n",
        "    param_grid,\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit grid search only on training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "best_nb = grid_search.best_estimator_\n",
        "\n",
        "print(\"\\nBest hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Predictions after tuning\n",
        "y_train_pred_tuned = best_nb.predict(X_train)\n",
        "y_val_pred_tuned = best_nb.predict(X_val)\n",
        "y_test_pred_tuned = best_nb.predict(X_test)\n",
        "\n",
        "print(f\"\\nTuned NB Train Accuracy: {accuracy_score(y_train, y_train_pred_tuned):.4f}\")\n",
        "print(f\"Tuned NB Validation Accuracy: {accuracy_score(y_val, y_val_pred_tuned):.4f}\")\n",
        "print(f\"Tuned NB Test Accuracy: {accuracy_score(y_test, y_test_pred_tuned):.4f}\")\n",
        "\n",
        "print(\"\\n--- Tuned Classification Report (Test Set) ---\")\n",
        "print(classification_report(y_test, y_test_pred_tuned, target_names=label_encoder.classes_.astype(str)))\n"
      ],
      "metadata": {
        "id": "hYodQ4IZAbn_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb3d6b70-8400-4d1d-97d5-fdf6fe058cf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline NB Train Accuracy: 0.5817\n",
            "Baseline NB Validation Accuracy: 0.5832\n",
            "Baseline NB Test Accuracy: 0.5811\n",
            "\n",
            "--- Baseline Classification Report (Test Set) ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.58      0.60      0.59    141567\n",
            "           1       0.58      0.57      0.57    138451\n",
            "\n",
            "    accuracy                           0.58    280018\n",
            "   macro avg       0.58      0.58      0.58    280018\n",
            "weighted avg       0.58      0.58      0.58    280018\n",
            "\n",
            "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
            "\n",
            "Best hyperparameters: {'alpha': 10.0}\n",
            "\n",
            "Tuned NB Train Accuracy: 0.5817\n",
            "Tuned NB Validation Accuracy: 0.5832\n",
            "Tuned NB Test Accuracy: 0.5811\n",
            "\n",
            "--- Tuned Classification Report (Test Set) ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.58      0.60      0.59    141567\n",
            "           1       0.58      0.57      0.57    138451\n",
            "\n",
            "    accuracy                           0.58    280018\n",
            "   macro avg       0.58      0.58      0.58    280018\n",
            "weighted avg       0.58      0.58      0.58    280018\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5) XGBoost ✅"
      ],
      "metadata": {
        "id": "C1p0x2jUNhsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Assume df is already defined\n",
        "X_tokens = df['trigrams_token'].tolist()  # List of lists of trigram strings\n",
        "y = df['target']\n",
        "\n",
        "# Vectorize using CountVectorizer with a custom analyzer\n",
        "vectorizer = CountVectorizer(analyzer=lambda x: x)\n",
        "X = vectorizer.fit_transform(X_tokens)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Train/Val/Test split\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y_encoded, test_size=0.4, stratify=y_encoded, random_state=42\n",
        ")\n",
        "\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
        ")\n",
        "\n",
        "# Baseline XGBoost Model\n",
        "xgb_model = XGBClassifier(\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='mlogloss',\n",
        "    random_state=42\n",
        ")\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions before tuning\n",
        "y_train_pred = xgb_model.predict(X_train)\n",
        "y_val_pred = xgb_model.predict(X_val)\n",
        "y_test_pred = xgb_model.predict(X_test)\n",
        "\n",
        "print(f\"Baseline XGBoost Train Accuracy: {accuracy_score(y_train, y_train_pred):.4f}\")\n",
        "print(f\"Baseline XGBoost Validation Accuracy: {accuracy_score(y_val, y_val_pred):.4f}\")\n",
        "print(f\"Baseline XGBoost Test Accuracy: {accuracy_score(y_test, y_test_pred):.4f}\")\n",
        "\n",
        "print(\"\\n--- Baseline Classification Report (Test Set) ---\")\n",
        "print(classification_report(y_test, y_test_pred, target_names=label_encoder.classes_.astype(str)))\n",
        "\n",
        "# -------------------- Hyperparameter Tuning --------------------\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'subsample': [0.8, 1.0]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n",
        "    param_grid,\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "best_xgb = grid_search.best_estimator_\n",
        "\n",
        "print(\"\\nBest hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Predictions after tuning\n",
        "y_train_pred_tuned = best_xgb.predict(X_train)\n",
        "y_val_pred_tuned = best_xgb.predict(X_val)\n",
        "y_test_pred_tuned = best_xgb.predict(X_test)\n",
        "\n",
        "print(f\"\\nTuned XGBoost Train Accuracy: {accuracy_score(y_train, y_train_pred_tuned):.4f}\")\n",
        "print(f\"Tuned XGBoost Validation Accuracy: {accuracy_score(y_val, y_val_pred_tuned):.4f}\")\n",
        "print(f\"Tuned XGBoost Test Accuracy: {accuracy_score(y_test, y_test_pred_tuned):.4f}\")\n",
        "\n",
        "print(\"\\n--- Tuned Classification Report (Test Set) ---\")\n",
        "print(classification_report(y_test, y_test_pred_tuned, target_names=label_encoder.classes_.astype(str)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmWUEYJ9Nh6r",
        "outputId": "75bd3a70-adbd-4b3c-c35b-46589d69895f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [14:20:33] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline XGBoost Train Accuracy: 0.6415\n",
            "Baseline XGBoost Validation Accuracy: 0.6224\n",
            "Baseline XGBoost Test Accuracy: 0.6218\n",
            "\n",
            "--- Baseline Classification Report (Test Set) ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.62      0.67      0.64    141567\n",
            "           1       0.63      0.57      0.60    138451\n",
            "\n",
            "    accuracy                           0.62    280018\n",
            "   macro avg       0.62      0.62      0.62    280018\n",
            "weighted avg       0.62      0.62      0.62    280018\n",
            "\n",
            "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [14:55:51] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best hyperparameters: {'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 200, 'subsample': 1.0}\n",
            "\n",
            "Tuned XGBoost Train Accuracy: 0.6705\n",
            "Tuned XGBoost Validation Accuracy: 0.6282\n",
            "Tuned XGBoost Test Accuracy: 0.6279\n",
            "\n",
            "--- Tuned Classification Report (Test Set) ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.62      0.67      0.65    141567\n",
            "           1       0.63      0.59      0.61    138451\n",
            "\n",
            "    accuracy                           0.63    280018\n",
            "   macro avg       0.63      0.63      0.63    280018\n",
            "weighted avg       0.63      0.63      0.63    280018\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6) LightGBM (Light Gradient Boosting Machine) ✅"
      ],
      "metadata": {
        "id": "e7Z4WlJWWv4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Keep the trigram list as is\n",
        "X_tokens = df['trigrams_token'].tolist()  # List of lists of trigram strings\n",
        "y = df['target']\n",
        "\n",
        "# Vectorize using CountVectorizer with a custom analyzer that expects pre-tokenized input\n",
        "vectorizer = CountVectorizer(analyzer=lambda x: x)\n",
        "X = vectorizer.fit_transform(X_tokens)\n",
        "\n",
        "# Convert the data in the sparse matrix to float32\n",
        "# LightGBM expects floating point data types.\n",
        "X = X.astype(np.float32)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split into train (60%) and temp (40%)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y_encoded, test_size=0.4, stratify=y_encoded, random_state=42\n",
        ")\n",
        "\n",
        "# Split temp into validation (20%) and test (20%)\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
        ")\n",
        "\n",
        "# Train baseline LightGBM model\n",
        "lgbm_model = LGBMClassifier(random_state=42)\n",
        "lgbm_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions before tuning\n",
        "y_train_pred = lgbm_model.predict(X_train)\n",
        "y_val_pred = lgbm_model.predict(X_val)\n",
        "y_test_pred = lgbm_model.predict(X_test)\n",
        "\n",
        "print(f\"Baseline LightGBM Train Accuracy: {accuracy_score(y_train, y_train_pred):.4f}\")\n",
        "print(f\"Baseline LightGBM Validation Accuracy: {accuracy_score(y_val, y_val_pred):.4f}\")\n",
        "print(f\"Baseline LightGBM Test Accuracy: {accuracy_score(y_test, y_test_pred):.4f}\")\n",
        "\n",
        "print(\"\\n--- Baseline Classification Report (Test Set) ---\")\n",
        "print(classification_report(y_test, y_test_pred, target_names=label_encoder.classes_.astype(str)))\n",
        "\n",
        "# -------------------- Hyperparameter Tuning --------------------\n",
        "param_grid = {\n",
        "    'num_leaves': [31, 50, 100],       # number of leaves in one tree\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'n_estimators': [100, 200, 500],\n",
        "    'max_depth': [-1, 10, 20]          # max tree depth (-1 means no limit)\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    LGBMClassifier(random_state=42),\n",
        "    param_grid,\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit grid search only on training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "best_lgbm = grid_search.best_estimator_\n",
        "\n",
        "print(\"\\nBest hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Predictions after tuning\n",
        "y_train_pred_tuned = best_lgbm.predict(X_train)\n",
        "y_val_pred_tuned = best_lgbm.predict(X_val)\n",
        "y_test_pred_tuned = best_lgbm.predict(X_test)\n",
        "\n",
        "print(f\"\\nTuned LightGBM Train Accuracy: {accuracy_score(y_train, y_train_pred_tuned):.4f}\")\n",
        "print(f\"Tuned LightGBM Validation Accuracy: {accuracy_score(y_val, y_val_pred_tuned):.4f}\")\n",
        "print(f\"Tuned LightGBM Test Accuracy: {accuracy_score(y_test, y_test_pred_tuned):.4f}\")\n",
        "\n",
        "print(\"\\n--- Tuned Classification Report (Test Set) ---\")\n",
        "print(classification_report(y_test, y_test_pred_tuned, target_names=label_encoder.classes_.astype(str)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2UPpSPvWwGq",
        "outputId": "8d057e49-4199-448d-8f75-979b1817fa2f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 415352, number of negative: 424700\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.291910 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1423\n",
            "[LightGBM] [Info] Number of data points in the train set: 840052, number of used features: 36\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.494436 -> initscore=-0.022257\n",
            "[LightGBM] [Info] Start training from score -0.022257\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline LightGBM Train Accuracy: 0.6183\n",
            "Baseline LightGBM Validation Accuracy: 0.6154\n",
            "Baseline LightGBM Test Accuracy: 0.6127\n",
            "\n",
            "--- Baseline Classification Report (Test Set) ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.61      0.66      0.63    141567\n",
            "           1       0.62      0.56      0.59    138451\n",
            "\n",
            "    accuracy                           0.61    280018\n",
            "   macro avg       0.61      0.61      0.61    280018\n",
            "weighted avg       0.61      0.61      0.61    280018\n",
            "\n",
            "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 415352, number of negative: 424700\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.200675 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1423\n",
            "[LightGBM] [Info] Number of data points in the train set: 840052, number of used features: 36\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.494436 -> initscore=-0.022257\n",
            "[LightGBM] [Info] Start training from score -0.022257\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "\n",
            "Best hyperparameters: {'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 500, 'num_leaves': 100}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tuned LightGBM Train Accuracy: 0.6826\n",
            "Tuned LightGBM Validation Accuracy: 0.6325\n",
            "Tuned LightGBM Test Accuracy: 0.6303\n",
            "\n",
            "--- Tuned Classification Report (Test Set) ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.63      0.67      0.65    141567\n",
            "           1       0.64      0.59      0.61    138451\n",
            "\n",
            "    accuracy                           0.63    280018\n",
            "   macro avg       0.63      0.63      0.63    280018\n",
            "weighted avg       0.63      0.63      0.63    280018\n",
            "\n"
          ]
        }
      ]
    }
  ]
}