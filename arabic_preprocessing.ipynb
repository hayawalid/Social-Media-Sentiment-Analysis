{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janaghoniem/Social-Media-Sentiment-Analysis/blob/main/arabic_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-16T23:22:14.171242Z",
          "start_time": "2025-04-16T23:22:14.156313Z"
        },
        "id": "870618d0213e528",
        "outputId": "90aeeba2-8c6e-48c2-d978-fd05afd89e9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "cell_type": "code",
      "source": [
        "import string\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from nltk.tokenize import word_tokenize\n",
        "from camel_tools.tokenizers.word import simple_word_tokenize\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import re\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "id": "870618d0213e528",
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'camel_tools'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9e35ca526b8b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcamel_tools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msimple_word_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'camel_tools'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-16T23:22:14.207243Z",
          "start_time": "2025-04-16T23:22:14.203986Z"
        },
        "id": "270251d2267e928d"
      },
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "id": "270251d2267e928d",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-16T23:22:14.950681Z",
          "start_time": "2025-04-16T23:22:14.280392Z"
        },
        "id": "81b1326b7a36071b"
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('arabic_reviews.csv')\n",
        "# Remove 'Mixed' class\n",
        "df = df[df['label'] != 'Mixed']"
      ],
      "id": "81b1326b7a36071b",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-16T23:22:14.963172Z",
          "start_time": "2025-04-16T23:22:14.958244Z"
        },
        "id": "cf02c3046865e224"
      },
      "cell_type": "code",
      "source": [
        "df"
      ],
      "id": "cf02c3046865e224",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-16T23:22:14.997694Z",
          "start_time": "2025-04-16T23:22:14.991290Z"
        },
        "id": "1ee5dc0e2d9cc663"
      },
      "cell_type": "code",
      "source": [
        "df[('label')].value_counts()"
      ],
      "id": "1ee5dc0e2d9cc663",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-16T23:22:15.136549Z",
          "start_time": "2025-04-16T23:22:15.043801Z"
        },
        "id": "5658f37fc4a510e7"
      },
      "cell_type": "code",
      "source": [
        "px.histogram(df, x=\"label\") #check for imbalance"
      ],
      "id": "5658f37fc4a510e7",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-16T23:22:15.362960Z",
          "start_time": "2025-04-16T23:22:15.203574Z"
        },
        "id": "4b805801e6dd488a"
      },
      "cell_type": "code",
      "source": [
        "df.duplicated().sum() #check duplicates"
      ],
      "id": "4b805801e6dd488a",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-16T23:22:15.385095Z",
          "start_time": "2025-04-16T23:22:15.376137Z"
        },
        "id": "d9134853610518ec"
      },
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "id": "d9134853610518ec",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-16T23:22:15.411698Z",
          "start_time": "2025-04-16T23:22:15.407203Z"
        },
        "id": "8e99b17590f7fa2a"
      },
      "cell_type": "code",
      "source": [
        "print(df['label'].unique())"
      ],
      "id": "8e99b17590f7fa2a",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "3ee976d54ea5efac"
      },
      "cell_type": "markdown",
      "source": [
        "# PREPROCESSING"
      ],
      "id": "3ee976d54ea5efac"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-16T23:22:15.439678Z",
          "start_time": "2025-04-16T23:22:15.437438Z"
        },
        "id": "45abdb6aa6273c4e"
      },
      "cell_type": "code",
      "source": [
        "#stop_words = list(set(stopwords.words('arabic')))\n",
        "\n",
        "stop_words = set([\n",
        "    'في', 'من', 'إلى', 'على', 'أن', 'لا', 'ما', 'هذا', 'هذه', 'ذلك',\n",
        "    'كان', 'يكون', 'هو', 'هي', 'مع', 'بين', 'عن', 'في', 'و', 'أو', 'إذ', 'إذا'\n",
        "])\n",
        "\n",
        "DIALECT_MSA_MAP = {\n",
        "    # Egyptian\n",
        "    \"مش\": \"ليس\",\n",
        "    \"دلوقتي\": \"الآن\",\n",
        "    \"إزاي\": \"كيف\",\n",
        "    \"كده\": \"هكذا\",\n",
        "\n",
        "    # Levantine\n",
        "    \"شو\": \"ماذا\",\n",
        "    \"ليش\": \"لماذا\",\n",
        "    \"كتير\": \"كثيراً\",\n",
        "    \"مافي\": \"لا يوجد\",\n",
        "\n",
        "    # Gulf\n",
        "    \"وايد\": \"كثيراً\",\n",
        "    \"زين\": \"جيد\",\n",
        "    \"شسالفة\": \"ما القصة\",\n",
        "    \"ايوا\": \"نعم\",\n",
        "\n",
        "    # Moroccan / Maghrebi\n",
        "    \"بزاف\": \"كثيراً\",\n",
        "    \"واش\": \"هل\",\n",
        "    \"شنو\": \"ماذا\",\n",
        "    \"دابا\": \"الآن\",\n",
        "\n",
        "    # Iraqi\n",
        "    \"هسة\": \"الآن\",\n",
        "    \"ماكو\": \"لا يوجد\",\n",
        "    \"شكو ماكو\": \"ما الأخبار؟\",\n",
        "    \"شلونك\": \"كيف حالك\",\n",
        "}"
      ],
      "id": "45abdb6aa6273c4e",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-16T23:22:15.494810Z",
          "start_time": "2025-04-16T23:22:15.493027Z"
        },
        "id": "ee8b774172546235"
      },
      "cell_type": "code",
      "source": [
        "arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n",
        "english_punctuations = string.punctuation\n",
        "punctuations_list = arabic_punctuations + english_punctuations"
      ],
      "id": "ee8b774172546235",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-16T23:22:15.523493Z",
          "start_time": "2025-04-16T23:22:15.521779Z"
        },
        "id": "7d10f0919d017c57"
      },
      "cell_type": "code",
      "source": [
        "def dialect_to_msa(text):\n",
        "    words = simple_word_tokenize(text)\n",
        "    return ' '.join(DIALECT_MSA_MAP.get(word, word) for word in words)\n"
      ],
      "id": "7d10f0919d017c57",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-16T23:22:15.528026Z",
          "start_time": "2025-04-16T23:22:15.526449Z"
        },
        "id": "da52c0bd3f2f4c27"
      },
      "cell_type": "code",
      "source": [
        "def remove_gibberish(text):\n",
        "    arabic_pattern = re.compile(r'[^\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FF\\uFB50-\\uFDFF\\uFE70-\\uFEFF\\s.,!?؟]')\n",
        "    return arabic_pattern.sub('', text)\n"
      ],
      "id": "da52c0bd3f2f4c27",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-16T23:22:15.536096Z",
          "start_time": "2025-04-16T23:22:15.534403Z"
        },
        "id": "d9579a40dbc42a9a"
      },
      "cell_type": "code",
      "source": [
        "def remove_diacritics(text):\n",
        "    arabic_diacritics = re.compile(\"\"\" ّ|َ|ً|ُ|ٌ|ِ|ٍ|ْ|ـ\"\"\", re.VERBOSE)\n",
        "    return re.sub(arabic_diacritics, '', str(text))\n"
      ],
      "id": "d9579a40dbc42a9a",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-16T23:22:15.543391Z",
          "start_time": "2025-04-16T23:22:15.541817Z"
        },
        "id": "a655a5cd8ff68146"
      },
      "cell_type": "code",
      "source": [
        "def remove_emoji(text):\n",
        "    regrex_pattern = re.compile(\"[\\U0001F600-\\U0001F64F\"\n",
        "                                \"\\U0001F300-\\U0001F5FF\"\n",
        "                                \"\\U0001F680-\\U0001F6FF\"\n",
        "                                \"\\U0001F1E0-\\U0001F1FF]+\", flags=re.UNICODE)\n",
        "    return regrex_pattern.sub(r'', text)\n"
      ],
      "id": "a655a5cd8ff68146",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-16T23:22:15.550822Z",
          "start_time": "2025-04-16T23:22:15.549375Z"
        },
        "id": "62dc085b6056071d"
      },
      "cell_type": "code",
      "source": [
        "def normalize_elongation(text):\n",
        "    return re.sub(r'(.)\\1+', r'\\1\\1', text)\n"
      ],
      "id": "62dc085b6056071d",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-16T23:22:15.558425Z",
          "start_time": "2025-04-16T23:22:15.556779Z"
        },
        "id": "be5a2df58471017c"
      },
      "cell_type": "code",
      "source": [
        "def normalize_letters(text):\n",
        "    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n",
        "    text = re.sub(\"ى\", \"ي\", text)\n",
        "    text = re.sub(\"ؤ\", \"ء\", text)\n",
        "    text = re.sub(\"ئ\", \"ء\", text)\n",
        "    return text\n"
      ],
      "id": "be5a2df58471017c",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-16T23:22:15.566041Z",
          "start_time": "2025-04-16T23:22:15.564128Z"
        },
        "id": "4754607bf591ee5a"
      },
      "cell_type": "code",
      "source": [
        "def clean_punctuation(text):\n",
        "    # Define punctuation to keep\n",
        "    keep = '،.؟!'  # Arabic comma, dot, question, exclamation\n",
        "\n",
        "    # Remove any punctuation that is NOT in 'keep'\n",
        "    # This keeps Arabic letters and spaces, removes other symbols\n",
        "    text = re.sub(rf'[^\\w\\s{keep}]', '', text)\n",
        "\n",
        "    #remove excessive dots like \"...\"\n",
        "    text = re.sub(r'\\.{2,}', '.', text)\n",
        "\n",
        "    # remove multiple spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text"
      ],
      "id": "4754607bf591ee5a",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-16T23:22:15.573771Z",
          "start_time": "2025-04-16T23:22:15.571870Z"
        },
        "id": "c13a3b46226a436f"
      },
      "cell_type": "code",
      "source": [
        "#pipline\n",
        "def clean_text(text):\n",
        "    text = \"\".join([word for word in text if word not in string.punctuation])\n",
        "    # text = dialect_to_msa(text)\n",
        "    text = remove_gibberish(text)\n",
        "    text = remove_emoji(text)\n",
        "    text = normalize_elongation(text)\n",
        "    text = remove_diacritics(text)\n",
        "    text = normalize_letters(text)\n",
        "    text = clean_punctuation(text)\n",
        "    tokens = word_tokenize(text)\n",
        "    text = ' '.join([word for word in tokens if word not in stop_words])\n",
        "    return text\n"
      ],
      "id": "c13a3b46226a436f",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-16T23:22:25.389613Z",
          "start_time": "2025-04-16T23:22:15.579813Z"
        },
        "id": "d03776c9267dfec2"
      },
      "cell_type": "code",
      "source": [
        "df['cleanedtext'] = df['text'].apply(clean_text)"
      ],
      "id": "d03776c9267dfec2",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-16T23:22:25.457757Z",
          "start_time": "2025-04-16T23:22:25.413845Z"
        },
        "id": "fe91c13125ad2d9f"
      },
      "cell_type": "code",
      "source": [
        "df"
      ],
      "id": "fe91c13125ad2d9f",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-16T23:22:25.663186Z",
          "start_time": "2025-04-16T23:22:25.643200Z"
        },
        "id": "f535d0edb2291b6b"
      },
      "cell_type": "code",
      "source": [
        "# encode label\n",
        "label_mapping = {'Negative': -1, 'Positive': 1}\n",
        "df['label'] = df['label'].str.strip()\n",
        "df['label'] = df['label'].map(label_mapping)\n",
        "\n",
        "df"
      ],
      "id": "f535d0edb2291b6b",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-16T23:22:29.053574Z",
          "start_time": "2025-04-16T23:22:25.774379Z"
        },
        "id": "64bf39778c74255a"
      },
      "cell_type": "code",
      "source": [
        "#TF-IDF vector\n",
        "vectorizer = TfidfVectorizer()\n",
        "x_tfidf = vectorizer.fit_transform(df['cleanedtext'])\n",
        "y = df['label']"
      ],
      "id": "64bf39778c74255a",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-16T23:22:29.108799Z",
          "start_time": "2025-04-16T23:22:29.098052Z"
        },
        "id": "27ad81775cd17abe"
      },
      "cell_type": "code",
      "source": [
        "df"
      ],
      "id": "27ad81775cd17abe",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-16T23:22:29.166597Z",
          "start_time": "2025-04-16T23:22:29.149629Z"
        },
        "id": "20d874ab527974b9"
      },
      "cell_type": "code",
      "source": [
        "print(x_tfidf)"
      ],
      "id": "20d874ab527974b9",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feed-Forward Neural Network"
      ],
      "metadata": {
        "id": "bv7UiNN2Mclw"
      },
      "id": "bv7UiNN2Mclw"
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=5000)  # or 10,000 if your data is big\n",
        "x_tfidf = vectorizer.fit_transform(arabic_df['cleanedtext'])"
      ],
      "metadata": {
        "id": "W4gH0P5lBXeK"
      },
      "execution_count": null,
      "outputs": [],
      "id": "W4gH0P5lBXeK"
    },
    {
      "cell_type": "code",
      "source": [
        "# Labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(arabic_df['label'])\n",
        "y_cat = to_categorical(y)"
      ],
      "metadata": {
        "id": "EBRijzSYCCA6"
      },
      "execution_count": null,
      "outputs": [],
      "id": "EBRijzSYCCA6"
    },
    {
      "cell_type": "code",
      "source": [
        "# Train/Test Split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_tfidf, y_cat, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "AjoG7qwXCEXU"
      },
      "execution_count": null,
      "outputs": [],
      "id": "AjoG7qwXCEXU"
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert sparse matrix to dense (required by Keras)\n",
        "x_train_dense = x_train.toarray()\n",
        "x_test_dense = x_test.toarray()"
      ],
      "metadata": {
        "id": "7B_ssY-tCGtf"
      },
      "execution_count": null,
      "outputs": [],
      "id": "7B_ssY-tCGtf"
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Input\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Define EarlyStopping\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Model with regularization and dropout\n",
        "model = Sequential([\n",
        "    Input(shape=(x_train_dense.shape[1],)),\n",
        "    Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "    Dropout(0.5),\n",
        "    Dense(y_cat.shape[1], activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train with EarlyStopping\n",
        "history = model.fit(\n",
        "    x_train_dense, y_train,\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    validation_split=0.1,\n",
        "    callbacks=[early_stop]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8VPhFwQCI2-",
        "outputId": "d22d7f64-2de2-4017-824b-9817acf210ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.7707 - loss: 0.6043 - val_accuracy: 0.8311 - val_loss: 0.5172\n",
            "Epoch 2/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 14ms/step - accuracy: 0.8409 - loss: 0.5133 - val_accuracy: 0.8382 - val_loss: 0.5049\n",
            "Epoch 3/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 15ms/step - accuracy: 0.8401 - loss: 0.5084 - val_accuracy: 0.8391 - val_loss: 0.5083\n",
            "Epoch 4/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 15ms/step - accuracy: 0.8466 - loss: 0.5029 - val_accuracy: 0.8335 - val_loss: 0.5090\n",
            "Epoch 5/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 14ms/step - accuracy: 0.8508 - loss: 0.4959 - val_accuracy: 0.8416 - val_loss: 0.5056\n"
          ]
        }
      ],
      "id": "P8VPhFwQCI2-"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Predict on test set\n",
        "y_test_pred = model.predict(x_test_dense)\n",
        "y_test_pred_classes = y_test_pred.argmax(axis=1)\n",
        "y_test_true = y_test.argmax(axis=1)\n",
        "\n",
        "# Accuracy and F1\n",
        "acc = accuracy_score(y_test_true, y_test_pred_classes)\n",
        "f1 = f1_score(y_test_true, y_test_pred_classes, average='weighted')  # or 'macro'\n",
        "\n",
        "print(f\"Test Accuracy: {acc:.4f}\")\n",
        "print(f\"Weighted F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAu2CfMVIvg_",
        "outputId": "fa474112-595c-4ed1-cadd-317842a11e10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m417/417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step\n",
            "Test Accuracy: 0.8332\n",
            "Weighted F1 Score: 0.8332\n"
          ]
        }
      ],
      "id": "IAu2CfMVIvg_"
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(text):\n",
        "    vec = vectorizer.transform([text]).toarray()\n",
        "    pred = model.predict(vec)\n",
        "    class_idx = pred.argmax()\n",
        "\n",
        "    # Map -1 and 1 to labels manually\n",
        "    index_to_label = {-1: \"negative\", 1: \"positive\"}\n",
        "    original_label = label_encoder.inverse_transform([class_idx])[0]\n",
        "    label_name = index_to_label[original_label]\n",
        "    print(text)\n",
        "    print(f\"Predicted Class: {label_name}\")"
      ],
      "metadata": {
        "id": "8IsyiuqILC8_"
      },
      "execution_count": null,
      "outputs": [],
      "id": "8IsyiuqILC8_"
    },
    {
      "cell_type": "code",
      "source": [
        "predict_sentiment(\"المنتج سيء للغاية\")\n",
        "predict_sentiment(\"أحببت هذا الفيلم كثيرًا\")\n",
        "predict_sentiment(\"رائع\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0jzmoGzLEU8",
        "outputId": "3d8007d7-38a4-496e-f680-ff4c4c6dfc01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "المنتج سيء للغاية\n",
            "Predicted Class: negative\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "أحببت هذا الفيلم كثيرًا\n",
            "Predicted Class: negative\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "رائع\n",
            "Predicted Class: positive\n"
          ]
        }
      ],
      "id": "n0jzmoGzLEU8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LSTM/GRU Model"
      ],
      "metadata": {
        "id": "RmIkKbamSyYF"
      },
      "id": "RmIkKbamSyYF"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, GRU, Bidirectional\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Step 1: Tokenization\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(arabic_df['cleanedtext'])\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(arabic_df['cleanedtext'])\n",
        "padded = pad_sequences(sequences, maxlen=100)  # You can adjust maxlen\n",
        "\n",
        "# Step 2: Labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(arabic_df['label'])  # assumes labels are -1, 1\n",
        "y_cat = to_categorical(y)\n",
        "\n",
        "# Step 3: Train/Test split\n",
        "x_train, x_test, y_train, y_test = train_test_split(padded, y_cat, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=10000, output_dim=128, input_length=100),\n",
        "    Bidirectional(LSTM(64, return_sequences=True)),\n",
        "    Dropout(0.5),\n",
        "    Bidirectional(LSTM(32)),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.4),\n",
        "    Dense(y_cat.shape[1], activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Step 5: Training\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    validation_split=0.1,\n",
        "    callbacks=[early_stop]\n",
        ")\n",
        "\n",
        "# Step 6: Evaluation\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "y_pred = model.predict(x_test).argmax(axis=1)\n",
        "y_true = y_test.argmax(axis=1)\n",
        "\n",
        "print(\"Test Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "print(\"F1 Score:\", f1_score(y_true, y_pred, average='weighted'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dYRbeODSw4e",
        "outputId": "8dd9dbbd-8b08-4951-ea35-c1e216615c30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 251ms/step - accuracy: 0.7373 - loss: 0.4948 - val_accuracy: 0.8526 - val_loss: 0.3315\n",
            "Epoch 2/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m378s\u001b[0m 249ms/step - accuracy: 0.8904 - loss: 0.2734 - val_accuracy: 0.8474 - val_loss: 0.3466\n",
            "Epoch 3/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 248ms/step - accuracy: 0.9214 - loss: 0.1988 - val_accuracy: 0.8412 - val_loss: 0.4231\n",
            "Epoch 4/20\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m380s\u001b[0m 247ms/step - accuracy: 0.9437 - loss: 0.1550 - val_accuracy: 0.8416 - val_loss: 0.4418\n",
            "\u001b[1m417/417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 64ms/step\n",
            "Test Accuracy: 0.8499212480312007\n",
            "F1 Score: 0.8497950272741241\n"
          ]
        }
      ],
      "id": "-dYRbeODSw4e"
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(text):\n",
        "    vec = vectorizer.transform([text]).toarray()\n",
        "    pred = model.predict(vec)\n",
        "    class_idx = pred.argmax()\n",
        "\n",
        "    # Map -1 and 1 to labels manually\n",
        "    index_to_label = {-1: \"negative\", 1: \"positive\"}\n",
        "    original_label = label_encoder.inverse_transform([class_idx])[0]\n",
        "    label_name = index_to_label[original_label]\n",
        "    print(text)\n",
        "    print(f\"Predicted Class: {label_name}\")"
      ],
      "metadata": {
        "id": "g10cneVaY_G8"
      },
      "execution_count": null,
      "outputs": [],
      "id": "g10cneVaY_G8"
    },
    {
      "cell_type": "code",
      "source": [
        "predict_sentiment(\"المنتج سيء للغاية\")\n",
        "predict_sentiment(\"أحببت هذا الفيلم كثيرًا\")\n",
        "predict_sentiment(\"رائع\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3747f609-c3a7-483c-cbc6-65f04774290d",
        "id": "Op6QqMbcY_G9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "المنتج سيء للغاية\n",
            "Predicted Class: negative\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "أحببت هذا الفيلم كثيرًا\n",
            "Predicted Class: negative\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "رائع\n",
            "Predicted Class: negative\n"
          ]
        }
      ],
      "id": "Op6QqMbcY_G9"
    },
    {
      "metadata": {
        "id": "60a1f299e1d3c3e"
      },
      "cell_type": "markdown",
      "source": [
        "# MODEL TRAINING"
      ],
      "id": "60a1f299e1d3c3e"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-16T23:22:31.651320Z",
          "start_time": "2025-04-16T23:22:29.260493Z"
        },
        "id": "96fa09df113a45c3"
      },
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x_tfidf, y, test_size=0.1, random_state=42)\n",
        "\n",
        "# Train classifier\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(x_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "id": "96fa09df113a45c3",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "d6e3b083aa2851db"
      },
      "cell_type": "markdown",
      "source": [
        "# ARABERT TEST"
      ],
      "id": "d6e3b083aa2851db"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}