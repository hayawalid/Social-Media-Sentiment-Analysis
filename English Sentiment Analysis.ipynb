{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# English Sentiment Analysis",
   "id": "8d866fb9dd3f4aca"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-17T09:47:10.725332Z",
     "start_time": "2025-04-17T09:47:10.717295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from ftfy import fix_text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from langdetect import detect, LangDetectException\n",
    "from tqdm import tqdm\n",
    "import html\n",
    "import string\n",
    "import re\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 126
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Loading",
   "id": "587490a91fc2722a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:18:41.629111Z",
     "start_time": "2025-04-17T00:18:39.344305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\Shosh\\Desktop\\Univeristy\\Semester 6\\Data Science\\Project\\english dataset.csv\", encoding=\"ISO-8859-1\", header=None)\n",
    "df.head()"
   ],
   "id": "cc809941825a26b2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   0           1                             2         3                4                                                  5\n",
       "0  0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton  is upset that he can't update his Facebook by ...\n",
       "2  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus  @Kenichan I dived many times for the ball. Man...\n",
       "3  0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF    my whole body feels itchy and like its on fire \n",
       "4  0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli  @nationwideclass no, it's not behaving at all...."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:18:41.671910Z",
     "start_time": "2025-04-17T00:18:41.660056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "columns = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"tweet\"]\n",
    "df.columns= columns\n",
    "df.head()"
   ],
   "id": "e38b3515fa67caf5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   target         ids                          date      flag             user                                              tweet\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton  is upset that he can't update his Facebook by ...\n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus  @Kenichan I dived many times for the ball. Man...\n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF    my whole body feels itchy and like its on fire \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli  @nationwideclass no, it's not behaving at all...."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:18:41.893873Z",
     "start_time": "2025-04-17T00:18:41.742387Z"
    }
   },
   "cell_type": "code",
   "source": "df.info()",
   "id": "3b4478288dd39a24",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count    Dtype \n",
      "---  ------  --------------    ----- \n",
      " 0   target  1600000 non-null  int64 \n",
      " 1   ids     1600000 non-null  int64 \n",
      " 2   date    1600000 non-null  object\n",
      " 3   flag    1600000 non-null  object\n",
      " 4   user    1600000 non-null  object\n",
      " 5   tweet   1600000 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 73.2+ MB\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:18:43.368729Z",
     "start_time": "2025-04-17T00:18:41.942519Z"
    }
   },
   "cell_type": "code",
   "source": "df.duplicated().sum()",
   "id": "573dcc4d92a0b4e2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:18:43.512998Z",
     "start_time": "2025-04-17T00:18:43.371278Z"
    }
   },
   "cell_type": "code",
   "source": "df.isna().sum()",
   "id": "e22781cde4f7f45c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target    0\n",
       "ids       0\n",
       "date      0\n",
       "flag      0\n",
       "user      0\n",
       "tweet     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:18:43.656256Z",
     "start_time": "2025-04-17T00:18:43.557956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# extract el cols el me7taga\n",
    "data = df[['tweet','target']]\n",
    "data.head()\n"
   ],
   "id": "3a9c32f375589697",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                               tweet  target\n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...       0\n",
       "1  is upset that he can't update his Facebook by ...       0\n",
       "2  @Kenichan I dived many times for the ball. Man...       0\n",
       "3    my whole body feels itchy and like its on fire        0\n",
       "4  @nationwideclass no, it's not behaving at all....       0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tweet Cleaning",
   "id": "5aeb1af3f88a7fd0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:18:43.747005Z",
     "start_time": "2025-04-17T00:18:43.739673Z"
    }
   },
   "cell_type": "code",
   "source": "data['tweet'][0]",
   "id": "8adefcfbacbf026c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:18:43.865787Z",
     "start_time": "2025-04-17T00:18:43.856372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_tweet(text):\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove @mentions\n",
    "    text = re.sub(r'#\\w+', '', text)  # Optional: Remove hashtags\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with one\n",
    "    return text.strip()\n",
    "\n",
    "\n"
   ],
   "id": "3e82eb169e8c9430",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:18:43.965377Z",
     "start_time": "2025-04-17T00:18:43.960018Z"
    }
   },
   "cell_type": "code",
   "source": "clean_tweet(data['tweet'][0])",
   "id": "c49599100f3b9d49",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"- A that's a bummer. You shoulda got David Carr of Third Day to do it. ;D\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:18:50.833263Z",
     "start_time": "2025-04-17T00:18:44.024724Z"
    }
   },
   "cell_type": "code",
   "source": "data['tweet'] = data['tweet'].apply(clean_tweet)",
   "id": "20642f9c5f18c187",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shosh\\AppData\\Local\\Temp\\ipykernel_11964\\339823103.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['tweet'] = data['tweet'].apply(clean_tweet)\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:18:50.861622Z",
     "start_time": "2025-04-17T00:18:50.851871Z"
    }
   },
   "cell_type": "code",
   "source": "data.head(10)",
   "id": "10a812b9fb28f0ff",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                               tweet  target\n",
       "0  - A that's a bummer. You shoulda got David Car...       0\n",
       "1  is upset that he can't update his Facebook by ...       0\n",
       "2  I dived many times for the ball. Managed to sa...       0\n",
       "3     my whole body feels itchy and like its on fire       0\n",
       "4  no, it's not behaving at all. i'm mad. why am ...       0\n",
       "5                                 not the whole crew       0\n",
       "6                                         Need a hug       0\n",
       "7  hey long time no see! Yes.. Rains a bit ,only ...       0\n",
       "8                           nope they didn't have it       0\n",
       "9                                     que me muera ?       0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- A that's a bummer. You shoulda got David Car...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I dived many times for the ball. Managed to sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no, it's not behaving at all. i'm mad. why am ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>not the whole crew</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Need a hug</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hey long time no see! Yes.. Rains a bit ,only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nope they didn't have it</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>que me muera ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Normalization",
   "id": "6d2f9ca5b16320e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:18:50.921645Z",
     "start_time": "2025-04-17T00:18:50.914621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def normalize_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove excessive punctuation (e.g., ... → empty, . → empty)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n"
   ],
   "id": "f9f04f8572e9f219",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:18:57.286167Z",
     "start_time": "2025-04-17T00:18:50.960080Z"
    }
   },
   "cell_type": "code",
   "source": "data['normalized_tweet'] = data['tweet'].apply(normalize_text)\n",
   "id": "d67cb2a2cf18845b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shosh\\AppData\\Local\\Temp\\ipykernel_11964\\3244975462.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['normalized_tweet'] = data['tweet'].apply(normalize_text)\n"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:18:57.305067Z",
     "start_time": "2025-04-17T00:18:57.288832Z"
    }
   },
   "cell_type": "code",
   "source": "data.head(10)",
   "id": "20af54bb0fe97d73",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                               tweet  target                                   normalized_tweet\n",
       "0  - A that's a bummer. You shoulda got David Car...       0  a thats a bummer you shoulda got david carr of...\n",
       "1  is upset that he can't update his Facebook by ...       0  is upset that he cant update his facebook by t...\n",
       "2  I dived many times for the ball. Managed to sa...       0  i dived many times for the ball managed to sav...\n",
       "3     my whole body feels itchy and like its on fire       0     my whole body feels itchy and like its on fire\n",
       "4  no, it's not behaving at all. i'm mad. why am ...       0  no its not behaving at all im mad why am i her...\n",
       "5                                 not the whole crew       0                                 not the whole crew\n",
       "6                                         Need a hug       0                                         need a hug\n",
       "7  hey long time no see! Yes.. Rains a bit ,only ...       0  hey long time no see yes rains a bit only a bi...\n",
       "8                           nope they didn't have it       0                            nope they didnt have it\n",
       "9                                     que me muera ?       0                                       que me muera"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>target</th>\n",
       "      <th>normalized_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- A that's a bummer. You shoulda got David Car...</td>\n",
       "      <td>0</td>\n",
       "      <td>a thats a bummer you shoulda got david carr of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he cant update his facebook by t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I dived many times for the ball. Managed to sa...</td>\n",
       "      <td>0</td>\n",
       "      <td>i dived many times for the ball managed to sav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no, it's not behaving at all. i'm mad. why am ...</td>\n",
       "      <td>0</td>\n",
       "      <td>no its not behaving at all im mad why am i her...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>not the whole crew</td>\n",
       "      <td>0</td>\n",
       "      <td>not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Need a hug</td>\n",
       "      <td>0</td>\n",
       "      <td>need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hey long time no see! Yes.. Rains a bit ,only ...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey long time no see yes rains a bit only a bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nope they didn't have it</td>\n",
       "      <td>0</td>\n",
       "      <td>nope they didnt have it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>que me muera ?</td>\n",
       "      <td>0</td>\n",
       "      <td>que me muera</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:19:22.528556Z",
     "start_time": "2025-04-17T00:18:57.349125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ftfy  # fixes text for you!\n",
    "\n",
    "# install it first if needed: pip install ftfy\n",
    "\n",
    "data[\"normalized_tweet\"] = data[\"normalized_tweet\"].apply(fix_text)\n"
   ],
   "id": "51d5557d356b8277",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shosh\\AppData\\Local\\Temp\\ipykernel_11964\\895449020.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"normalized_tweet\"] = data[\"normalized_tweet\"].apply(fix_text)\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Duplication",
   "id": "c2b9c46c4bed3f0f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:19:22.912258Z",
     "start_time": "2025-04-17T00:19:22.552215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "duplicates=data.duplicated(subset='normalized_tweet')\n",
    "counts= duplicates.value_counts()\n",
    "print(counts)"
   ],
   "id": "7ab9f7c096a7211a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    1519313\n",
      "True       80687\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:19:23.970697Z",
     "start_time": "2025-04-17T00:19:22.912258Z"
    }
   },
   "cell_type": "code",
   "source": "data.duplicated().sum()",
   "id": "7c5e7308d5338441",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(47969)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:19:24.612800Z",
     "start_time": "2025-04-17T00:19:24.022727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Find duplicated tweets\n",
    "conflicts = data[data.duplicated(subset='normalized_tweet', keep=False)]\n",
    "\n",
    "#Group by tweet and collect targets\n",
    "conflicting_tweets = conflicts.groupby('normalized_tweet')['target'].apply(list)\n",
    "\n",
    "#Filter only tweets that have conflicting targets\n",
    "conflicting_tweets = conflicting_tweets[conflicting_tweets.apply(lambda x: len(set(x)) > 1)]\n",
    "\n",
    "#Create a DataFrame for display\n",
    "conflicting_tweets_df = pd.DataFrame({\n",
    "    'normalized_tweet': conflicting_tweets.index,\n",
    "    'Conflicting Targets': conflicting_tweets.values\n",
    "})\n",
    "\n",
    "#Truncate long lists in 'Conflicting Targets' (show first 5 elements)\n",
    "conflicting_tweets_df['Conflicting Targets'] = conflicting_tweets_df['Conflicting Targets'].apply(\n",
    "    lambda x: f\"{x[:5]}... (total {len(x)})\" if len(x) > 5 else x\n",
    ")\n",
    "\n",
    "#Set display options\n",
    "pd.set_option('display.max_colwidth', 50)  # Limit tweet width\n",
    "pd.set_option('display.width', 1000)       # Ensure table fits horizontally\n",
    "\n",
    "# Display first 10 rows\n",
    "print(conflicting_tweets_df.head(20))\n",
    "\n",
    "# Optional: Show total count\n",
    "print(f\"\\nTotal conflicting tweets: {len(conflicting_tweets_df)}\")"
   ],
   "id": "10cda91b7b8cb052",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     normalized_tweet              Conflicting Targets\n",
      "0                                                      [0, 0, 0, 0, 0]... (total 3439)\n",
      "1   00 and the kiss i want to see it im so happy t...                           [0, 4]\n",
      "2                                                   1     [0, 0, 4, 4, 4]... (total 6)\n",
      "3   1 1 rohan slept well but woke up at 6am then a...                           [0, 4]\n",
      "4                                 1 exam down 4 to go                           [0, 4]\n",
      "5   1 hour53 minutes actually from 85th to 33rd ah...                           [0, 4]\n",
      "6                                          1 more day                           [0, 4]\n",
      "7   1 new outfit and a pair of shoes is allifound ...                        [0, 0, 4]\n",
      "8   1 uh huhs sures x 2 happy bday to you 3 awwwww...                           [0, 4]\n",
      "9                                           100 agree                           [0, 4]\n",
      "10  1027 am in two hours i have to go at school th...                           [0, 4]\n",
      "11                            173 days until new moon                           [0, 4]\n",
      "12  19 days until sssc until then i have to be bor...                           [0, 4]\n",
      "13  2 and a half hour maths exam actually killed m...                           [0, 4]\n",
      "14                                       2 hours left                           [0, 4]\n",
      "15                                       2 more exams                           [0, 4]\n",
      "16                                       2 more hours                           [0, 4]\n",
      "17                                       2 more weeks                           [0, 4]\n",
      "18  2nd show only knew about yesterday will buy ti...                           [0, 4]\n",
      "19  3 days leave then easter no work for a week ex...                           [0, 4]\n",
      "\n",
      "Total conflicting tweets: 4110\n"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:19:24.817724Z",
     "start_time": "2025-04-17T00:19:24.619872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a DataFrame for export (without truncation)\n",
    "conflicting_tweets_df = pd.DataFrame({\n",
    "    'normalized_tweet': conflicting_tweets.index,\n",
    "    'Conflicting Targets': conflicting_tweets.values,\n",
    "    'Number of Conflicts': conflicting_tweets.apply(len)  # Add count of conflicts\n",
    "})\n",
    "\n",
    "# Export to Excel (full content, no truncation)\n",
    "output_file = \"conflicting_tweets_full1.xlsx\"\n",
    "conflicting_tweets_df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Successfully saved all {len(conflicting_tweets_df)} conflicting tweets to '{output_file}'\")"
   ],
   "id": "fc1abb811fb761b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved all 4110 conflicting tweets to 'conflicting_tweets_full1.xlsx'\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:19:24.865441Z",
     "start_time": "2025-04-17T00:19:24.860121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# def resolve_conflict(labels, tweet):\n",
    "#     # Threshold for short tweets (adjust as needed)\n",
    "#     word_count = len(tweet.split())\n",
    "#     if word_count < 2:\n",
    "#         return 'drop'\n",
    "#\n",
    "#     label_counts = pd.Series(labels).value_counts()\n",
    "#\n",
    "#     if len(label_counts) == 1:\n",
    "#         return label_counts.index[0]\n",
    "#     elif label_counts.get(0, 0) == label_counts.get(4, 0):\n",
    "#         return 2  # Equal → Neutral\n",
    "#     elif abs(label_counts.get(0, 0) - label_counts.get(4, 0)) == 1:\n",
    "#         return 'drop'  # Too close, too risky\n",
    "#     else:\n",
    "#         return label_counts.idxmax()  # Strong majority\n"
   ],
   "id": "62f21bfcdf331aab",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:19:24.875625Z",
     "start_time": "2025-04-17T00:19:24.865441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def resolve_conflict(labels, tweet):\n",
    "    # Threshold for short tweets (adjust as needed)\n",
    "    word_count = len(tweet.split())\n",
    "    if word_count < 2:\n",
    "        return 'drop'\n",
    "\n",
    "    label_counts = pd.Series(labels).value_counts()\n",
    "\n",
    "    if len(label_counts) == 1:\n",
    "        return label_counts.index[0]\n",
    "    elif label_counts.get(0, 0) == label_counts.get(4, 0):\n",
    "        return 'drop'  # Previously neutral, now drop\n",
    "    elif abs(label_counts.get(0, 0) - label_counts.get(4, 0)) == 1:\n",
    "        return 'drop'  # Too close, too risky\n",
    "    else:\n",
    "        return label_counts.idxmax()  # Strong majority\n"
   ],
   "id": "af508cd4133d4153",
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:21:58.484940Z",
     "start_time": "2025-04-17T00:19:24.897466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Group by normalized tweet\n",
    "grouped = data.groupby(\"normalized_tweet\")[\"target\"].apply(list).reset_index(name=\"labels\")\n",
    "\n",
    "#Apply the conflict resolution function\n",
    "grouped[\"resolved_label\"] = grouped.apply(\n",
    "    lambda row: resolve_conflict(row[\"labels\"], row[\"normalized_tweet\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Merge resolved labels back to main dataframe\n",
    "data = data.merge(grouped[[\"normalized_tweet\", \"resolved_label\"]], on=\"normalized_tweet\", how=\"left\")\n",
    "\n",
    "#Drop rows marked as 'drop'\n",
    "data = data[data[\"resolved_label\"] != \"drop\"].copy()\n",
    "\n",
    "#Update target column\n",
    "data[\"target\"] = data[\"resolved_label\"]\n",
    "\n",
    "#Drop the helper column\n",
    "data.drop(columns=[\"resolved_label\"], inplace=True)\n",
    "\n",
    "# Optional: Reset index\n",
    "data.reset_index(drop=True, inplace=True)\n"
   ],
   "id": "e0e5055e4e0696d0",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:21:58.850276Z",
     "start_time": "2025-04-17T00:21:58.521739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dups = data[data.duplicated(subset=\"normalized_tweet\", keep=False)]\n",
    "print(f\"Total remaining duplicated normalized tweets: {len(dups)}\")\n"
   ],
   "id": "942fff56dd9b1ce9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total remaining duplicated normalized tweets: 79428\n"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:22:02.004043Z",
     "start_time": "2025-04-17T00:21:58.854039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "conflicts_check = data.groupby(\"normalized_tweet\")[\"target\"].nunique()\n",
    "conflicting_final = conflicts_check[conflicts_check > 1]\n",
    "print(f\"Tweets with conflicting labels after cleanup: {len(conflicting_final)}\")\n"
   ],
   "id": "d244310d3a639f68",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets with conflicting labels after cleanup: 0\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:22:02.903801Z",
     "start_time": "2025-04-17T00:22:02.022360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Drop duplicate tweets after resolving conflicts\n",
    "data = data.drop_duplicates(subset=[\"normalized_tweet\"]).reset_index(drop=True)\n",
    "\n",
    "# Check again for duplicates\n",
    "dups = data[data.duplicated(subset=\"normalized_tweet\", keep=False)]\n",
    "print(f\"Total remaining duplicated normalized tweets: {len(dups)}\")\n"
   ],
   "id": "a64d26c6cbaff988",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total remaining duplicated normalized tweets: 0\n"
     ]
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:22:04.170006Z",
     "start_time": "2025-04-17T00:22:02.935146Z"
    }
   },
   "cell_type": "code",
   "source": "data.duplicated().sum()",
   "id": "3b41a16f6b465d9c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 87
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Additional Text Cleaning\n",
    "1. Remove unnecessary punctuation to enhance textual clarity.\n",
    "2. Correct spelling errors to improve accuracy. If a word remains nonsensical after correction, it is classified as an outlier for removal.\n",
    "3. Apply the TF-IDF model, which requires properly spelled words, to effectively identify and filter out low-value or meaningless terms.\n",
    "\n"
   ],
   "id": "67e685faeea7c6b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:22:04.192007Z",
     "start_time": "2025-04-17T00:22:04.185846Z"
    }
   },
   "cell_type": "code",
   "source": "data.shape",
   "id": "1ef304454a9d6efb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1512041, 3)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:22:04.231829Z",
     "start_time": "2025-04-17T00:22:04.222900Z"
    }
   },
   "cell_type": "code",
   "source": "data.head()",
   "id": "de136c0e3db03b27",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                               tweet target                                   normalized_tweet\n",
       "0  - A that's a bummer. You shoulda got David Car...      0  a thats a bummer you shoulda got david carr of...\n",
       "1  is upset that he can't update his Facebook by ...      0  is upset that he cant update his facebook by t...\n",
       "2  I dived many times for the ball. Managed to sa...      0  i dived many times for the ball managed to sav...\n",
       "3     my whole body feels itchy and like its on fire      0     my whole body feels itchy and like its on fire\n",
       "4  no, it's not behaving at all. i'm mad. why am ...      0  no its not behaving at all im mad why am i her..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>target</th>\n",
       "      <th>normalized_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- A that's a bummer. You shoulda got David Car...</td>\n",
       "      <td>0</td>\n",
       "      <td>a thats a bummer you shoulda got david carr of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he cant update his facebook by t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I dived many times for the ball. Managed to sa...</td>\n",
       "      <td>0</td>\n",
       "      <td>i dived many times for the ball managed to sav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no, it's not behaving at all. i'm mad. why am ...</td>\n",
       "      <td>0</td>\n",
       "      <td>no its not behaving at all im mad why am i her...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:23:34.510997Z",
     "start_time": "2025-04-17T00:22:04.281417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define punctuation considered meaningless (not including ? and !)\n",
    "meaningless_punctuations = [\"_\", \"-\", \"___\", \"....\", \"....\", \"...\", \"--\"]\n",
    "\n",
    "# Check if a token is meaningless punctuation\n",
    "def contains_meaningless_punct(text):\n",
    "    tokens = text.split()\n",
    "    return any(\n",
    "        re.fullmatch(rf\"[{re.escape(punct)}]+\", token) for punct in meaningless_punctuations for token in tokens\n",
    "    )\n",
    "\n",
    "# Detect tweets with meaningless punctuation tokens\n",
    "punct_token_tweets = data[data[\"normalized_tweet\"].apply(contains_meaningless_punct)]\n",
    "\n",
    "# Show how many tweets contain these meaningless punctuations\n",
    "print(f\"Tweets with meaningless punctuation tokens: {len(punct_token_tweets)}\")\n",
    "\n",
    "# Display the first 10 tweets with meaningless punctuation\n",
    "print(punct_token_tweets[\"normalized_tweet\"].head(10))\n"
   ],
   "id": "d2657498ae7b2639",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets with meaningless punctuation tokens: 1108\n",
      "2047     think im an insomniac i just cant sleep birthd...\n",
      "3796     this will have to do i lost the password to th...\n",
      "4172     rainy day so bad trying to make my hair look g...\n",
      "4288     goign 2 scholl baghhh _ im tires woke up early...\n",
      "5254     i want to come home from work im sick _ my leg...\n",
      "8261     finally going to the social security offices _...\n",
      "8715     honey what i must do for you tell me quothiquo...\n",
      "12477    ugggh ___ have to go cut the grass then write ...\n",
      "12826    wish i could watch im out of town and only mob...\n",
      "13301    i spent the whole morning practicing piano i n...\n",
      "Name: normalized_tweet, dtype: object\n"
     ]
    }
   ],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:23:34.565623Z",
     "start_time": "2025-04-17T00:23:34.562151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_meaningless_punct_tokens(text):\n",
    "    tokens = text.split()\n",
    "    cleaned = [\n",
    "        token for token in tokens\n",
    "        if not re.fullmatch(rf\"[{re.escape(string.punctuation)}]+\", token)\n",
    "    ]\n",
    "    return \" \".join(cleaned)\n"
   ],
   "id": "b26c53b699032ea4",
   "outputs": [],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:24:15.846776Z",
     "start_time": "2025-04-17T00:23:34.565623Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Apply the function\n",
    "data[\"normalized_tweet\"] = data[\"normalized_tweet\"].apply(remove_meaningless_punct_tokens)\n"
   ],
   "id": "a76b0f94957ec59e",
   "outputs": [],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:25:37.839014Z",
     "start_time": "2025-04-17T00:24:15.865472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Count the tweets with meaningless punctuation after removal\n",
    "remaining_punct_tweets = data[data[\"normalized_tweet\"].apply(contains_meaningless_punct)]\n",
    "print(f\"Remaining tweets with meaningless punctuation tokens: {len(remaining_punct_tweets)}\")\n"
   ],
   "id": "55bc9e9e19ae2d29",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining tweets with meaningless punctuation tokens: 0\n"
     ]
    }
   ],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:25:37.880531Z",
     "start_time": "2025-04-17T00:25:37.873624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# #will see the accuracy without correcting spelling mistakes: 78%\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "# from autocorrect import Speller\n",
    "#\n",
    "# # Initialize the spell checker\n",
    "# spell = Speller()\n",
    "#\n",
    "# # Function to correct spelling\n",
    "# def correct_spelling_autocorrect(text):\n",
    "#     return spell(text)\n",
    "#\n",
    "# # Batch processing for spell correction\n",
    "# def batch_process_spell_check(df, batch_size=10000):\n",
    "#     corrected_texts = []\n",
    "#     for start in tqdm(range(0, len(df), batch_size)):\n",
    "#         batch = df[\"normalized_tweet\"].iloc[start:start+batch_size]\n",
    "#         corrected_batch = batch.apply(correct_spelling_autocorrect)\n",
    "#         corrected_texts.extend(corrected_batch)\n",
    "#     return corrected_texts\n",
    "#\n",
    "# # Assuming `data` is the DataFrame containing your tweets\n",
    "# corrected_tweets = batch_process_spell_check(data)\n",
    "#\n",
    "# # Add the corrected tweets to the DataFrame\n",
    "# data[\"corrected_tweet\"] = corrected_tweets\n",
    "#\n",
    "# # Optionally, save the DataFrame with corrected tweets\n",
    "# data.to_csv(\"corrected_tweets_autocorrect.csv\", index=False)\n",
    "#\n",
    "# # Check the first few rows of the corrected tweets\n",
    "# print(data[[\"normalized_tweet\", \"corrected_tweet\"]].head())\n"
   ],
   "id": "af8771a55a08836e",
   "outputs": [],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:25:37.926290Z",
     "start_time": "2025-04-17T00:25:37.923085Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "129b88e417ebe159",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:25:38.141247Z",
     "start_time": "2025-04-17T00:25:37.932130Z"
    }
   },
   "cell_type": "code",
   "source": "data[\"normalized_tweet\"] = data[\"normalized_tweet\"].apply(html.unescape)\n",
   "id": "e35257069289287f",
   "outputs": [],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:25:38.577201Z",
     "start_time": "2025-04-17T00:25:38.160796Z"
    }
   },
   "cell_type": "code",
   "source": "data[\"normalized_tweet\"] = data[\"normalized_tweet\"].str.replace(r\"ï½\", \"'\", regex=True)\n",
   "id": "7a97cf2a09ca6a37",
   "outputs": [],
   "execution_count": 96
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Outliers and Error handling\n",
    "things I noticed:\n",
    "1. numbers only without text\n",
    "2. one/two words 3amla conflict gamed (mafrod handled)\n",
    "3. punctuation bas without text (mafrod handled)"
   ],
   "id": "40959e49578d11d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "i will first detect then decide what needs to be removed",
   "id": "ebd01dff96d516d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:25:41.427964Z",
     "start_time": "2025-04-17T00:25:38.626180Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Detect Short Tweets (possible outliers)\n",
    "short_outliers = data[data[\"normalized_tweet\"].str.split().apply(len) < 2]\n",
    "print(f\"Short tweets (2 tokens): {len(short_outliers)}\")\n",
    "print(short_outliers[\"normalized_tweet\"].head(10))\n"
   ],
   "id": "5ed0c08006b62de0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short tweets (2 tokens): 3\n",
      "766334      cool\n",
      "1099614        o\n",
      "1114603    happy\n",
      "Name: normalized_tweet, dtype: object\n"
     ]
    }
   ],
   "execution_count": 97
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:25:44.095995Z",
     "start_time": "2025-04-17T00:25:41.450757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Drop short tweets with less than 2 tokens\n",
    "data = data[data[\"normalized_tweet\"].str.split().apply(len) >= 2]\n"
   ],
   "id": "73e5b1a26a000225",
   "outputs": [],
   "execution_count": 98
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:25:46.703959Z",
     "start_time": "2025-04-17T00:25:44.104612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Detect Long Tweets\n",
    "long_outliers = data[data[\"normalized_tweet\"].str.split().apply(len) >= 100]\n",
    "print(f\"Long tweets (≥100 tokens): {len(long_outliers)}\")\n",
    "print(long_outliers[\"normalized_tweet\"].head(3))\n"
   ],
   "id": "9e1e5488faf8fcde",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long tweets (≥100 tokens): 0\n",
      "Series([], Name: normalized_tweet, dtype: object)\n"
     ]
    }
   ],
   "execution_count": 99
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:25:47.655216Z",
     "start_time": "2025-04-17T00:25:46.738835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Regex pattern for detecting non-ASCII characters\n",
    "import re\n",
    "\n",
    "def is_garbled(text):\n",
    "    return bool(re.search(r\"[^\\x00-\\x7F]\", text))\n",
    "\n",
    "# Apply the check\n",
    "weird_tweets = data[data[\"normalized_tweet\"].apply(is_garbled)]\n",
    "\n",
    "print(f\"Tweets with non-ASCII (possibly garbled): {len(weird_tweets)}\")\n",
    "print(weird_tweets[\"normalized_tweet\"].head(10))\n"
   ],
   "id": "9b3bf9be3fd9a198",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets with non-ASCII (possibly garbled): 10203\n",
      "6365      ï 'k pouty face shitty day out in boston again...\n",
      "31912     the sun is shining and translation of a really...\n",
      "80660     i feel so sick today maybe its all that icecre...\n",
      "118189    crap' i really wanted to make it for ½ but im ...\n",
      "127840    a bus full of kids showed up at my starbucks j...\n",
      "168746         oh joy its gong to be a long weekebd yipee â\n",
      "184101                 lt½' asking myself things i shouldnt\n",
      "196666    ugh mth'r famp½r my bacon egg and cheese bk cr...\n",
      "209764    perdi 2 followers pq falei q estou bipolar up ...\n",
      "230762    faceyourmangacom áááááá¹áááá ááááááááá ááááááá...\n",
      "Name: normalized_tweet, dtype: object\n"
     ]
    }
   ],
   "execution_count": 100
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:25:48.524688Z",
     "start_time": "2025-04-17T00:25:47.655216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Replace common problematic characters\n",
    "replacements = {\n",
    "    '½': '0.5',\n",
    "    '¼': '0.25',\n",
    "    '¾': '0.75',\n",
    "    'â€™': \"'\",\n",
    "    'â€œ': '\"',\n",
    "    'â€': '\"',\n",
    "    'â€“': '-',  # en-dash\n",
    "    'â€”': '-',  # em-dash\n",
    "    'ï': '',     # junk\n",
    "    'á': '',     # remove accented duplicates\n",
    "    'ãª': '',    # junk\n",
    "    'â': '',     # catch-all\n",
    "}\n",
    "\n",
    "def clean_garbled(text):\n",
    "    for bad, good in replacements.items():\n",
    "        text = text.replace(bad, good)\n",
    "    return text\n",
    "\n",
    "data[\"normalized_tweet\"] = data[\"normalized_tweet\"].apply(clean_garbled)\n"
   ],
   "id": "b97e2cc85d942b08",
   "outputs": [],
   "execution_count": 101
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T00:25:49.446436Z",
     "start_time": "2025-04-17T00:25:48.543132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def is_garbled(text):\n",
    "    return bool(re.search(r\"[^\\x00-\\x7F]\", text))\n",
    "\n",
    "# Apply the check\n",
    "weird_tweets = data[data[\"normalized_tweet\"].apply(is_garbled)]\n",
    "\n",
    "print(f\"Tweets with non-ASCII (possibly garbled): {len(weird_tweets)}\")\n",
    "print(weird_tweets[\"normalized_tweet\"].head(10))\n"
   ],
   "id": "bcf86adb5f1a1a35",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets with non-ASCII (possibly garbled): 3975\n",
      "230762                              faceyourmangacom ¹  ¹¹¹\n",
      "230772    åå èå åèäè åæåæ²æåå0.5ä0.5 å sä0.25äè0.75¹æèäè...\n",
      "230884                       nîo curti î musicî novî dî nîo\n",
      "231411    dãteste ana ivanovic eliminated at the french ...\n",
      "231497         thãi rãºt kinh nghim cho lºn thææng äau nã y\n",
      "232285                                            øøù ùùøùù\n",
      "232441    ce greu este sä scriu cu diacritice atunci cãn...\n",
      "232463    ce greu este sä scriu cu diacritice atunci cãn...\n",
      "232577                 cabelodealgodãodoceisthenewblack not\n",
      "233019                      chºc thc cẠäm qua gi mi äc 2 ch\n",
      "Name: normalized_tweet, dtype: object\n"
     ]
    }
   ],
   "execution_count": 102
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T01:21:33.938694Z",
     "start_time": "2025-04-17T00:25:49.466818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == \"en\"\n",
    "    except LangDetectException:\n",
    "        return False\n",
    "\n",
    "# Prepare\n",
    "batch_size = 10000\n",
    "english_flags = []\n",
    "\n",
    "# Loop over the data in batches\n",
    "for start in tqdm(range(0, len(data), batch_size)):\n",
    "    end = start + batch_size\n",
    "    batch = data[\"normalized_tweet\"].iloc[start:end]\n",
    "    flags = [is_english(text) for text in batch]\n",
    "    english_flags.extend(flags)\n",
    "\n",
    "# Add results to DataFrame\n",
    "data[\"is_english\"] = english_flags\n",
    "\n",
    "# Filter English tweets\n",
    "data = data[data[\"is_english\"]].drop(columns=[\"is_english\"])\n"
   ],
   "id": "325622ec0537781",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 152/152 [55:43<00:00, 22.00s/it]\n"
     ]
    }
   ],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T01:21:34.951486Z",
     "start_time": "2025-04-17T01:21:33.996629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def is_garbled(text):\n",
    "    return bool(re.search(r\"[^\\x00-\\x7F]\", text))\n",
    "\n",
    "# Step 1: Find garbled tweets\n",
    "garbled_mask = data[\"normalized_tweet\"].apply(is_garbled)\n",
    "garbled_tweets = data[garbled_mask]\n",
    "\n",
    "# Step 2: Preview how many and what they look like\n",
    "print(f\"Garbled tweets found: {len(garbled_tweets)}\")\n",
    "print(garbled_tweets[\"normalized_tweet\"].head(10))\n",
    "\n"
   ],
   "id": "de7055339977dc9b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Garbled tweets found: 1976\n",
      "230762                              faceyourmangacom ¹  ¹¹¹\n",
      "231411    dãteste ana ivanovic eliminated at the french ...\n",
      "234187    friend on her way round again fiancã still bei...\n",
      "234658    welcome back im sorry to hear that you still h...\n",
      "234830    did that just happen what i just sãderling def...\n",
      "236596    a tomamme un goyur y a seguã empollando inglã ...\n",
      "236678    minesweeper keeps crashin on my itouch i think...\n",
      "237260    watching fandf4 why must they runin all the se...\n",
      "237279    im sorry i did not mean to hurt my little giir...\n",
      "237603    waahh the ending of æçå³ç was so good sad its ...\n",
      "Name: normalized_tweet, dtype: object\n"
     ]
    }
   ],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T01:21:35.085295Z",
     "start_time": "2025-04-17T01:21:34.971181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Drop anything still garbled\n",
    "data = data[~garbled_mask]"
   ],
   "id": "5f04e7f3b91e32fe",
   "outputs": [],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T01:21:37.385546Z",
     "start_time": "2025-04-17T01:21:35.086915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Detect Gibberish / Repeated Characters\n",
    "def is_gibberish(text):\n",
    "    return bool(re.search(r\"(.)\\1{4,}\", text))  # e.g. \"sooooo happy\" or \"aaaaaaah\"\n",
    "\n",
    "gibberish_tweets = data[data[\"normalized_tweet\"].apply(is_gibberish)]\n",
    "print(f\"Gibberish-like tweets: {len(gibberish_tweets)}\")\n",
    "print(gibberish_tweets[\"normalized_tweet\"].head(10))\n"
   ],
   "id": "2d020413e8883d7a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gibberish-like tweets: 27225\n",
      "69          agreed i saw the failwhale allllll day today\n",
      "111    im sooo sad they killed off kutner on house wh...\n",
      "130    haha its so cooooold in the d and no but you s...\n",
      "168    i had on my page for sooooo long until it got ...\n",
      "252    ooooooh sealclap see i download shitloads of z...\n",
      "263    poor socks luvvvvv the golden retriever i want...\n",
      "270    oooooooo who with im not neither but thats bec...\n",
      "313    oh did i mention it quotgooooood moooorniiiiii...\n",
      "348    stupid movies we watched mirrors ugggggh stooo...\n",
      "390    twiggassssssss i been out of range all day im ...\n",
      "Name: normalized_tweet, dtype: object\n"
     ]
    }
   ],
   "execution_count": 106
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T01:21:40.135386Z",
     "start_time": "2025-04-17T01:21:37.403226Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def normalize_repeats(text):\n",
    "    return re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", text)  # replace 3+ same chars with 2\n",
    "\n",
    "# Apply it to the dataset\n",
    "data[\"normalized_tweet\"] = data[\"normalized_tweet\"].apply(normalize_repeats)\n"
   ],
   "id": "74f89490485a6626",
   "outputs": [],
   "execution_count": 107
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T01:21:42.598801Z",
     "start_time": "2025-04-17T01:21:40.152776Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gibberish_tweets = data[data[\"normalized_tweet\"].apply(is_gibberish)]\n",
    "print(f\"Gibberish-like tweets: {len(gibberish_tweets)}\")\n",
    "print(gibberish_tweets[\"normalized_tweet\"].head(10))"
   ],
   "id": "9a6dd874dd956541",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gibberish-like tweets: 0\n",
      "Series([], Name: normalized_tweet, dtype: object)\n"
     ]
    }
   ],
   "execution_count": 108
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T02:11:53.111524Z",
     "start_time": "2025-04-17T01:21:42.614585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Detect Non-English Tweets, already handled bas bent2kd\n",
    "def is_non_english(text):\n",
    "    try:\n",
    "        return detect(text) != \"en\"\n",
    "    except LangDetectException:\n",
    "        return True\n",
    "\n",
    "data[\"non_english\"] = data[\"normalized_tweet\"].apply(is_non_english)\n",
    "non_english_tweets = data[data[\"non_english\"] == True]\n",
    "print(f\"Non-English tweets: {len(non_english_tweets)}\")\n",
    "print(non_english_tweets[\"normalized_tweet\"].head(10))\n"
   ],
   "id": "50e729324fdc5ecd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-English tweets: 13918\n",
      "27                                   im sad now misslilly\n",
      "227                             i still cant find my keys\n",
      "260                       i have to take my sidekick back\n",
      "325                   my man crush jake peavy let me down\n",
      "482                           no u supposed to be my date\n",
      "743         i emailed you yesterday and u never responded\n",
      "892     why were u sleeping ur gonna be up all night n...\n",
      "1038                            i wanna go to lamb of god\n",
      "1041                          awh damn my puppy has ticks\n",
      "1097     now i want amanita someone made me feel unliving\n",
      "Name: normalized_tweet, dtype: object\n"
     ]
    }
   ],
   "execution_count": 109
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T02:11:55.072695Z",
     "start_time": "2025-04-17T02:11:53.200925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check tweets that contain ONLY numbers (possibly with spaces)\n",
    "# Enable progress_apply for tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Define regex pattern to match ONLY numbers (and optional spaces between them)\n",
    "number_only_pattern = re.compile(r\"^\\d+(?:\\s+\\d+)*$\")\n",
    "\n",
    "# Function to check for number-only tweets\n",
    "def is_only_numbers(text):\n",
    "    return bool(number_only_pattern.match(text))\n",
    "\n",
    "# OPTIONAL: define batch size if your dataset is very large\n",
    "batch_size = 10000\n",
    "\n",
    "# Container for all matching tweets\n",
    "only_numbers = pd.DataFrame()\n",
    "\n",
    "# If dataset is small, you can skip batching and just use this:\n",
    "# only_numbers = data[data[\"normalized_tweet\"].progress_apply(is_only_numbers)]\n",
    "\n",
    "# Batch processing\n",
    "for i in tqdm(range(0, len(data), batch_size), desc=\"Processing batches\"):\n",
    "    batch = data.iloc[i:i + batch_size].copy()\n",
    "    matched = batch[batch[\"normalized_tweet\"].progress_apply(is_only_numbers)]\n",
    "    only_numbers = pd.concat([only_numbers, matched], ignore_index=True)\n",
    "\n",
    "print(f\"\\n✅ Found {len(only_numbers)} tweets with only numbers.\")\n",
    "print(only_numbers[\"normalized_tweet\"].head())\n",
    "\n"
   ],
   "id": "d55a89b5ebb527a1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/139 [00:00<?, ?it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 813196.32it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1564455.05it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 610338.03it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1191969.99it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1632660.18it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 728076.66it/s]\n",
      "Processing batches:   4%|▍         | 6/139 [00:00<00:02, 54.33it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 634942.63it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1667450.11it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1490884.02it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1550631.82it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 747274.80it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 837019.36it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 624115.23it/s]\n",
      "Processing batches:   9%|▉         | 13/139 [00:00<00:02, 62.64it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1327059.42it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 2835330.22it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 806798.62it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1270000.61it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1541740.12it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1367782.16it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 628002.64it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 626576.64it/s]\n",
      "Processing batches:  15%|█▌        | 21/139 [00:00<00:01, 68.67it/s]\n",
      "100%|██████████| 10000/10000 [00:00<?, ?it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1505115.01it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1899078.15it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 632777.74it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 948551.27it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<?, ?it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1480098.81it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 932855.30it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 721737.28it/s]\n",
      "Processing batches:  22%|██▏       | 30/139 [00:00<00:01, 74.48it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 622134.15it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<?, ?it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1439610.09it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1102690.54it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<?, ?it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 8237046.35it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 561824.93it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 973879.45it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 4977220.84it/s]\n",
      "Processing batches:  28%|██▊       | 39/139 [00:00<00:01, 78.63it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 729850.35it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<?, ?it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1405409.46it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 807155.72it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1185199.92it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1222508.38it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 689591.77it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<?, ?it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<?, ?it/s]\n",
      "Processing batches:  35%|███▍      | 48/139 [00:00<00:01, 80.92it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 562133.65it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1026355.41it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<?, ?it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 627091.87it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1349692.37it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<?, ?it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 629614.66it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<?, ?it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<?, ?it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1661307.88it/s]\n",
      "Processing batches:  42%|████▏     | 58/139 [00:00<00:00, 82.63it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1036475.15it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 627392.04it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<?, ?it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<?, ?it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<?, ?it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 506326.08it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 842229.72it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 2118442.35it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 898464.97it/s]\n",
      "Processing batches:  48%|████▊     | 67/139 [00:00<00:00, 82.32it/s]\n",
      "100%|██████████| 10000/10000 [00:00<?, ?it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<?, ?it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1511460.90it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1735191.13it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1288176.90it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<?, ?it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<?, ?it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 2871827.46it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 595925.72it/s]\n",
      "Processing batches:  55%|█████▍    | 76/139 [00:00<00:00, 80.91it/s]\n",
      "100%|██████████| 10000/10000 [00:00<?, ?it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<?, ?it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 659284.80it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 639531.59it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 692106.53it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<?, ?it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<?, ?it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 496702.39it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<?, ?it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<?, ?it/s]\n",
      "Processing batches:  62%|██████▏   | 86/139 [00:01<00:00, 83.93it/s]\n",
      "100%|██████████| 10000/10000 [00:00<?, ?it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 803752.87it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<?, ?it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 626670.25it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1976860.07it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1495615.46it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 631834.05it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 620826.52it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1172543.12it/s]\n",
      "Processing batches:  68%|██████▊   | 95/139 [00:01<00:00, 78.92it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 628812.33it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<?, ?it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<?, ?it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 484291.57it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1217398.78it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1830374.86it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 738420.80it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 635029.15it/s]\n",
      "Processing batches:  74%|███████▍  | 103/139 [00:01<00:00, 78.33it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 580518.47it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 704700.01it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1050914.28it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 851929.40it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1024450.20it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1024300.09it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1288651.84it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1024175.03it/s]\n",
      "Processing batches:  80%|███████▉  | 111/139 [00:01<00:00, 72.33it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1247413.75it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 998168.49it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 998049.73it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1024425.18it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1021381.71it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1693094.90it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1403105.74it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 628115.49it/s]\n",
      "Processing batches:  86%|████████▌ | 119/139 [00:01<00:00, 66.96it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 519611.50it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 629983.48it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1246635.16it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 898176.37it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<?, ?it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1593096.32it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1682162.51it/s]\n",
      "Processing batches:  91%|█████████ | 126/139 [00:01<00:00, 64.94it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 885920.92it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 897350.08it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1346961.69it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1899078.15it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 2191152.44it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 629331.25it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<?, ?it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 2078755.02it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 895835.97it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1050598.40it/s]\n",
      "Processing batches:  98%|█████████▊| 136/139 [00:01<00:00, 72.75it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 1734473.58it/s]\n",
      "\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 2790064.52it/s]\n",
      "\n",
      "100%|██████████| 9756/9756 [00:00<?, ?it/s]\n",
      "Processing batches: 100%|██████████| 139/139 [00:01<00:00, 75.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Found 0 tweets with only numbers.\n",
      "Series([], Name: normalized_tweet, dtype: object)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 110
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T02:11:57.270679Z",
     "start_time": "2025-04-17T02:11:55.076214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Remove any digit sequences from within the tweets\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r\"\\b\\d+\\b\", \"\", text)  # remove standalone numbers\n",
    "\n",
    "data[\"normalized_tweet\"] = data[\"normalized_tweet\"].apply(remove_numbers)\n"
   ],
   "id": "7652d0ec1c756d34",
   "outputs": [],
   "execution_count": 111
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T02:11:59.033587Z",
     "start_time": "2025-04-17T02:11:57.287720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Enable progress_apply for tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Define the batch size (adjust as needed)\n",
    "batch_size = 10000\n",
    "\n",
    "# Regex patterns for detecting numbers and text\n",
    "number_pattern = r\"\\d\"\n",
    "text_pattern = r\"[a-zA-Z]\"\n",
    "\n",
    "# Container to hold matching tweets\n",
    "numbers_with_text = pd.DataFrame()\n",
    "\n",
    "# Batch processing\n",
    "for i in tqdm(range(0, len(data), batch_size), desc=\"Processing batches\"):\n",
    "    batch = data.iloc[i:i + batch_size].copy()\n",
    "\n",
    "    # Filter tweets containing both numbers and letters\n",
    "    matched_batch = batch[batch[\"normalized_tweet\"].str.contains(number_pattern) &\n",
    "                          batch[\"normalized_tweet\"].str.contains(text_pattern)]\n",
    "\n",
    "    # Concatenate matched tweets\n",
    "    numbers_with_text = pd.concat([numbers_with_text, matched_batch], ignore_index=True)\n",
    "\n",
    "# Output the number of matched tweets\n",
    "print(f\"Tweets with both text and numbers: {len(numbers_with_text)}\")\n",
    "print(numbers_with_text[\"normalized_tweet\"].head())\n"
   ],
   "id": "854830947792af02",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 139/139 [00:01<00:00, 80.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets with both text and numbers: 80302\n",
      "0    some1 hacked my account on aim now i have to m...\n",
      "1                        sorry bed time came here gmt1\n",
      "2                               ugh92 degrees tomorrow\n",
      "3    i miss my ps3 its out of commission wutcha pla...\n",
      "4    i had such a nice day too bad the rain comes i...\n",
      "Name: normalized_tweet, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T02:12:00.326015Z",
     "start_time": "2025-04-17T02:11:59.058324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Remove digits that appear inside words like \"cool99\"\n",
    "data[\"normalized_tweet\"] = data[\"normalized_tweet\"].str.replace(r\"\\d+\", \"\", regex=True)\n"
   ],
   "id": "49632d852056f637",
   "outputs": [],
   "execution_count": 113
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T02:12:00.855185Z",
     "start_time": "2025-04-17T02:12:00.343177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Drop empty tweets\n",
    "data = data[data[\"normalized_tweet\"].str.strip().astype(bool)]\n"
   ],
   "id": "c3c5138bceb5744e",
   "outputs": [],
   "execution_count": 114
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Class Imbalance",
   "id": "1e519e9653fc02c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T09:47:58.857243Z",
     "start_time": "2025-04-17T09:47:58.792876Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.pie(data['target'].value_counts(), labels=['Negative', 'Poitive'], autopct=\"%1.0f%%\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "5f7ecae217b41378",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 128
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## NLP (tokenization and TF-IDF)",
   "id": "e8b605b5c1da608d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T02:12:52.439332Z",
     "start_time": "2025-04-17T02:12:00.895808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Example: Tokenize a single tweet\n",
    "data[\"tokens\"] = data[\"normalized_tweet\"].apply(word_tokenize)\n"
   ],
   "id": "daf29595f41b250b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    }
   ],
   "execution_count": 115
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T02:12:55.994554Z",
     "start_time": "2025-04-17T02:12:52.447394Z"
    }
   },
   "cell_type": "code",
   "source": "data[\"tokens\"] = data[\"tokens\"].apply(lambda x: [word.lower() for word in x])\n",
   "id": "7af3aca8462611d6",
   "outputs": [],
   "execution_count": 116
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T02:12:58.771205Z",
     "start_time": "2025-04-17T02:12:56.023983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "data[\"tokens\"] = data[\"tokens\"].apply(lambda x: [word for word in x if word not in stop_words])\n"
   ],
   "id": "917d5e212ef3d8c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    }
   ],
   "execution_count": 117
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T02:14:43.490728Z",
     "start_time": "2025-04-17T02:12:58.808636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "stemmer = PorterStemmer()\n",
    "data[\"tokens\"] = data[\"tokens\"].apply(lambda x: [stemmer.stem(word) for word in x])\n"
   ],
   "id": "57e6b92d1493067c",
   "outputs": [],
   "execution_count": 118
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T02:14:57.624676Z",
     "start_time": "2025-04-17T02:14:43.539257Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from scipy.sparse import csr_matrix\n",
    "#\n",
    "# # Apply TF-IDF to your tweet data\n",
    "# tfidf = TfidfVectorizer(max_features=5000)  # Adjust max_features if needed\n",
    "# X_tfidf = tfidf.fit_transform(data[\"normalized_tweet\"])\n",
    "#\n",
    "# # We can work with the sparse matrix directly without converting it to a dense DataFrame\n",
    "# # If needed, we can convert the sparse matrix to a DataFrame later for analysis or export\n",
    "#\n",
    "# # For example, getting the feature names:\n",
    "# feature_names = tfidf.get_feature_names_out()\n",
    "#\n",
    "# # If needed, you can convert it to a sparse DataFrame\n",
    "# tfidf_sparse_df = pd.DataFrame.sparse.from_spmatrix(X_tfidf, columns=feature_names)\n",
    "#\n",
    "# # Show first 5 rows (or any sample)\n",
    "# print(tfidf_sparse_df.head())\n"
   ],
   "id": "51ddb42b97be7bd4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   aa  aah  aahh  aaron  abandoned  abc  ability  abit  able  about  ...  yumm  yummy  yup  zac  zealand  zero  zombie  zone  zoo  zz\n",
      "0   0    0     0      0          0    0        0     0     0      0  ...     0      0    0    0        0     0       0     0    0   0\n",
      "1   0    0     0      0          0    0        0     0     0      0  ...     0      0    0    0        0     0       0     0    0   0\n",
      "2   0    0     0      0          0    0        0     0     0      0  ...     0      0    0    0        0     0       0     0    0   0\n",
      "3   0    0     0      0          0    0        0     0     0      0  ...     0      0    0    0        0     0       0     0    0   0\n",
      "4   0    0     0      0          0    0        0     0     0      0  ...     0      0    0    0        0     0       0     0    0   0\n",
      "\n",
      "[5 rows x 5000 columns]\n"
     ]
    }
   ],
   "execution_count": 119
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T11:01:02.321408Z",
     "start_time": "2025-04-17T10:59:45.198737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout, SpatialDropout1D\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(data['normalized_tweet'])\n",
    "X = tokenizer.texts_to_sequences(data['normalized_tweet'])\n",
    "\n",
    "# Pad sequences to the same length\n",
    "X_pad = pad_sequences(X, maxlen=100)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=5000, output_dim=128, input_length=100))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_pad, data['target'], epochs=5, batch_size=64, validation_split=0.2)\n"
   ],
   "id": "c1202f2e7a3d1844",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shosh\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential_2\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (\u001B[38;5;33mEmbedding\u001B[0m)         │ ?                      │   \u001B[38;5;34m0\u001B[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout1d_2             │ ?                      │             \u001B[38;5;34m0\u001B[0m │\n",
       "│ (\u001B[38;5;33mSpatialDropout1D\u001B[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001B[38;5;33mLSTM\u001B[0m)                   │ ?                      │   \u001B[38;5;34m0\u001B[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001B[38;5;33mDense\u001B[0m)                 │ ?                      │   \u001B[38;5;34m0\u001B[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout1d_2             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001B[1m  291/17372\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m27:35\u001B[0m 97ms/step - accuracy: 0.0036 - loss: -3.7584"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[142]\u001B[39m\u001B[32m, line 25\u001B[39m\n\u001B[32m     22\u001B[39m model.summary()\n\u001B[32m     24\u001B[39m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m25\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_pad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mtarget\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m64\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_split\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.2\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001B[39m, in \u001B[36mfilter_traceback.<locals>.error_handler\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    115\u001B[39m filtered_tb = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    116\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m117\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    119\u001B[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:371\u001B[39m, in \u001B[36mTensorFlowTrainer.fit\u001B[39m\u001B[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001B[39m\n\u001B[32m    369\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m step, iterator \u001B[38;5;129;01min\u001B[39;00m epoch_iterator:\n\u001B[32m    370\u001B[39m     callbacks.on_train_batch_begin(step)\n\u001B[32m--> \u001B[39m\u001B[32m371\u001B[39m     logs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    372\u001B[39m     callbacks.on_train_batch_end(step, logs)\n\u001B[32m    373\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.stop_training:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:219\u001B[39m, in \u001B[36mTensorFlowTrainer._make_function.<locals>.function\u001B[39m\u001B[34m(iterator)\u001B[39m\n\u001B[32m    215\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mfunction\u001B[39m(iterator):\n\u001B[32m    216\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\n\u001B[32m    217\u001B[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001B[32m    218\u001B[39m     ):\n\u001B[32m--> \u001B[39m\u001B[32m219\u001B[39m         opt_outputs = \u001B[43mmulti_step_on_iterator\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    220\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m opt_outputs.has_value():\n\u001B[32m    221\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001B[39m, in \u001B[36mfilter_traceback.<locals>.error_handler\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    148\u001B[39m filtered_tb = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    149\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m150\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    151\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    152\u001B[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001B[39m, in \u001B[36mFunction.__call__\u001B[39m\u001B[34m(self, *args, **kwds)\u001B[39m\n\u001B[32m    830\u001B[39m compiler = \u001B[33m\"\u001B[39m\u001B[33mxla\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mnonXla\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    832\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m._jit_compile):\n\u001B[32m--> \u001B[39m\u001B[32m833\u001B[39m   result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    835\u001B[39m new_tracing_count = \u001B[38;5;28mself\u001B[39m.experimental_get_tracing_count()\n\u001B[32m    836\u001B[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001B[39m, in \u001B[36mFunction._call\u001B[39m\u001B[34m(self, *args, **kwds)\u001B[39m\n\u001B[32m    875\u001B[39m \u001B[38;5;28mself\u001B[39m._lock.release()\n\u001B[32m    876\u001B[39m \u001B[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001B[39;00m\n\u001B[32m    877\u001B[39m \u001B[38;5;66;03m# run the first trace but we should fail if variables are created.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m878\u001B[39m results = \u001B[43mtracing_compilation\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcall_function\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    879\u001B[39m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_variable_creation_config\u001B[49m\n\u001B[32m    880\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    881\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._created_variables:\n\u001B[32m    882\u001B[39m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mCreating variables on a non-first call to a function\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    883\u001B[39m                    \u001B[33m\"\u001B[39m\u001B[33m decorated with tf.function.\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001B[39m, in \u001B[36mcall_function\u001B[39m\u001B[34m(args, kwargs, tracing_options)\u001B[39m\n\u001B[32m    137\u001B[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001B[32m    138\u001B[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001B[32m--> \u001B[39m\u001B[32m139\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunction\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# pylint: disable=protected-access\u001B[39;49;00m\n\u001B[32m    140\u001B[39m \u001B[43m    \u001B[49m\u001B[43mflat_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfunction\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcaptured_inputs\u001B[49m\n\u001B[32m    141\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001B[39m, in \u001B[36mConcreteFunction._call_flat\u001B[39m\u001B[34m(self, tensor_inputs, captured_inputs)\u001B[39m\n\u001B[32m   1318\u001B[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001B[32m   1319\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001B[32m   1320\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[32m   1321\u001B[39m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1322\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_inference_function\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcall_preflattened\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1323\u001B[39m forward_backward = \u001B[38;5;28mself\u001B[39m._select_forward_and_backward_functions(\n\u001B[32m   1324\u001B[39m     args,\n\u001B[32m   1325\u001B[39m     possible_gradient_type,\n\u001B[32m   1326\u001B[39m     executing_eagerly)\n\u001B[32m   1327\u001B[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001B[39m, in \u001B[36mAtomicFunction.call_preflattened\u001B[39m\u001B[34m(self, args)\u001B[39m\n\u001B[32m    214\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcall_preflattened\u001B[39m(\u001B[38;5;28mself\u001B[39m, args: Sequence[core.Tensor]) -> Any:\n\u001B[32m    215\u001B[39m \u001B[38;5;250m  \u001B[39m\u001B[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m216\u001B[39m   flat_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcall_flat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    217\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.function_type.pack_output(flat_outputs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001B[39m, in \u001B[36mAtomicFunction.call_flat\u001B[39m\u001B[34m(self, *args)\u001B[39m\n\u001B[32m    249\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m record.stop_recording():\n\u001B[32m    250\u001B[39m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._bound_context.executing_eagerly():\n\u001B[32m--> \u001B[39m\u001B[32m251\u001B[39m     outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_bound_context\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcall_function\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    252\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    253\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    254\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfunction_type\u001B[49m\u001B[43m.\u001B[49m\u001B[43mflat_outputs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    255\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    256\u001B[39m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    257\u001B[39m     outputs = make_call_op_in_graph(\n\u001B[32m    258\u001B[39m         \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    259\u001B[39m         \u001B[38;5;28mlist\u001B[39m(args),\n\u001B[32m    260\u001B[39m         \u001B[38;5;28mself\u001B[39m._bound_context.function_call_options.as_attrs(),\n\u001B[32m    261\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001B[39m, in \u001B[36mContext.call_function\u001B[39m\u001B[34m(self, name, tensor_inputs, num_outputs)\u001B[39m\n\u001B[32m   1686\u001B[39m cancellation_context = cancellation.context()\n\u001B[32m   1687\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m cancellation_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1688\u001B[39m   outputs = \u001B[43mexecute\u001B[49m\u001B[43m.\u001B[49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1689\u001B[39m \u001B[43m      \u001B[49m\u001B[43mname\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mutf-8\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1690\u001B[39m \u001B[43m      \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1691\u001B[39m \u001B[43m      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtensor_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1692\u001B[39m \u001B[43m      \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1693\u001B[39m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   1694\u001B[39m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1695\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1696\u001B[39m   outputs = execute.execute_with_cancellation(\n\u001B[32m   1697\u001B[39m       name.decode(\u001B[33m\"\u001B[39m\u001B[33mutf-8\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m   1698\u001B[39m       num_outputs=num_outputs,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1702\u001B[39m       cancellation_manager=cancellation_context,\n\u001B[32m   1703\u001B[39m   )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001B[39m, in \u001B[36mquick_execute\u001B[39m\u001B[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[39m\n\u001B[32m     51\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m     52\u001B[39m   ctx.ensure_initialized()\n\u001B[32m---> \u001B[39m\u001B[32m53\u001B[39m   tensors = \u001B[43mpywrap_tfe\u001B[49m\u001B[43m.\u001B[49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     54\u001B[39m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     55\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m core._NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m     56\u001B[39m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 142
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## bazbt el target",
   "id": "a97777d20a2192b5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T02:14:57.832101Z",
     "start_time": "2025-04-17T02:14:57.739309Z"
    }
   },
   "cell_type": "code",
   "source": "print(data[\"target\"].unique())\n",
   "id": "9ac23122bf439be1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.int64(0) np.int64(4)]\n"
     ]
    }
   ],
   "execution_count": 120
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T02:14:58.688495Z",
     "start_time": "2025-04-17T02:14:57.839608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert target to integers if they are valid\n",
    "data[\"target\"] = data[\"target\"].apply(lambda x: int(x) if isinstance(x, (int, float)) else x)\n",
    "\n",
    "# Check unique values again to see if they are consistent\n",
    "print(data[\"target\"].unique())\n"
   ],
   "id": "3d4db1eace5e1b4f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 4]\n"
     ]
    }
   ],
   "execution_count": 121
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T02:14:58.785204Z",
     "start_time": "2025-04-17T02:14:58.779038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Convert target values according to the specified mapping\n",
    "# data[\"target\"] = data[\"target\"].replace({0: -1, 2: 0, 4: 1})\n",
    "#\n",
    "# # Check unique values again to see if they are consistent\n",
    "# print(data[\"target\"].unique())\n"
   ],
   "id": "a88be3253fc62e6d",
   "outputs": [],
   "execution_count": 122
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T02:14:59.171298Z",
     "start_time": "2025-04-17T02:14:58.869176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert target values: 0 → -1, 4 → 1\n",
    "data[\"target\"] = data[\"target\"].replace({0: -1, 4: 1})\n",
    "\n",
    "# Check unique values again to ensure correctness\n",
    "print(data[\"target\"].unique())\n",
    "# Drop neutral tweets if still present\n",
    "data = data[data[\"target\"] != 2]\n"
   ],
   "id": "28ff7228d50830e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  1]\n"
     ]
    }
   ],
   "execution_count": 123
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T09:43:43.885415Z",
     "start_time": "2025-04-17T09:43:43.873112Z"
    }
   },
   "cell_type": "code",
   "source": "data.head()",
   "id": "75405276bd96ea6a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                               tweet  target                                   normalized_tweet  non_english                                             tokens\n",
       "0  - A that's a bummer. You shoulda got David Car...      -1  a thats a bummer you shoulda got david carr of...        False  [that, bummer, shoulda, got, david, carr, thir...\n",
       "1  is upset that he can't update his Facebook by ...      -1  is upset that he cant update his facebook by t...        False  [upset, cant, updat, facebook, text, might, cr...\n",
       "2  I dived many times for the ball. Managed to sa...      -1  i dived many times for the ball managed to sav...        False  [dive, mani, time, ball, manag, save, rest, go...\n",
       "3     my whole body feels itchy and like its on fire      -1     my whole body feels itchy and like its on fire        False             [whole, bodi, feel, itchi, like, fire]\n",
       "4  no, it's not behaving at all. i'm mad. why am ...      -1  no its not behaving at all im mad why am i her...        False                        [behav, im, mad, cant, see]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>target</th>\n",
       "      <th>normalized_tweet</th>\n",
       "      <th>non_english</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- A that's a bummer. You shoulda got David Car...</td>\n",
       "      <td>-1</td>\n",
       "      <td>a thats a bummer you shoulda got david carr of...</td>\n",
       "      <td>False</td>\n",
       "      <td>[that, bummer, shoulda, got, david, carr, thir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>is upset that he cant update his facebook by t...</td>\n",
       "      <td>False</td>\n",
       "      <td>[upset, cant, updat, facebook, text, might, cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I dived many times for the ball. Managed to sa...</td>\n",
       "      <td>-1</td>\n",
       "      <td>i dived many times for the ball managed to sav...</td>\n",
       "      <td>False</td>\n",
       "      <td>[dive, mani, time, ball, manag, save, rest, go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>-1</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>False</td>\n",
       "      <td>[whole, bodi, feel, itchi, like, fire]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no, it's not behaving at all. i'm mad. why am ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>no its not behaving at all im mad why am i her...</td>\n",
       "      <td>False</td>\n",
       "      <td>[behav, im, mad, cant, see]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 125
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Making sure classes dont overlap",
   "id": "3712b3f62d884930"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T09:56:44.597209Z",
     "start_time": "2025-04-17T09:56:42.227190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assuming binary classification: 0 and 1\n",
    "tokens_class_0 = set([token for tokens in data[data['target'] == -1]['tokens'] for token in tokens])\n",
    "tokens_class_1 = set([token for tokens in data[data['target'] == 1]['tokens'] for token in tokens])\n",
    "\n",
    "overlap = tokens_class_0.intersection(tokens_class_1)\n",
    "print(f\"Overlapping tokens between classes: {len(overlap)}\")\n"
   ],
   "id": "b222fab8d8055b56",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlapping tokens between classes: 66159\n"
     ]
    }
   ],
   "execution_count": 130
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T10:10:33.393783Z",
     "start_time": "2025-04-17T10:10:25.621781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Token counters per class\n",
    "tokens_0 = [t for tokens in data[data['target'] == -1]['tokens'] for t in tokens]\n",
    "tokens_1 = [t for tokens in data[data['target'] == 1]['tokens'] for t in tokens]\n",
    "\n",
    "counter_0 = Counter(tokens_0)\n",
    "counter_1 = Counter(tokens_1)\n",
    "\n",
    "# Vocabulary size\n",
    "full_vocab = set(counter_0.keys()) | set(counter_1.keys())\n",
    "vocab_size = len(full_vocab)\n",
    "top_n = int(vocab_size * 0.8)  # Adjust % here\n",
    "\n",
    "# Unique class-specific tokens\n",
    "unique_0 = set(counter_0.keys()) - set(counter_1.keys())\n",
    "unique_1 = set(counter_1.keys()) - set(counter_0.keys())\n",
    "\n",
    "# Top-N unique tokens from each class\n",
    "top_0 = set([t for t, _ in counter_0.most_common(top_n) if t in unique_0])\n",
    "top_1 = set([t for t, _ in counter_1.most_common(top_n) if t in unique_1])\n",
    "\n",
    "selected_tokens = top_0 | top_1\n",
    "\n",
    "# Filter tokens\n",
    "data['tokens_filtered'] = data['tokens'].apply(lambda tokens: [t for t in tokens if t in selected_tokens])\n"
   ],
   "id": "6a5474a4519c0054",
   "outputs": [],
   "execution_count": 133
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T10:10:34.042286Z",
     "start_time": "2025-04-17T10:10:33.411835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data['token_count_before'] = data['tokens'].apply(len)\n",
    "data['token_count_after'] = data['tokens_filtered'].apply(len)\n",
    "\n",
    "retention_ratio = (data['token_count_after'].sum() / data['token_count_before'].sum()) * 100\n",
    "print(f\"Token retention after filtering: {retention_ratio:.2f}%\")\n"
   ],
   "id": "cec4924a5de31faa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token retention after filtering: 2.71%\n"
     ]
    }
   ],
   "execution_count": 134
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T10:18:35.202456Z",
     "start_time": "2025-04-17T10:18:34.308234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for word in ['awe', 'love', 'miss']:\n",
    "    count_pos = sum(word in tokens for tokens in data[data['target'] == 1]['tokens'])\n",
    "    count_neg = sum(word in tokens for tokens in data[data['target'] == 0]['tokens'])\n",
    "    print(f\"{word}: Positive={count_pos}, Negative={count_neg}\")\n"
   ],
   "id": "8c0daf82700ad532",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "awe: Positive=316, Negative=0\n",
      "love: Positive=51242, Negative=0\n",
      "miss: Positive=8211, Negative=0\n"
     ]
    }
   ],
   "execution_count": 135
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T10:19:59.534397Z",
     "start_time": "2025-04-17T10:19:58.731768Z"
    }
   },
   "cell_type": "code",
   "source": "print(data[data['tokens'].apply(lambda x: 'love' in x)]['target'].value_counts())\n",
   "id": "d260a764498415c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      " 1    51242\n",
      "-1    19057\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 136
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Saving data to new csv file",
   "id": "9c5a0af51dd024c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T09:53:45.041178Z",
     "start_time": "2025-04-17T09:53:39.611799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#columns: normalized_tweet, tokens, target\n",
    "selected_cols = data[['normalized_tweet', 'tokens', 'target']]\n",
    "\n",
    "# Save to CSV\n",
    "selected_cols.to_csv('English_cleaned.csv', index=False)\n",
    "\n"
   ],
   "id": "9d4e6c580e21bbc4",
   "outputs": [],
   "execution_count": 129
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
